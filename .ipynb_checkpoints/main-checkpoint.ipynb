{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D Reconstruction of Fractured Femoral Bone from X-Ray Based Image**\n",
    "\n",
    "*Implement by Mr.Danupong Buttongkum*\n",
    "\n",
    "* Last Modified Date: **11-09-2022**\n",
    "* Device: **Tyan GPU**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preparation \n",
    "\n",
    "Import nessesary toolbox and add-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T04:28:14.676433Z",
     "start_time": "2020-10-23T04:28:12.909910Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simple Toolbox \n",
    "from __future__ import print_function, division\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import skimage\n",
    "from skimage import io, transform, measure\n",
    "import scipy\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import sys\n",
    "import ipyvolume as ipv\n",
    "import k3d\n",
    "import pythreejs   # for controlling the Camera using with ipyvolume\n",
    "import ipywidgets\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import joblib\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# Advance Toolbox \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F                                      # useful stateless functions\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torch.autograd import Function, Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "from torchvision import utils, transforms\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from optuna.visualization import plot_contour, plot_edf, plot_intermediate_values, plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_param_importances, plot_slice\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "\n",
    "selectdataset = int(input('Select dataset: \\n1) Dataset3 \\n2) Dataset5 \\n3) Dataset2 \\n4) CU_Dataset2 \\n5) CU_Dataset4_Unaligned \\n6) Siriraj_dataset \\n\\n Input : '))\n",
    "if selectdataset==1:\n",
    "    root_dir = r'D:\\FEW PhD\\Datasets\\HNSC\\Cleaning Data\\Dataset3'     \n",
    "    training_file = r'D:\\FEW PhD\\Datasets\\HNSC\\Cleaning Data\\Dataset3\\Training_set.xlsx'\n",
    "    val_file = r'D:\\FEW PhD\\Datasets\\HNSC\\Cleaning Data\\Dataset3\\Validation_set.xlsx'\n",
    "elif selectdataset==2:\n",
    "    root_dir = r'D:\\FEW PhD\\Datasets\\HNSC\\Cleaning Data\\Dataset5'\n",
    "    training_file = r'D:\\FEW PhD\\Datasets\\HNSC\\Cleaning Data\\Dataset5\\Training_set.xlsx'\n",
    "    val_file = r'D:\\FEW PhD\\Datasets\\HNSC\\Cleaning Data\\Dataset5\\Validation_set.xlsx'\n",
    "elif selectdataset==3:\n",
    "    root_dir = r'D:\\FEW PhD\\Datasets\\HNSC\\Cleaning Data\\Dataset2'\n",
    "    training_file = r'D:\\FEW PhD\\Datasets\\HNSC\\Cleaning Data\\Dataset2\\Training_set2.xlsx'\n",
    "    val_file = r'D:\\FEW PhD\\Datasets\\HNSC\\Cleaning Data\\Dataset2\\Validation_set2.xlsx'\n",
    "elif selectdataset==4:\n",
    "    root_dir = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2'\n",
    "    augment_dataset = int(input('Select : [1] Non-Fracture-Augmention  [2] Fracture-Augmentation  = '))\n",
    "    if augment_dataset == 1:\n",
    "        training_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\Training_set.xlsx'\n",
    "        val_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\Validation_set.xlsx'\n",
    "    elif augment_dataset == 2:\n",
    "        training_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\Training_set2.xlsx'\n",
    "        val_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\Validation_set.xlsx'\n",
    "    else:\n",
    "        assert augment_dataset <= 2 , 'Fail to select dataset'\n",
    "elif selectdataset==5:\n",
    "    root_dir = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned'    # including fractural augmented data\n",
    "    augment_dataset = int(input('Select : [1] Scale12 [2] Scale123  [3] Extra  = '))\n",
    "    if augment_dataset == 1:\n",
    "        training_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TrainingSet_Scale12.xlsx'\n",
    "        val_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TestingSet_Scale12.xlsx'\n",
    "    elif augment_dataset == 2:\n",
    "        training_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TrainingSet_Scale123.xlsx'\n",
    "        val_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TestingSet_Scale123.xlsx'\n",
    "    elif augment_dataset == 3:\n",
    "        training_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TrainingSet_Scale123_extra.xlsx'\n",
    "        val_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TestingSet_Scale12.xlsx'\n",
    "    else:\n",
    "        assert augment_dataset <= 3 , 'Fail to select dataset'\n",
    "elif selectdataset==6:\n",
    "    root_dir = r'D:\\FEW PhD\\Datasets\\Siriraj Dataset\\Cleaning Data\\Siriraj_UnalignData'\n",
    "    training_file = r'D:\\FEW PhD\\Datasets\\Siriraj Dataset\\Cleaning Data\\Siriraj_UnalignData\\Siriraj_testset.xlsx'\n",
    "    val_file = r'D:\\FEW PhD\\Datasets\\Siriraj Dataset\\Cleaning Data\\Siriraj_UnalignData\\Siriraj_testset.xlsx'\n",
    "else:\n",
    "    assert selectdataset <= 4 , 'Fail to select dataset'\n",
    "\n",
    "    \n",
    "print('Training set {}'.format(training_file))\n",
    "print('Test set {}'.format(val_file))\n",
    "\n",
    "use_gpu = int(input('Input: [1] Use GPU  [2] Use CPU : '))\n",
    "if use_gpu==1:\n",
    "    USE_GPU = True\n",
    "    print('\\nDevice: GPU')\n",
    "else:\n",
    "    USE_GPU = False\n",
    "    print('\\nDevice: CPU')\n",
    "dtype1 = torch.float32        # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    print('Using device : CUDA')\n",
    "    device = torch.device(\"cuda\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print('   Device name({}): {}'.format(i, torch.cuda.get_device_name(i)))\n",
    "        print('     {}'.format(torch.cuda.get_device_properties(i)))\n",
    "    print('Current device: {}  >>>  {}'.format(torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device())))\n",
    "else:\n",
    "    print('Using device : CPU')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "#from utils import show_sample, FemurDataset, NormalizeSample, ToTensor6, ToTensor7\n",
    "from utils import show_sample, FemurDataset, FemurDataset2, NormalizeSample, NormalizeSample2, NormalizeSample3, ToTensor7, ToTensor8, ToTensor8Plus, ToTensor9\n",
    "from matricesOperator import iou, TP, FP, TN, FN, union, hausdorff_voxel, overlap_based_metrices, mesh_surface_nearest_distance, surface_distance_measurement\n",
    "#from losses import DiceLoss, BCEDiceLoss, MulticlassBCEDiceLoss, MulticlassBCEDiceLoss2, BCEHNMDiceLoss\n",
    "from losses import FocalLossMulticlass\n",
    "#from LossToolbox.FocalLoss.focal_loss import FocalLoss_Ori, BinaryFocalLoss, FEWFocalLoss, FEWFocalLoss2\n",
    "#from LossToolbox.TverskyLoss.binarytverskyloss import FocalBinaryTverskyLoss, BinaryTverskyLossV2\n",
    "from HausdorffLoss.hausdorff_metric import HausdorffDistance, HausdorffDistanceV2\n",
    "#from HausdorffLoss.hausdorff_loss import HausdorffDTLoss, HausdorffERLoss\n",
    "\n",
    "print('\\n--- END ---')\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "print('Current device: {}'.format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visual training dataset\n",
    "files_frame = pd.read_excel(training_file)    # if file is .csv  use pd.read_csv() instead\n",
    "print('Total training sample = 0-{}'.format(len(files_frame.count(1))-1))\n",
    "#n = int(input('Input training sample = '))\n",
    "n = 50    # well-align sample\n",
    "print('File name: ' + files_frame.iloc[n,1] + ' | '+files_frame.iloc[n,2] + ' | ' + files_frame.iloc[n,3])\n",
    "t1 = time.time()\n",
    "target = np.load(os.path.join(root_dir, files_frame.iloc[n,1]))    # Intensity[n,0] or Mask[n,1]\n",
    "view1 = np.load(os.path.join(root_dir, files_frame.iloc[n,2]))\n",
    "view2 = np.load(os.path.join(root_dir, files_frame.iloc[n,3]))\n",
    "print('target classes = {}'.format(np.sort(np.unique(target))))\n",
    "#i = np.uint8(input('Input i = '))\n",
    "#target = np.uint8(target==i)\n",
    "'''t2 = time.time()\n",
    "sample = {'AP':ap , 'LAT':lat , 'Target':target , 'Output':target}\n",
    "t3 = time.time()\n",
    "show_sample(sample, view='all', showOutput=False, detail=True)\n",
    "t4 = time.time()\n",
    "print('t1-2 = {} sec.\\nt2-3 = {} sec.\\nt3-4 = {} sec.\\n'.format(t2-t1,t3-t2,t4-t3))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dataset implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Manual select dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visual training dataset \n",
    "files_frame = pd.read_excel(training_file)\n",
    "print('Total training sample = 0-{}'.format(len(files_frame.count(1))-1))\n",
    "%n = int(input('Input training sample = '))\n",
    "n = 50\n",
    "print('File name: ' + files_frame.iloc[n,1] + ' | '+files_frame.iloc[n,2] + ' | ' + files_frame.iloc[n,3])\n",
    "target = np.load(os.path.join(root_dir, files_frame.iloc[n,1]))    # Intensity[n,0] or Mask[n,1]\n",
    "ap = np.load(os.path.join(root_dir, files_frame.iloc[n,2]))\n",
    "lat = np.load(os.path.join(root_dir, files_frame.iloc[n,3]))\n",
    "\n",
    "classes = np.unique(target)\n",
    "print('classes = {}'.format(classes))\n",
    "'''if ap.ndim == 2:               # set image to get proper tensor's image format\n",
    "    ap = ap[np.newaxis,...]\n",
    "    lat = lat[np.newaxis,...]'''\n",
    "print('target = {}   {}'.format(target.shape, target.dtype))\n",
    "print('ap = {}   {}'.format(ap.shape, ap.dtype))\n",
    "print('lat = {}   {}'.format(lat.shape, lat.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(12,12)\n",
    "fig.tight_layout()   # fig.tight_layout(pad=10)\n",
    "ax[0].imshow(ap, cmap='gray')\n",
    "ax[0].set_title('DRR AP Projection')\n",
    "ax[1].imshow(lat, cmap='gray')\n",
    "ax[1].set_title('DRR LAT Projection')\n",
    "plt.show()\n",
    "\n",
    "ipv.figure(lighting=False)\n",
    "i = 1\n",
    "ipv.volshow(np.uint8(target==i))\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For ToTensor7 : Chula Dataset\n",
    "# create 4D np.array:  first-dim<0.5 for background,  first-dim>0.5 for foreground\n",
    "# target2[0]=background ; target2[1]=boneMask ; target2[2]=fracMask ;\n",
    "target2 = np.zeros((3,*target.shape))\n",
    "print('target2 = {}   {}'.format(target2.shape, target2.dtype))\n",
    "t1 = time.time()\n",
    "#target2[2][(target==40)] = 1                        # logical\n",
    "target2[2] = np.uint8(target==40)*2                  # create new np.uint8\n",
    "t2 = time.time()\n",
    "#target2[1][(target>0)*(target<10)] = 1              # logical\n",
    "#target2[1] = np.where((target>0)*(target<10),1,0)   # np.where\n",
    "target2[1] = np.uint8((target>0)*(target<10))        # create new np.uint8\n",
    "t3 = time.time()\n",
    "print('Sampling time = {} + {} = {} sec'.format(t2-t1, t3-t2, t3-t1))\n",
    "print('target2 = {}   {}'.format(target2.shape, target2.dtype))\n",
    "intersec = target2[2]\n",
    "print('intersec = {}'.format(intersec.shape, intersec.dtype))\n",
    "print('intersec.sum = {:,.2f}'.format(intersec.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "I0 = target2[0]\n",
    "I1 = target2[1]\n",
    "I2 = target2[2]\n",
    "print('I0 = {}   {}\\nI1 = {}   {}'.format(I0.shape,I0.dtype,I1.shape,I1.dtype))\n",
    "print('I0 = {:,.2f}\\nI1 = {:,.2f}\\nI = {:,.2f}'.format(I0.sum(),I1.sum(),I0.sum()+I1.sum()))\n",
    "print('foreground/background ratio = {:,.4f}'.format(I1.sum()/I0.sum()))\n",
    "ipv.figure()\n",
    "ipv.volshow(I0)\n",
    "ipv.show()\n",
    "ipv.figure()\n",
    "ipv.volshow(I1)\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Use DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils import FemurDataset , show_sample , NormalizeSample, ToTensor, ToTensor2, ToTensor3, ToTensor4, ToTensor5, ToTensor6, ToTensor7, ToTensor8\n",
    "first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ToTensorTest(object):\n",
    "    \"\"\"\n",
    "        Convert ndarrays in sample to Tensors = [N,D,H,W] which have values={0,1,2}\n",
    "        Input:  [0] = background\n",
    "                [1,2,3,...,10] = fragment of femur bone\n",
    "                [20] = pelvic bone\n",
    "                [30] = soft-tissue\n",
    "                [40] = fracture\n",
    "                [50] = comminute\n",
    "        Output: Positive non-zeros class in same channel which have label = 0,1,2  except pelvic volume\n",
    "        classes = {0==background  1==femurMask  2==fracMask}\n",
    "    \"\"\"\n",
    "    def __call__(self, sample):        # callable classes\n",
    "        print('### ToTensor8 ###')\n",
    "        target, view1, view2 = sample['Target'], sample['view1'], sample['view2']   # target = [D,W,-H]\n",
    "        classes = np.unique(target)\n",
    "        print('    target = {}   {}   {}'.format(target.shape,type(target), target.dtype))\n",
    "        print('    classes = {}'.format(classes))\n",
    "        if view1.ndim == 2:               # set image to get proper tensor's image format\n",
    "            view1 = view1[np.newaxis,...]    # 1-view\n",
    "            view2 = view2[np.newaxis,...]  # 2-view\n",
    "        \n",
    "        # create 3D np.array:  first-dim<0.5 for background,  first-dim>0.5 for foreground\n",
    "        t1 = time.time()\n",
    "        target2 = np.zeros(target.shape)    # [D,W,-H]\n",
    "        target2[ target==40 ] = 2\n",
    "        target2[ (target>0)*(target<10) ] = 1\n",
    "        \n",
    "        # transpose: [D,W,-H] to [D,H,W]\n",
    "        target2 = np.flip(target2.transpose(0,2,1), axis=1).copy()\n",
    "        t2 = time.time()\n",
    "        print('   target2 = {}   {}   {}'.format(target2.shape, type(target2), target2.dtype))\n",
    "        print('   view1 = {}   {}   {}'.format(view1.shape, type(view1), view1.dtype))\n",
    "        print('   view2 = {}   {}   {}'.format(view2.shape, type(view2), view2.dtype))\n",
    "        print('   Time = {} sec.'.format(t2-t1))\n",
    "        \n",
    "        return {'Target': torch.from_numpy(target2).int().squeeze(),\n",
    "                'view1': torch.from_numpy(view1).float(),\n",
    "                'view2':torch.from_numpy(view2).float() }\n",
    "    \n",
    "print(' --- END --- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_transformedFemur = FemurDataset2(csv_file=training_file, root_dir=root_dir, \n",
    "                                      transform=transforms.Compose([NormalizeSample2(),ToTensorTest()]))\n",
    "trainLoader = DataLoader(train_transformedFemur, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "if first==True:\n",
    "    print('First testing sample')\n",
    "    femurIter = iter(trainLoader)\n",
    "    first = False\n",
    "else:\n",
    "    print('Next testing sample')\n",
    "    \n",
    "t1 = time.time()\n",
    "sample = next(femurIter)\n",
    "t2 = time.time()\n",
    "print(sample.keys())\n",
    "view1 = sample['view1'].squeeze()\n",
    "view2 = sample['view2'].squeeze()\n",
    "target = sample['Target'].squeeze()\n",
    "print('target = {}   {}   fractureCount={:,.0f}'.format(target.size(),target.dtype,(target==2).sum()))\n",
    "print('Sampling time = {} sec.\\n'.format(t2-t1))\n",
    "print('Classes list ={}'.format(torch.unique(target)))\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,12))\n",
    "ax[0].imshow(view1, cmap='gray')\n",
    "ax[1].imshow(view2, cmap='gray')\n",
    "fig.show()\n",
    "\n",
    "ipv.figure()\n",
    "ipv.volshow(target==1)   # for ToTensor6-8\n",
    "#ipv.view(270, 90)\n",
    "ipv.show()\n",
    "time.sleep(1)\n",
    "ipv.figure()\n",
    "'''ipv.volshow(target==2)   # for ToTensor6-8\n",
    "ipv.view(270, 90)\n",
    "ipv.show()'''\n",
    "\n",
    "'''backMask = torch.sum(target[0,0])\n",
    "boneMask = torch.sum(target[0,1])\n",
    "fracMask = torch.sum(target[0,2])\n",
    "print('Background = {:,.1f}   :{:,.4f}%'.format(backMask, backMask/(256**3)*100))\n",
    "print('boneMask = {:,.1f}   :{:,.4f}%'.format(boneMask, boneMask/(256**3)*100))\n",
    "print('fracMask = {:,.1f}   :{:,.5f}%\\n'.format(fracMask, fracMask/(256**3)*100))\n",
    "ipv.figure()\n",
    "ipv.volshow(target[0,0])\n",
    "ipv.view(270, 90)\n",
    "ipv.show()\n",
    "ipv.figure()\n",
    "ipv.volshow(target[0,1])\n",
    "ipv.view(270, 90)\n",
    "ipv.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example for 2D Convolutional and ConvolutionTranspose layer\n",
    "\n",
    "# With square kernels and equal stride\n",
    "m1 = nn.ConvTranspose2d(16, 33, 3, stride=1, padding=0)\n",
    "# non-square kernels and unequal stride and with padding\n",
    "m2 = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(0, 0))\n",
    "input1 = torch.randn(20, 16, 50, 100)\n",
    "print('input1.size = {}'.format(input1.size()))\n",
    "output1 = m1(input1)\n",
    "print('output1.size = {}'.format(output1.size()))\n",
    "output2 = m2(input1)\n",
    "print('output2.size = {}\\n'.format(output2.size()))\n",
    "\n",
    "\n",
    "# exact output size can be also specified as an argument\n",
    "input2 = torch.randn(1, 16, 256, 256)\n",
    "print('input2.size = {}'.format(input2.size()))\n",
    "downsample = nn.Conv2d(16, 32, 7, stride=1, padding=3)\n",
    "upsample = nn.ConvTranspose2d(16, 32, 2, stride=2, padding=0)\n",
    "DownOutput = downsample(input2)\n",
    "print('DownOutput.size = {}'.format(DownOutput.size()))\n",
    "UpOutput = upsample(input2)\n",
    "print('UpOutput.size = .{}'.format(UpOutput.size()))\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "#   Output features size calculation of Convolutional layer        #\n",
    "#                                                                  #\n",
    "#  1) nn.Conv2d:                 output = (input+2*P-F)/S+1        #\n",
    "#                                                                  #\n",
    "#  2) nn.ConvTranspose2d:        output = (input-1)*S-2*P+F        #\n",
    "#    Hint: use F=2*P+1  and  S=1 to get output.size = input.size   #\n",
    "#                                                                  #\n",
    "#          This calculation can applied for nn.Conv3d              #\n",
    "####################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example for 3D-Convolutional and 3D-ConvolutionTranspose layer\n",
    "\n",
    " # With square kernels and equal stride\n",
    "m = nn.Conv3d(1, 4, kernel_size=3, stride=1, padding=1)\n",
    "m2 = nn.ConvTranspose3d(2, 8, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "input = torch.randn(2, 2, 32, 32, 32)\n",
    "#print('input = {}'.format(input))\n",
    "print('input.size() = {}'.format(input.size()))\n",
    "\n",
    "t1 = time.time()\n",
    "output = m2(input)\n",
    "t2 = time.time()\n",
    "#print('output = {}'.format(output))\n",
    "print('output.size = {}'.format(output.size()))\n",
    "print('Total calculation time = {} sec.'.format(t2-t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T08:15:21.112199Z",
     "start_time": "2020-10-21T08:15:21.051197Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example for nn.BatchNorm2d and nn.LayerNorm\n",
    "\n",
    "m1 = nn.BatchNorm2d(100)                # With Learnable Parameters\n",
    "#m1 = nn.BatchNorm2d(100, affine=False)  # Without Learnable Parameters\n",
    "input1 = torch.randn(20, 100, 35, 45)\n",
    "output1 = m1(input1)\n",
    "print('output1.size = {}'.format(output1.size()))\n",
    "print('m1.parameter = {} \\n'.format(m1.parameters))\n",
    "\n",
    "input2 = torch.randn(20, 5, 10, 10)\n",
    "print('input of LayerNorm = {}'.format(input2.size()[1:]))\n",
    "#m2 = nn.LayerNorm(input2.size()[1:])    # With Learnable Parameters \n",
    "#m2 = nn.LayerNorm(input2.size()[1:], elementwise_affine=False)     # Without Learnable Parameters\n",
    "m2 = nn.LayerNorm([10, 10])             # Normalize over last two dimensions\n",
    "#m2 = nn.LayerNorm(10)                   # Normalize over last dimension of size 10\n",
    "output2 = m2(input2)\n",
    "print('output2.size = {}'.format(output2.size()))\n",
    "print('m2.parameters = {}'.format(m2.parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example for MaxPooling and MaxUnPooling\n",
    "\n",
    "pool = nn.MaxPool2d(2, stride=1, return_indices=True)\n",
    "unpool = nn.MaxUnpool2d(2, stride=1)\n",
    "input1 = torch.tensor([[[[ 1.,  2,  3,  4],\n",
    "                            [ 5,  6,  7,  8],\n",
    "                            [ 9, 10, 11, 12],\n",
    "                            [13, 14, 15, 16]]]])\n",
    "\n",
    "print('\\n     ***** Down Sampling *****')\n",
    "output, indices = pool(input1)\n",
    "print('MaxPool2d result = ', output)\n",
    "print('MaxPool2d indices = ', indices)\n",
    "\n",
    "print('\\n     ***** Up Sampling *****')\n",
    "unpool_result = unpool(output, indices)\n",
    "print('MaxUnPool2d result = ', unpool_result)\n",
    "\n",
    "'''\n",
    "unpool_result2 = unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))\n",
    "print('MaxUnPool2d result with specified output_size = ')\n",
    "print(unpool_result2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example for nn.MaxUnpool3d  >>> Error!!!\n",
    "\n",
    "pool = nn.MaxPool3d(kernel_size=3, stride=2, return_indices=True)\n",
    "unpool = nn.MaxUnpool3d(kernel_size=3, stride=2)\n",
    "input1 = torch.randn(20, 16, 51, 33, 15)\n",
    "\n",
    "output, indices = pool(input1)\n",
    "print('output.size = {}'.format(output.size()))\n",
    "print('indices.size = {}'.format(indices.size()))\n",
    "\n",
    "unpooled_output = unpool(output, indices)\n",
    "print('unpooled_output.size = {}'.format(unpooled_output.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:48:54.691644Z",
     "start_time": "2020-10-21T07:48:54.671640Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example for torch.expand(size)\n",
    "\n",
    "q = torch.Tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print('q = {}'.format(q))\n",
    "print('size_q = {} \\n'.format(q.size()))\n",
    "\n",
    "q2 = q.expand(3,3,3)\n",
    "print('q2 = {}'.format(q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example: Maxpooling and MaxUnpooling\n",
    "\n",
    "pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "input1 = torch.tensor([[[[ 1.,  2,  3,  4],\n",
    "                            [ 5,  6,  7,  8],\n",
    "                            [ 9, 10, 11, 12],\n",
    "                            [13, 14, 15, 16]]]])\n",
    "\n",
    "print('     ***** Down Sampling *****')\n",
    "output, indices = pool(input1)\n",
    "print('output = {}'.format(output))\n",
    "print('output.size = {}'.format(output.size()))\n",
    "print('indices = {}'.format(indices))\n",
    "print('indices.size = {}'.format(indices.size()))\n",
    "print('\\n     ***** Up Sampling *****')\n",
    "unpool_result = unpool(output, indices)\n",
    "print('MaxUnPool2d result = {} \\n'.format(unpool_result))\n",
    "\n",
    "print('     ***** Up Sampling 3D*****')\n",
    "output2 = output.expand(1,2,2,2)/2\n",
    "print('output2 = {}'.format(output2))\n",
    "#indices2 = indices.expand(1,2,2,2)\n",
    "indices2 = torch.Tensor([[[[ 5,  7],\n",
    "          [13, 15]],\n",
    "         [[ 5,  7],\n",
    "          [13, 15]]]])\n",
    "\n",
    "print('indices2 = {}'.format(indices2))\n",
    "unpool_result2 = unpool(output,indices, output_size=torch.Size([2,1,2,2]))\n",
    "print('unpool_result2 = {}'.format(unpooled_result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example: nn.Upsample(scale_factor, size, mode) = interpolately expanding of Tensor\n",
    "\n",
    "#input1 = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)\n",
    "input2 = torch.randint(1,10, size=(1,1,2,2), dtype=torch.float32)  # try to vary size=(N,2D-4D)\n",
    "print('input = {}'.format(input2))\n",
    "print('input.size = {}\\n'.format(input2.size()))\n",
    "###  {mode} for nn.Upsample class  ###\n",
    "# nearest           (N,2D-4D) = (N,C,(D),(H),W)    >>  (N,C,(2D),(2H),2W)\n",
    "# linear            (N,2D) = (N,C,W)               >>  (N,C,2W)\n",
    "# bilinear          (N,3D) = (N,C,H,W)             >>  (N,C,2H,2W)\n",
    "# bicubic           (N,3D) = (N,C,H,W)             >>  (N,C,2H,2W)\n",
    "# trilinear         (N,4D) = (N,C,D,H,W)           >>  (N,C,2D,2H,2W)\n",
    "########################################\n",
    "mode_selected = 'bilinear'\n",
    "model1 = nn.Upsample(scale_factor=2, mode=mode_selected, align_corners=None)\n",
    "model2 = nn.Upsample(scale_factor=2, mode=mode_selected, align_corners=True)\n",
    "output1 = model1(input2)\n",
    "output2 = model2(input2)\n",
    "print('output1 = {}'.format(output1))\n",
    "print('output2 = {}'.format(output2))\n",
    "print('output.size = {}'.format(output1.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "Specify data path and Setup computational device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Architecture  (2D>3D mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Convolution block and Class nn.Sequectial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm2d = nn.InstanceNorm2d\n",
    "#norm2d = nn.BatchNorm2d\n",
    "norm3d = nn.InstanceNorm3d\n",
    "#norm3d = nn.BatchNorm3d\n",
    "\n",
    "def single_dense_block(in_f, out_f, *args, **kwargs):\n",
    "    \"\"\" Creat single encode block (norm+activation+conv)\n",
    "    Args:   in_f  = input channel (int) for nn.Conv2d \n",
    "            out_f = output channel (int) for nn.Conv2d and nn.BatchNorm2d or nn.LayerNorm\n",
    "            res = feature resolution for nn.LayerNorm\n",
    "    \"\"\"\n",
    "    return nn.Sequential(norm2d(in_f),\n",
    "                         nn.ReLU(),\n",
    "                         #nn.LeakyReLU(0.01),\n",
    "                         nn.Conv2d(in_f, out_f, *args, **kwargs))\n",
    "\n",
    "def encode_block(in_f, out_f, res, *args, **kwargs):\n",
    "    \"\"\" Creat single encode block (conv+norm+activation)\n",
    "    Args:   in_f  = input channel (int) for nn.Conv2d \n",
    "            out_f = output channel (int) for nn.Conv2d and nn.BatchNorm2d or nn.LayerNorm\n",
    "            res = feature resolution for nn.LayerNorm\n",
    "    \"\"\"\n",
    "    '''\n",
    "    print('in_f = {}'.format(in_f))\n",
    "    print('out_f = {}'.format(out_f))\n",
    "    print('res = {}'.format(res))\n",
    "    '''\n",
    "    return nn.Sequential(nn.Conv2d(in_f, out_f, *args, **kwargs), \n",
    "                         norm2d(out_f),\n",
    "                         #norm2d([out_f,res,res]),\n",
    "                         nn.ReLU(),\n",
    "                         #nn.LeakyReLU(0.01)\n",
    "                        )\n",
    "\n",
    "def decode_block(in_f, out_f, res, *args, **kwargs):\n",
    "    \"\"\" Creat single decode block (conv+norm+activation)\n",
    "    Args:   in_f  = input channel (int) for nn.Conv2d \n",
    "            out_f = output channel (int) for nn.Conv2d and nn.BatchNorm2d\n",
    "            res = feature resolution for nn.LayerNorm\n",
    "    \"\"\"\n",
    "    return nn.Sequential(nn.Conv2d(in_f, out_f, *args, **kwargs), \n",
    "                         norm2d(out_f),\n",
    "                         #nn.Dropout2d(p=0.2),\n",
    "                         #norm2d([out_f,res,res]),\n",
    "                         nn.ReLU(),\n",
    "                         #nn.LeakyReLU(0.01)\n",
    "                        )\n",
    "\n",
    "def fully_conv2d(in_f, out_f, res, *args, **kwargs):\n",
    "    \"\"\" Obsolete!!! Creat single nxn-convolution (conv + norm + activation)\n",
    "    Args:   in_f  = input channel (int) for nn.Conv2d \n",
    "            out_f = output channel (int) for nn.Conv2d and nn.BatchNorm2d\n",
    "            res = feature resolution for nn.LayerNorm\n",
    "    \"\"\"\n",
    "    return nn.Sequential(nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "                         norm2d(out_f),\n",
    "                         #norm2d([out_f,res,res]),\n",
    "                         nn.ReLU(),\n",
    "                         #nn.LeakyReLU(0.01)\n",
    "                        )\n",
    "\n",
    "def conv2d_block(in_f, out_f, res, *args, **kwargs):\n",
    "    \"\"\" Creat single nxn-convolution (conv + norm + activation)\n",
    "    Args:   in_f  = input channel (int) for nn.Conv2d \n",
    "            out_f = output channel (int) for nn.Conv2d and nn.BatchNorm2d\n",
    "            res = feature resolution for nn.LayerNorm\n",
    "    \"\"\"\n",
    "    return nn.Sequential(nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "                         norm2d(out_f),\n",
    "                         #norm2d([out_f,res,res]),\n",
    "                         nn.ReLU(),\n",
    "                         #nn.LeakyReLU(0.01)\n",
    "                        )\n",
    "\n",
    "def conv3d_block(in_f, out_f, res, *args, **kwargs):\n",
    "    \"\"\" Create single 3D-convolution block (conv3d + norm + activation)\n",
    "    Args:   in_f = input channel (int) for nn.Conv3d\n",
    "            out_f = output channel (int) for nn.Conv3D\n",
    "            res = feature resolution for nn.LayerNorm\n",
    "    \"\"\"\n",
    "    return nn.Sequential(nn.Conv3d(in_f, out_f, *args, **kwargs),\n",
    "                         norm3d(out_f),\n",
    "                         #norm3d([out_f,res,res,res]),\n",
    "                         nn.ReLU(),\n",
    "                         #nn.LeakyReLU(0.01)\n",
    "                        )\n",
    "\n",
    "def convtranspose2d_block(in_f, out_f, res, *args, **kwargs):\n",
    "    \"\"\" Create single 3D-transpose-convolution block (convtranspose3d + norm + activation)\n",
    "    Args:   in_f = input channel (int) for nn.ConvTranspose3d\n",
    "            out_f = output channel (int) for nn.ConvTranspose3D\n",
    "            res = feature resolution for nn.LayerNorm\n",
    "    \"\"\"\n",
    "    return nn.Sequential(nn.ConvTranspose2d(in_f, out_f, *args, **kwargs),\n",
    "                         norm2d(out_f),\n",
    "                         nn.ReLU(),\n",
    "                         #nn.LeakyReLU(0.01)\n",
    "                        )\n",
    "\n",
    "def convtranspose3d_block(in_f, out_f, res, *args, **kwargs):\n",
    "    \"\"\" Create single 3D-transpose-convolution block (convtranspose3d + norm + activation)\n",
    "    Args:   in_f = input channel (int) for nn.ConvTranspose3d\n",
    "            out_f = output channel (int) for nn.ConvTranspose3D\n",
    "            res = feature resolution for nn.LayerNorm\n",
    "    \"\"\"\n",
    "    return nn.Sequential(nn.ConvTranspose3d(in_f, out_f, *args, **kwargs),\n",
    "                         norm3d(out_f),\n",
    "                         nn.ReLU(),\n",
    "                         #nn.LeakyReLU(0.01)\n",
    "                        )\n",
    "\n",
    "\n",
    " ##################  Weight Initialization ########################\n",
    "def linear_initialize_sequence(sequential):\n",
    "    \"\"\" Initialize 2D-convolution parameter\n",
    "    Args:   nn.Sequential with include nn.Conv2d\n",
    "    \"\"\"\n",
    "    for seq in sequential:\n",
    "        #print('\\nseq = {}'.format(seq))\n",
    "        for module in seq:\n",
    "            #print('module = {}'.format(module))\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0.1)\n",
    "                #print('weight = {}'.format(module.weight))\n",
    "                #print('bias = {}'.format(module.bias))\n",
    "\n",
    "def conv2d_initialize_sequence(sequential):\n",
    "    \"\"\" Initialize 2D-convolution parameter\n",
    "    Args:   nn.Sequential with include nn.Conv2d\n",
    "    \"\"\"\n",
    "    for seq in sequential:\n",
    "        #print('\\nseq = {}'.format(seq))\n",
    "        for module in seq:\n",
    "            #print('module = {}'.format(module))\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0.1)\n",
    "                #print('weight = {}'.format(module.weight))\n",
    "                #print('bias = {}'.format(module.bias))\n",
    "                \n",
    "def conv3d_initialize_sequence(sequential):\n",
    "    for seq in sequential:\n",
    "        #print('\\nseq = {}'.format(seq))\n",
    "        for module in seq:\n",
    "            #print('module = {}'.format(module))\n",
    "            if isinstance(module, nn.Conv3d):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0.1)\n",
    "                #print('weight = {}'.format(module.weight))\n",
    "                #print('bias = {}'.format(module.bias))\n",
    "                \n",
    "def convtranspose2d_initialize_sequence(sequential):\n",
    "    for seq in sequential:\n",
    "        #print('\\nseq = {}'.format(seq))\n",
    "        for module in seq:\n",
    "            #print('module = {}'.format(module))\n",
    "            if isinstance(module, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0.1)\n",
    "                #print('weight = {}'.format(module.weight))\n",
    "                #print('bias = {}'.format(module.bias))\n",
    "\n",
    "def convtranspose3d_initialize_sequence(sequential):\n",
    "    for seq in sequential:\n",
    "        #print('\\nseq = {}'.format(seq))\n",
    "        for module in seq:\n",
    "            #print('module = {}'.format(module))\n",
    "            if isinstance(module, nn.ConvTranspose3d):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0.1)\n",
    "                #print('weight = {}'.format(module.weight))\n",
    "                #print('bias = {}'.format(module.bias))\n",
    "\n",
    "                \n",
    " ################## Class layer ################## \n",
    "class LinearLayer(nn.Module):\n",
    "    \"\"\" Create sigle block of linear layer with norm and activatetion \n",
    "    \"\"\"\n",
    "    def __init__(self, lin_sz, *args, **kwargs):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        linear_block = [ single_linear_block(in_f, out_f) \n",
    "                        for in_f,out_f in zip(lin_sz,lin_sz[1:])  ]\n",
    "        self.linear_block = nn.Sequential(*linear_block)\n",
    "        linear_initialize_sequence(self.linear_block)\n",
    "    def forward(self, x):\n",
    "        return self.linear_block(x)\n",
    "    \n",
    "class LinearLayer_bn(nn.Module):\n",
    "    \"\"\" Create sigle block of linear layer with norm and activatetion \n",
    "    \"\"\"\n",
    "    def __init__(self, lin_sz, bn_sz, *args, **kwargs):\n",
    "        super(LinearLayer_bn, self).__init__()\n",
    "        linear_bn_block = [ single_linear_bn_block(in_f, out_f, bn_sz) \n",
    "                        for in_f,out_f in zip(lin_sz,lin_sz[1:])  ]\n",
    "        self.linear_bn_block = nn.Sequential( *linear_bn_block )\n",
    "        linear_initialize_sequence(self.linear_block)\n",
    "    def forward(self, x):\n",
    "        return self.linear_bn_block(x)\n",
    "    \n",
    "class MyEncoder(nn.Module):\n",
    "    \"\"\" Create a bundle of level for Encoder\n",
    "    Args:  en_sz = 2D-list [[encode_block1],[encode_block2],[encode_block3],...,[encode_blockN]]\n",
    "            *args, **kwargs = up on decode_block\n",
    "    \"\"\"\n",
    "    def __init__(self, en_sz, *args, **kwargs):\n",
    "        super(MyEncoder, self).__init__()\n",
    "        encode_blocks = [ encode_block(in_f, out_f, *args, **kwargs) \n",
    "                         for in_f, out_f in zip(en_sz,en_sz[1:]) ]\n",
    "        self.encode_blocks = nn.Sequential( *encode_blocks )\n",
    "        #print(self.encode_blocks,'\\n',type(self.encode_blocks))\n",
    "        conv2d_initialize_sequence(self.encode_blocks)\n",
    "    def forward(self, x):\n",
    "        return self.encode_blocks(x)\n",
    "    \n",
    "class MyDenseLayer(nn.Module):\n",
    "    \"\"\" Create a bundle of dense layer\n",
    "    Args:   in_f = input channel \n",
    "            k = growth rate of channel\n",
    "            layer = number of composite layer\n",
    "            bn_size = bottom neck size\n",
    "            *args, **kwargs = up on single_dense_block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_f, k, level, *args, **kwargs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        dense_blocks = [ single_dense_block(in_f + i*k, k, *args, **kwargs) \n",
    "                         for i in range(level) ]\n",
    "        self.dense_blocks = nn.Sequential(*dense_blocks)\n",
    "        conv2d_initialize_sequence(self.dense_blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_cat = x\n",
    "        for i , level in enumerate(self.dense_blocks):\n",
    "            if i == 0:\n",
    "                x = level(x_cat)\n",
    "            else:\n",
    "                x = level(x_cat)\n",
    "            x_cat = torch.cat((x_cat,x),dim=1)\n",
    "            #print('   x_cat = {}'.format(x_cat.size()))\n",
    "            #print('   x = {}'.format(x.size()))\n",
    "        #print('   # Dense layer output = {} \\n'.format(x_cat.size()))\n",
    "        return x_cat\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "    \"\"\" Create a bundle of level\n",
    "    Args:  de_sz = 2D-list [[decode_block1],[decode_block2],[decode_block3],...,[decode_blockN]]\n",
    "            *args, **kwargs = up on decode_block\n",
    "    \"\"\"\n",
    "    def __init__(self, de_sz, *args, **kwargs):\n",
    "        super(MyDecoder, self).__init__()\n",
    "        decode_blocks = [ decode_block(in_f, out_f, *args, **kwargs) \n",
    "                         for in_f, out_f in zip(de_sz,de_sz[1:]) ]\n",
    "        self.decode_blocks = nn.Sequential( *decode_blocks )\n",
    "        #print(self.decode_blocks,'\\n',type(self.decode_blocks))\n",
    "        conv2d_initialize_sequence(self.decode_blocks)\n",
    "    def forward(self, x):\n",
    "        return self.decode_blocks(x)\n",
    "    \n",
    "class AxialFusion(nn.Module):\n",
    "    \"\"\" Axially fuse view1 and view2 2D_features by nn.Conv2d \n",
    "    \"\"\"\n",
    "    def __init__(self, de3d_sz, *args, **kwargs):\n",
    "        super(AxialFusion, self).__init__()\n",
    "        fusion_blocks = [ conv2d_block(in_f, out_f, *args, **kwargs) \n",
    "                          for in_f, out_f in zip(de3d_sz,de3d_sz[1:]) ]\n",
    "        self.fusion_blocks = nn.Sequential( *fusion_blocks )\n",
    "        conv2d_initialize_sequence(self.fusion_blocks)\n",
    "    def forward(self, x):\n",
    "        return self.fusion_blocks(x)\n",
    "    \n",
    "class Fusion2d(nn.Module):\n",
    "    \"\"\" Fusion view1 and view2 features by nn.Conv2d \n",
    "    \"\"\"\n",
    "    def __init__(self, de_sz, *args, **kwargs):\n",
    "        super(Fusion2d, self).__init__()\n",
    "        fusion_blocks = [ decode_block(in_f, out_f, *args, **kwargs) \n",
    "                         for in_f, out_f in zip(de_sz,de_sz[1:]) ]\n",
    "        self.fusion_blocks = nn.Sequential( *fusion_blocks )\n",
    "        conv2d_initialize_sequence(self.fusion_blocks)\n",
    "    def forward(self, x):\n",
    "        return self.fusion_blocks(x)\n",
    "    \n",
    "class MyDecoder3d(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, de3d_sz, *args, **kwargs):\n",
    "        super(MyDecoder3d, self).__init__()\n",
    "        #print('\\nMyDecoder3d')\n",
    "        decode3d_blocks = [ conv3d_block(in_f, out_f, *args, **kwargs)\n",
    "                           for in_f, out_f in zip(de3d_sz,de3d_sz[1:]) ]\n",
    "        #print(decode3d_blocks,'\\n',type(decode3d_blocks))\n",
    "        self.decode3d_blocks = nn.Sequential( *decode3d_blocks )\n",
    "    def forward(self, x):\n",
    "        return self.decode3d_blocks(x)\n",
    "    \n",
    "class FullyConv3d(nn.Module):  # same function as MyDeCoder3d\n",
    "    \"\"\" Creater a bundle of level\n",
    "    Args:   final_sz = 1D-list [final_sz1,final_sz2,...,final_szN]\n",
    "            res = H and W dimension resolution \n",
    "            *args, **kwargs = up on fully_conv2d\n",
    "    \"\"\"\n",
    "    def __init__(self, final_sz, *args, **kwargs):\n",
    "        super(FullyConv3d, self).__init__()\n",
    "        conv3d_blocks = [ conv3d_block( in_f, out_f, *args, **kwargs) \n",
    "                           for in_f , out_f in zip(final_sz,final_sz[1:])]\n",
    "        self.conv3d_blocks = nn.Sequential( *conv3d_blocks )\n",
    "        conv3d_initialize_sequence(self.conv3d_blocks)\n",
    "    def forward(self, x):\n",
    "        return self.conv3d_blocks(x)\n",
    "    \n",
    "class UpConv2d(nn.Module):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    def __init__(self, final_sz, *args, **kwargs):\n",
    "        super(UpConv2d, self).__init__()\n",
    "        convtranspose2d_blocks = [ convtranspose2d_block( in_f, out_f, *args, **kwargs) \n",
    "                                  for in_f , out_f in zip(final_sz,final_sz[1:])]\n",
    "        self.convtranspose2d_blocks = nn.Sequential( *convtranspose2d_blocks )\n",
    "        convtranspose2d_initialize_sequence(self.convtranspose2d_blocks)\n",
    "    def forward(self, x):\n",
    "        return self.convtranspose2d_blocks(x)\n",
    "    \n",
    "class UpConv3d(nn.Module):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    def __init__(self, final_sz, *args, **kwargs):\n",
    "        super(UpConv3d, self).__init__()\n",
    "        convtranspose3d_blocks = [ convtranspose3d_block( in_f, out_f, *args, **kwargs) \n",
    "                                  for in_f , out_f in zip(final_sz,final_sz[1:])]\n",
    "        self.convtranspose3d_blocks = nn.Sequential( *convtranspose3d_blocks )\n",
    "        convtranspose3d_initialize_sequence(self.convtranspose3d_blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.convtranspose3d_blocks(x)\n",
    "    \n",
    "class FinalClassify3d(nn.Module):    # New Version\n",
    "    \"\"\" Creater a bundle of level\n",
    "    Args:   final_sz = 1D-list [final_sz1,final_sz2,...,final_szN]\n",
    "            res = H and W dimension resolution \n",
    "            *args, **kwargs = up on fully_conv2d\n",
    "    \"\"\"\n",
    "    def __init__(self, final_sz, *args, **kwargs):\n",
    "        super(FinalClassify3d, self).__init__()\n",
    "        classify3d_blocks = [ conv3d_block( in_f, out_f, *args, **kwargs) \n",
    "                           for in_f , out_f in zip(final_sz,final_sz[1:])]\n",
    "        self.classify3d_blocks = nn.Sequential( *classify3d_blocks )\n",
    "        \n",
    "        #conv3d_initialize_sequence(self.classify3d_blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classify3d_blocks(x)\n",
    "'''    \n",
    "class FinalClassify3d(nn.Module):    # Old version\n",
    "    \"\"\" Creater a bundle of level\n",
    "    Args:   final_sz = 1D-list [final_sz1,final_sz2,...,final_szN]\n",
    "            res = H and W dimension resolution \n",
    "            *args, **kwargs = up on fully_conv2d\n",
    "    \"\"\"\n",
    "    def __init__(self, final_sz, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        classify_blocks = [ conv3d_block( in_f, out_f, *args, **kwargs) \n",
    "                           for in_f , out_f in zip(final_sz,final_sz[1:])]\n",
    "        self.classify_blocks = nn.Sequential( *classify_blocks )\n",
    "        conv3d_initialize_sequence(self.classify3d_blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classify_blocks(x)\n",
    "'''\n",
    "\n",
    "class UpsampleHW(nn.Module):\n",
    "    \"\"\" Upsample 3D-volume or 5-dimension data only H & W dimensions \n",
    "        keeping N, C, D dimension as the same\n",
    "    Args:   input = 5-dimensional yorch.tensor\n",
    "            scale_factor = 2, \n",
    "            mode = 'Linear' or 'Bilinear'\n",
    "            align_corners = True or False\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_factor, mode='bilinear', align_corners=True):\n",
    "        super(UpsampleHW, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.align_corners = align_corners\n",
    "        self.upsample = nn.Upsample(scale_factor=self.scale_factor, \n",
    "                                    mode=self.mode, \n",
    "                                    align_corners=self.align_corners )\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 5 , 'Upsample3d: input should be 5D-torch.tensor'\n",
    "        N, C, D, H, W = x.size()\n",
    "        for i in range(C):\n",
    "            if i==0:\n",
    "                x_up = self.upsample(x[:,i,:,:,:]).unsqueeze(dim=1)\n",
    "            else:\n",
    "                x_up = torch.cat( (x_up,self.upsample(x[:,i,:,:,:]).unsqueeze(dim=1)) , dim=1)\n",
    "            #print('x_up = {}   {}'.format(x_up.size(), x_up.dtype))\n",
    "        return x_up\n",
    "        \n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Recon2X3D5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-GPU or DataPallel-Multiple-GPU\n",
    "'''\n",
    "encode_feat1 = list()\n",
    "encode_feat2 = list()\n",
    "decode_feat1 = list()\n",
    "decode_feat2 = list()\n",
    "fusion_feat = list()\n",
    "fusionUp_feat = list()\n",
    "'''\n",
    "class Recon2X3D5(nn.Module):   # Signle GPU  Convolute without Dilation\n",
    "    def __init__(self, in_f, en_sz, de_sz, de3d_sz, final_sz, *args, **kwargs):\n",
    "        super(Recon2X3D5, self).__init__()\n",
    "        assert len(en_sz)== len(de_sz) , 'These input {en_sz} and {de_sz} can not build Recon3DDenseUNet'\n",
    "        \n",
    "        # Prepare feature resolution\n",
    "        input_res = 256\n",
    "        self.res1 = [ round(input_res*0.5**i) for i in range(len(en_sz)) ]\n",
    "        self.res2 = [ round(input_res*0.5**i) for i in range(len(en_sz)) ]\n",
    "        self.res3 = [ round(input_res*0.5**i) for i in range(len(en_sz)) ]\n",
    "        self.res1.reverse()\n",
    "        self.res2.pop(-1)\n",
    "        self.res2.reverse()\n",
    "        self.res3.pop(-1)\n",
    "        self.res3.pop(-1)\n",
    "        self.res3.reverse()\n",
    "        self.res4 = self.res1.copy()\n",
    "        self.res4.reverse()\n",
    "        self.res4.pop(0)\n",
    "        '''print('self.res1 = {}'.format(self.res1))\n",
    "        print('self.res2 = {}'.format(self.res2))\n",
    "        print('self.res3 = {}'.format(self.res3))\n",
    "        print('self.res4 = {}\\n'.format(self.res4))'''\n",
    "        \n",
    "        self.en_sz = en_sz\n",
    "        #print('en_sz = {}\\n'.format(self.en_sz))\n",
    "        \n",
    "        cat1 = [ x[0]+x[1]*x[2] for x in en_sz]\n",
    "        cat1.reverse()\n",
    "        cat1.pop(0)\n",
    "        #cat2 = [ en_sz[-1][0]+en_sz[-1][1]*en_sz[-1][2] , *[ x[-1] for x in de_sz ] ]\n",
    "        cat2 = [ x[-1] for x in de_sz ]\n",
    "        cat = [ x1+x2 for x1,x2 in zip(cat1,cat2) ]\n",
    "        cat = [en_sz[-1][0]+en_sz[-1][1]*en_sz[-1][2], *cat]\n",
    "        '''print('cat1 = {}'.format(cat1))\n",
    "        print('cat2 = {}'.format(cat2))\n",
    "        print('cat = {}\\n'.format(cat))'''\n",
    "        \n",
    "        self.de_sz = [ [x1,*x2] for x1,x2 in zip(cat,de_sz)]\n",
    "        #print('self.de_sz = {}\\n'.format(self.de_sz))\n",
    "        \n",
    "        self.de3d_sz = de3d_sz\n",
    "        #print('self.de3d_sz = {}\\n'.format(self.de3d_sz))\n",
    "        \n",
    "        self.final_sz = final_sz\n",
    "        #print('self.final_sz = {}\\n'.format(self.final_sz))\n",
    "        \n",
    "        ############################################\n",
    "        # Class Layer description : MyDenseLayer >> MyDecoder >> UpConv2d >> MyDecoder3d >> UpConv3d >> nn.Conv3d\n",
    "        # Starting layer\n",
    "        self.first_layer1 = nn.Conv2d(in_f, en_sz[0][0], kernel_size=3, stride=1, padding=1)\n",
    "        self.first_layer2 = nn.Conv2d(in_f, en_sz[0][0], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Dense Connection Encode layer\n",
    "        self.dense_layer1 = nn.ModuleList([ MyDenseLayer(en_sz[i][0],en_sz[i][1],en_sz[i][2], \n",
    "                                                         kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(en_sz))] )\n",
    "        self.dense_layer2 = nn.ModuleList([ MyDenseLayer(en_sz[i][0],en_sz[i][1],en_sz[i][2], \n",
    "                                                         kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(en_sz))] )\n",
    "        # Pooling2D\n",
    "        self.pool2d = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, return_indices=True)\n",
    "        #self.adaptpool2d = nn.ModuleList([ nn.AdaptiveMaxPool2d((x,x)) for x in self.res4])\n",
    "        \n",
    "        # Decode layer for 2D\n",
    "        self.decode_layer1 = nn.ModuleList([ MyDecoder(self.de_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(self.de_sz))] )\n",
    "        self.decode_layer2 = nn.ModuleList([ MyDecoder(self.de_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(self.de_sz))] )\n",
    "\n",
    "        # UpConv2d for Decoder\n",
    "        self.upconv2d1 = nn.ModuleList( [ UpConv2d([self.de_sz[i][-1],self.de_sz[i][-1]], self.res1[i], \n",
    "                                                   kernel_size=2, stride=2, padding=0) \n",
    "                                          for i in range(len(self.de_sz)) ] )\n",
    "        self.upconv2d2 = nn.ModuleList( [ UpConv2d([self.de_sz[i][-1],self.de_sz[i][-1]], self.res1[i], \n",
    "                                                   kernel_size=2, stride=2, padding=0) \n",
    "                                          for i in range(len(self.de_sz)) ] )\n",
    "        \n",
    "        # for 3D connection pyramid for Fusion\n",
    "        #   1) for concatenation and averaging\n",
    "        self.final_layer = nn.ModuleList([ MyDecoder3d(self.final_sz[i], self.res1[i], \n",
    "                                                       kernel_size=3, stride=1, padding=1) \n",
    "                                         for i in range(len(self.final_sz))])\n",
    "        #   2) Custom {kernel_size, stride, padding}\n",
    "        '''self.final_layer = nn.ModuleList([ MyDecoder3d(self.final_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1 ) \n",
    "                                         for i in range(len(self.final_sz)-1)])\n",
    "        self.final_layer.append( MyDecoder3d(self.final_sz[-1], self.res1[-1], kernel_size=1, stride=1, padding=0) )'''\n",
    "        #   3) Custom last final_layer\n",
    "        '''self.final_layer = nn.ModuleList([ MyDecoder3d(self.final_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1 ) \n",
    "                                         for i in range(len(self.final_sz)-1)])\n",
    "        last_final_layer = nn.Sequential(nn.Conv3d(self.final_sz[-1][0],self.final_sz[-1][1],kernel_size=2,stride=1,padding=1),\n",
    "                                         nn.InstanceNorm3d(self.final_sz[-1][1]),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv3d(self.final_sz[-1][1],self.final_sz[-1][2],kernel_size=2,stride=1,padding=0),\n",
    "                                         nn.InstanceNorm3d(self.final_sz[-1][2]),\n",
    "                                         nn.ReLU(),\n",
    "                                        )\n",
    "        self.final_layer.append( last_final_layer )'''\n",
    "        \n",
    "        # UpConv3d for Fusion\n",
    "        self.upconv3d = nn.ModuleList( [ UpConv3d([self.final_sz[i][-1], 3], self.res2[i], \n",
    "                                                  kernel_size=2, stride=2, padding=0) \n",
    "                                          for i in range(len(self.de_sz)-1) ] )   # upsample to 1-volume\n",
    "        '''self.upconv3d = nn.ModuleList( [ UpConv3d( [self.final_sz[i][-1],4], self.res2[i], \n",
    "                                                      kernel_size=2, stride=2, padding=0) \n",
    "                                          for i in range(len(self.de_sz)-1) ] )  # upsample to N-sub-volumes'''\n",
    "        # Final classification by Conv3d(1x1x1)\n",
    "        self.final_layer2 = nn.Sequential(nn.Conv3d(self.final_sz[-1][-1], 3, kernel_size=1, stride=1, padding=0),\n",
    "                                          nn.Softmax(dim=1) )  # Change output channel to {2,3} upto ToTensor version\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        #print('\\n Running Recon2X3D \\n')\n",
    "        #print('\\n--- Encode loop ---')\n",
    "        x1 = self.first_layer1(x1)\n",
    "        x2 = self.first_layer2(x2)\n",
    "        encode_trace1 = []\n",
    "        encode_trace2 = []\n",
    "        i = 0\n",
    "        for layer1 , layer2 in zip(self.dense_layer1 , self.dense_layer2):   # for loop on each nn.Sequential layer\n",
    "            x1 = layer1(x1)         # layer = each dense blocks\n",
    "            x2 = layer2(x2)         # layer = each dense blocks\n",
    "            #print('Encode1 layer:{}   |  h1 = {}   {}   {}'.format(i, x1.size(), x1.dtype, x1.get_device()))\n",
    "            #print('Encode2 layer:{}   |  h2 = {}   {}   {}'.format(i, x2.size(), x2.dtype, x2.get_device()))\n",
    "            encode_trace1.append(x1)    # trace x for concatenation with decode layer\n",
    "            encode_trace2.append(x2)    # trace x for concatenation with decode layer\n",
    "            #encode_feat1.append(x1)\n",
    "            #encode_feat2.append(x2)\n",
    "            if i != len(self.dense_layer1)-1:\n",
    "                x1 , _ = self.pool2d(x1)        # for MaxPool2d or AvgPool2d\n",
    "                x2 , _ = self.pool2d(x2)        # for MaxPool2d or AvgPool2d\n",
    "                #x1 = self.adaptpool2d[i](x1)        # for AdaptivePooling\n",
    "                #x2 = self.adaptpool2d[i](x2)        # for AdaptivePooling\n",
    "                #print('                  |  h1_pool = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "                #print('                  |  h2_pool = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "            i += 1\n",
    "        #[print('encode_trace1 = {}'.format(x.size())) for x in encode_trace1]\n",
    "        #[print('encode_trace2 = {}'.format(x.size())) for x in encode_trace2]\n",
    "        \n",
    "        \n",
    "        #print('\\n--- Decode loop and Fusion loop ---')\n",
    "        res = [ round(x1[-1]/x2) for x1,x2 in zip(self.de_sz,self.res1) ]\n",
    "        #print('inside loop res = {}'.format(res))\n",
    "        i = 0\n",
    "        for layer1,layer2,fusionlayer in zip(self.decode_layer1, self.decode_layer2, self.final_layer):\n",
    "            #print('Level: {}'.format(i))\n",
    "            if i==0:\n",
    "                x1 = encode_trace1[len(self.dense_layer1)-1-i]\n",
    "                x2 = encode_trace2[len(self.dense_layer2)-1-i]\n",
    "            else:   # encoder and decoder fusion\n",
    "                x1 = torch.cat( (encode_trace1[len(self.dense_layer1)-1-i],x1), dim=1)\n",
    "                x2 = torch.cat( (encode_trace2[len(self.dense_layer1)-1-i],x2), dim=1)\n",
    "            N,C,H,W = x1.size()\n",
    "            #print('            |  x1_cat = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "            #print('            |  x2_cat = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "            x1 = layer1(x1)\n",
    "            x2 = layer2(x2)\n",
    "            #print('            |  x1_conv2d = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "            #print('            |  x2_conv2d = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "            \n",
    "            # extract feature come out from final conv2d encoder-decoder\n",
    "            #decode_feat1.append(x1)\n",
    "            #decode_feat2.append(x2)\n",
    "            \n",
    "            # Covert to 3D volumes or 5D-Tensor\n",
    "            X1 = x1.view(-1,res[i],H,H,W)\n",
    "            X2 = x2.view(-1,res[i],H,H,W).transpose(4,2).flip(4)\n",
    "            #print('            |  X1_view3d = {}   {}   {}'.format(X1.size(), X1.dtype, X1.get_device()))\n",
    "            #print('            |  X2_view3d = {}   {}   {}'.format(X2.size(), X2.dtype, X2.get_device()))\n",
    "            \n",
    "            # 3D volume fusion (averaging)\n",
    "            '''if i==0:\n",
    "                X = (X1+X2)/2\n",
    "            else:\n",
    "                X = (X1+X2+X)/3'''\n",
    "                \n",
    "            # 3D volume fusion (concatenation)\n",
    "            if i==0:\n",
    "                #X = torch.cat((X1,X2,(X1*X2)), dim=1)   # not working  21011802\n",
    "                #X = torch.cat((X1,X2,(X1+X2)/2,(X1*X2)), dim=1)\n",
    "                #X = torch.cat(((X1+X2)/2,(X1*X2)), dim=1)\n",
    "                #X = torch.cat((X1**2,X2**2,(X1+X2)/2,(X1*X2)), dim=1)\n",
    "                X = torch.cat((X1,X2), dim=1)\n",
    "            else:\n",
    "                #X = torch.cat((X1,X2,(X1*X2),X), dim=1)  # not working  21011802\n",
    "                #X = (torch.cat((X1,X2,(X1+X2)/2,(X1*X2)), dim=1) + X)/2\n",
    "                #X = (torch.cat(((X1+X2)/2,(X1*X2)), dim=1) + X )/2\n",
    "                #X = (torch.cat((X1**2,X2**2,(X1+X2)/2,(X1*X2)), dim=1) + X)/2\n",
    "                X = torch.cat((X1,X2,X), dim=1)\n",
    "            \n",
    "            #print('            |  X_cat3d = {}   {}   {}'.format(X.size(), X.dtype, X.get_device()))\n",
    "            X = fusionlayer(X)\n",
    "            #print('            |  X_fusion3d = {}   {}   {}'.format(X.size(), X.dtype, X.get_device()))\n",
    "            #fusion_feat.append(X)\n",
    "            if i!=len(self.decode_layer1)-1:\n",
    "                x1 = self.upconv2d1[i](x1)\n",
    "                x2 = self.upconv2d1[i](x2)\n",
    "                X = self.upconv3d[i](X)\n",
    "                #fusionUp_feat.append(X)\n",
    "                #print('               |  x1_upconv2d = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "                #print('               |  x2_upconv2d = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "                #print('               |  X_upconv3d = {}   {}   {}'.format(X.size(), X.dtype, X.get_device()))\n",
    "            i+=1\n",
    "        #print('\\n--- Final classify ---')\n",
    "        X = self.final_layer2(X)\n",
    "        #print('   X final = {}   {}   {} \\n'.format(X.size(), X.dtype, X.get_device()))\n",
    "        return X\n",
    "        \n",
    "def test_Recon2X3D5():\n",
    "    torch.cuda.empty_cache()\n",
    "    #en_sz = [[in_f,res,k,layer],[k*layer,res2,k,layer]]\n",
    "    in_c = 1\n",
    "    # Averaging AP & LAT features\n",
    "    '''en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "    de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "    final_sz = [[1,32,32],[1,32,32],[1,32,32],[1,16,16],[1,16,16],[1,16,16],[1,16,16]] # 2-fusion layer'''\n",
    "    \n",
    "    # Concatenation AP & LAT features\n",
    "    en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "    de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]   # work\n",
    "    final_sz = [[2,16,16],[3,16,16],[3,16,16,16],[3,16,16],[3,16,16],[3,16,16],[3,16,16]]\n",
    "    \n",
    "    # Concatenation from AP & LAT sub-feature-volumes\n",
    "    '''en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "    de_sz = [[64,4*8],[128,8*8],[128,16*4],[256,32*4],[256,64*2],[256,128*2],[256,256]] \n",
    "    final_sz = [[16,32,32],[16+4,32,32],[8+4,32,32],[8+4,16,16],[4+4,16,16],[4+4,16,16],[2+4,16,16]]'''\n",
    "    \n",
    "    de3d_sz = [[8,4],[16,8],[32,16],[64,32],[128,64],[256,128],[512,256]]\n",
    "    model = Recon2X3D5( in_c, en_sz, de_sz, de3d_sz, final_sz).to(device=device)\n",
    "    model = nn.DataParallel(model)\n",
    "    #print(model,'\\n')\n",
    "    #print(model.__dict__.keys())\n",
    "    n = 3\n",
    "    x1 = torch.randn(n,1,256,256).to(device=device)\n",
    "    x2 = torch.randn(n,1,256,256).to(device=device)\n",
    "    #print('x = {}'.format(x.size()))\n",
    "    time1 = time.time()\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        output = model(x1,x2)\n",
    "        #torch.cuda.synchronize()\n",
    "    time2 = time.time()\n",
    "    print('\\n Total running time = {} sec. \\n'.format(time2-time1))\n",
    "    assert output.size()==torch.Size([n,3,256,256,256]) , 'output size error!'\n",
    "    \n",
    "#test_Recon2X3D5()\n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Recon2X3D6 (Axial + Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Recon2X3D6(nn.Module):   # Signle GPU  Convolute without Dilation\n",
    "    def __init__(self, in_f, en_sz, de_sz, de3d_sz, final_sz, *args, **kwargs):\n",
    "        super(Recon2X3D6, self).__init__()\n",
    "        assert len(en_sz)== len(de_sz) , 'These input {en_sz} and {de_sz} can not build Recon3DDenseUNet'\n",
    "        \n",
    "        # Prepare feature resolution\n",
    "        input_res = 256\n",
    "        self.res1 = [ round(input_res*0.5**i) for i in range(len(en_sz)) ]\n",
    "        self.res2 = [ round(input_res*0.5**i) for i in range(len(en_sz)) ]\n",
    "        self.res3 = [ round(input_res*0.5**i) for i in range(len(en_sz)) ]\n",
    "        self.res1.reverse()\n",
    "        self.res2.pop(-1)\n",
    "        self.res2.reverse()\n",
    "        self.res3.pop(-1)\n",
    "        self.res3.pop(-1)\n",
    "        self.res3.reverse()\n",
    "        self.res4 = self.res1.copy()\n",
    "        self.res4.reverse()\n",
    "        self.res4.pop(0)\n",
    "        '''print('self.res1 = {}'.format(self.res1))\n",
    "        print('self.res2 = {}'.format(self.res2))\n",
    "        print('self.res3 = {}'.format(self.res3))\n",
    "        print('self.res4 = {}\\n'.format(self.res4))'''\n",
    "        \n",
    "        self.en_sz = en_sz\n",
    "        cat1 = [ x[0]+x[1]*x[2] for x in en_sz]\n",
    "        cat1.reverse()\n",
    "        cat1.pop(0)\n",
    "        #cat2 = [ en_sz[-1][0]+en_sz[-1][1]*en_sz[-1][2] , *[ x[-1] for x in de_sz ] ]\n",
    "        cat2 = [ x[-1] for x in de_sz ]\n",
    "        cat = [ x1+x2 for x1,x2 in zip(cat1,cat2) ]\n",
    "        cat = [en_sz[-1][0]+en_sz[-1][1]*en_sz[-1][2], *cat]\n",
    "        '''print('cat1 = {}'.format(cat1))\n",
    "        print('cat2 = {}'.format(cat2))\n",
    "        print('cat = {}\\n'.format(cat))'''\n",
    "        \n",
    "        self.de_sz = [ [x1,*x2] for x1,x2 in zip(cat,de_sz)]\n",
    "        self.de3d_sz = de3d_sz\n",
    "        self.final_sz = final_sz\n",
    "        \n",
    "        ############################################\n",
    "        # Class Layer description : MyDenseLayer >> MyDecoder >> UpConv2d >> MyDecoder3d >> UpConv3d >> nn.Conv3d\n",
    "        # Starting layer\n",
    "        self.first_layer1 = nn.Conv2d(in_f, en_sz[0][0], kernel_size=3, stride=1, padding=1)\n",
    "        self.first_layer2 = nn.Conv2d(in_f, en_sz[0][0], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Dense Connection Encode layer\n",
    "        self.dense_layer1 = nn.ModuleList([ MyDenseLayer(en_sz[i][0],en_sz[i][1],en_sz[i][2], \n",
    "                                                         kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(en_sz))] )\n",
    "        self.dense_layer2 = nn.ModuleList([ MyDenseLayer(en_sz[i][0],en_sz[i][1],en_sz[i][2], \n",
    "                                                         kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(en_sz))] )\n",
    "        # Pooling2D\n",
    "        self.pool2d = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, return_indices=True)\n",
    "        #self.adaptpool2d = nn.ModuleList([ nn.AdaptiveMaxPool2d((x,x)) for x in self.res4])\n",
    "        \n",
    "        # Decode layer for 2D\n",
    "        self.decode_layer1 = nn.ModuleList([ MyDecoder(self.de_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(self.de_sz))] )\n",
    "        self.decode_layer2 = nn.ModuleList([ MyDecoder(self.de_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(self.de_sz))] )\n",
    "\n",
    "        # UpConv2d for Decoder\n",
    "        self.upconv2d1 = nn.ModuleList( [ UpConv2d([self.de_sz[i][-1],self.de_sz[i][-1]], self.res1[i], \n",
    "                                                   kernel_size=2, stride=2, padding=0) \n",
    "                                          for i in range(len(self.de_sz)) ] )\n",
    "        self.upconv2d2 = nn.ModuleList( [ UpConv2d([self.de_sz[i][-1],self.de_sz[i][-1]], self.res1[i], \n",
    "                                                   kernel_size=2, stride=2, padding=0) \n",
    "                                          for i in range(len(self.de_sz)) ] )\n",
    "        \n",
    "        # AxialFusion\n",
    "        self.axialD = nn.ModuleList([ AxialFusion(self.de3d_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        self.axialH = nn.ModuleList([ AxialFusion(self.de3d_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        self.axialW = nn.ModuleList([ AxialFusion(self.de3d_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        \n",
    "        # for 3D connection pyramid for 3D-Fusion\n",
    "        self.final_layer = nn.ModuleList([ MyDecoder3d(self.final_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                          for i in range(len(self.final_sz))])\n",
    "        \n",
    "        # UpConv3d for 3D-Fusion\n",
    "        self.upconv3d = nn.ModuleList( [ UpConv3d([self.final_sz[i][-1], 3], self.res2[i], \n",
    "                                                   kernel_size=2, stride=2, padding=0) \n",
    "                                          for i in range(len(self.de_sz)-1) ] )   # upsample to 1-volume\n",
    "        '''self.upconv3d = nn.ModuleList( [ UpConv3d( [self.final_sz[i][-1],4], self.res2[i], \n",
    "                                                      kernel_size=2, stride=2, padding=0) \n",
    "                                           for i in range(len(self.de_sz)-1) ] )   # upsample to N-sub-volumes'''\n",
    "                                         \n",
    "        # Final classification by Conv3d(1x1x1)\n",
    "        self.final_layer2 = nn.Sequential(nn.Conv3d(self.final_sz[-1][-1], 3, kernel_size=1, stride=1, padding=0),\n",
    "                                          nn.Softmax(dim=1) )  # Change output channel to {2,3} upto ToTensor version\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        #print('\\n Running Recon2X3D6 \\n')\n",
    "        #print('\\n--- Encode loop ---')\n",
    "        x1 = self.first_layer1(x1)\n",
    "        x2 = self.first_layer2(x2)\n",
    "        encode_trace1 = []\n",
    "        encode_trace2 = []\n",
    "        i = 0\n",
    "        for layer1 , layer2 in zip(self.dense_layer1 , self.dense_layer2):   # for loop on each nn.Sequential layer\n",
    "            x1 = layer1(x1)         # layer = each dense blocks\n",
    "            x2 = layer2(x2)         # layer = each dense blocks\n",
    "            #print('Encode1 layer:{}   |  h1 = {}   {}   {}'.format(i, x1.size(), x1.dtype, x1.get_device()))\n",
    "            #print('Encode2 layer:{}   |  h2 = {}   {}   {}'.format(i, x2.size(), x2.dtype, x2.get_device()))\n",
    "            encode_trace1.append(x1)    # trace x for concatenation with decode layer\n",
    "            encode_trace2.append(x2)    # trace x for concatenation with decode layer\n",
    "            if i != len(self.dense_layer1)-1:\n",
    "                x1 , _ = self.pool2d(x1)        # for MaxPool2d or AvgPool2d\n",
    "                x2 , _ = self.pool2d(x2)        # for MaxPool2d or AvgPool2d\n",
    "                #x1 = self.adaptpool2d[i](x1)        # for AdaptivePooling\n",
    "                #x2 = self.adaptpool2d[i](x2)        # for AdaptivePooling\n",
    "                #print('                  |  h1_pool = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "                #print('                  |  h2_pool = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "            i += 1\n",
    "        #[print('encode_trace1 = {}'.format(x.size())) for x in encode_trace1]\n",
    "        #[print('encode_trace2 = {}'.format(x.size())) for x in encode_trace2]\n",
    "        \n",
    "        #print('\\n--- Decode loop and Fusion loop ---')\n",
    "        res = [ round(x1[-1]/x2) for x1,x2 in zip(self.de_sz,self.res1) ]\n",
    "        #print('inside loop res = {}'.format(res))\n",
    "        i = 0\n",
    "        for layer1,layer2,axialD,axialH,axialW,fusion3d in zip(self.decode_layer1, self.decode_layer2,\n",
    "                                                               self.axialD ,self.axialH, self.axialW,\n",
    "                                                               self.final_layer):\n",
    "            #print('Level: {}'.format(i))\n",
    "            if i==0:\n",
    "                x1 = encode_trace1[len(self.dense_layer1)-1-i]\n",
    "                x2 = encode_trace2[len(self.dense_layer2)-1-i]\n",
    "            else:   # encoder and decoder fusion\n",
    "                x1 = torch.cat( (encode_trace1[len(self.dense_layer1)-1-i] , x1) , dim=1)\n",
    "                x2 = torch.cat( (encode_trace2[len(self.dense_layer1)-1-i] , x2) , dim=1)\n",
    "            N,C,H,W = x1.size()\n",
    "            #print('            |  x1_cat = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "            #print('            |  x2_cat = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "            x1 = layer1(x1)\n",
    "            x2 = layer2(x2)\n",
    "            #print('            |  x1_conv2d = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "            #print('            |  x2_conv2d = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "            \n",
    "            # extract feature come out from final conv2d encoder-decoder\n",
    "            #feature1.append(x1)\n",
    "            #feature2.append(x2)\n",
    "            \n",
    "            # Axial fusion along D, H, W\n",
    "            xd = torch.cat( (x1,x2.transpose(3,1).flip(3)), dim=1 )                  # N,2C,H,W >>[Conv2d]>> N,C,H,W\n",
    "            xh = torch.cat( (x1,x2.transpose(3,1).flip(3)), dim=2 ).transpose(2,1)   # N,C,2H,W >> N,2H,C,W >>[Conv2d]>> N,H,C,W >> N,C,H,W\n",
    "            xw = torch.cat( (x1,x2.transpose(3,1).flip(3)), dim=3 ).transpose(3,1)   # N,C,H,2W >> N,C,H,2W >>[Conv2d]>> N,W,H,C >> N,C,H,W\n",
    "            #print('            |  xd_cat = {}   {}   {}'.format(xd.size(), xd.dtype, xd.get_device()))\n",
    "            #print('            |  xh_cat = {}   {}   {}'.format(xh.size(), xh.dtype, xh.get_device()))\n",
    "            #print('            |  xw_cat = {}   {}   {}'.format(xw.size(), xw.dtype, xw.get_device()))\n",
    "            xd = axialD(xd)\n",
    "            xh = axialH(xh).transpose(2,1)    # axialD(xh).transpose(2,1)\n",
    "            xw = axialW(xw).transpose(3,1)    # axialD(xh).transpose(3,1)\n",
    "            #print('            |  xd_axial = {}   {}   {}'.format(xd.size(), xd.dtype, xd.get_device()))\n",
    "            #print('            |  xh_axial = {}   {}   {}'.format(xh.size(), xh.dtype, xh.get_device()))\n",
    "            #print('            |  xw_axial = {}   {}   {}'.format(xw.size(), xw.dtype, xw.get_device()))\n",
    "            \n",
    "            # Covert to 3D volumes\n",
    "            XD = xd.view(-1, res[i], H, H, W)\n",
    "            XH = xh.view(-1, res[i], H, H, W)\n",
    "            XW = xw.view(-1, res[i], H, H, W)\n",
    "            #print('            |  XD_view = {}   {}   {}'.format(XD.size(), XD.dtype, XD.get_device()))\n",
    "            #print('            |  XH_view = {}   {}   {}'.format(XH.size(), XH.dtype, XH.get_device()))\n",
    "            #print('            |  XW_view = {}   {}   {}'.format(XW.size(), XW.dtype, XW.get_device()))\n",
    "            \n",
    "            # Fusion\n",
    "            #print('\\n Fusion loop')\n",
    "            if i == 0:\n",
    "                X = torch.cat( (XD,XH,XW), dim=1 )                # cat everything  and  # avg of cat\n",
    "                #X = (XD + XH + XW)/3                              # avg everything\n",
    "            else:\n",
    "                X = torch.cat( (XD,XH,XW,X), dim=1 )               # cat everything\n",
    "                #X = (X + XD + XH + XW)/4                           # avg everything\n",
    "                #X = ( torch.cat( (XD,XH,XW), dim=1 ) + X )/2       # avg of cat\n",
    "            #print('            |  X_cat3d = {}   {}   {}'.format(X.size(), X.dtype, X.get_device()))\n",
    "            \n",
    "            X = fusion3d(X)\n",
    "            #print('            |  X_fusion3d = {}   {}   {}'.format(X.size(), X.dtype, X.get_device()))\n",
    "            if i!=len(self.decode_layer1)-1:\n",
    "                x1 = self.upconv2d1[i](x1)\n",
    "                x2 = self.upconv2d1[i](x2)\n",
    "                X = self.upconv3d[i](X)\n",
    "                #print('               |  x1_upconv2d = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "                #print('               |  x2_upconv2d = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "                #print('               |  X_upconv3d = {}   {}   {}'.format(X.size(), X.dtype, X.get_device()))\n",
    "            i+=1\n",
    "            \n",
    "        ##print('\\n--- Final classify ---')\n",
    "        X = self.final_layer2(X)\n",
    "        #print('   X final = {}   {}   {} \\n'.format(X.size(), X.dtype, X.get_device()))\n",
    "        return X\n",
    "        \n",
    "def test_Recon2X3D6():\n",
    "    torch.cuda.empty_cache()\n",
    "    # Axial + Fusion of AP & LAT features\n",
    "    in_c = 1\n",
    "    en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "    de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "    de3d_sz = [[8,8,4],[16,16,8],[32,32,16],[64,64,32],[128,128,64],[256,256,128],[512,512,256]] # 2-axial layer per level\n",
    "    final_sz = [[3,32,32],[6,32,32],[6,32,32],[6,16,16],[6,16,16],[6,16,16],[6,16,16]] # 2-fusion layer per level\n",
    "    model = Recon2X3D6( in_c, en_sz, de_sz, de3d_sz, final_sz).to(device=device)\n",
    "    #model = nn.DataParallel(model)\n",
    "    #print(model,'\\n')\n",
    "    #print(model.__dict__.keys())\n",
    "    n = 1\n",
    "    x1 = torch.randn(n,1,256,256).to(device=device)\n",
    "    x2 = torch.randn(n,1,256,256).to(device=device)\n",
    "    time1 = time.time()\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        output = model(x1,x2)\n",
    "        #torch.cuda.synchronize()\n",
    "    time2 = time.time()\n",
    "    assert output.size()==torch.Size([n,3,256,256,256]) , 'output size error!'\n",
    "    print('\\n Total running time = {} sec. \\n'.format(time2-time1))\n",
    "    \n",
    "#test_Recon2X3D6()\n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Recon2X3D7 (Axial+Inception+Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Recon2X3D7(nn.Module):\n",
    "    ##### AxialInceptionFusion of AP & LAT features #####\n",
    "    def __init__(self, in_f, en_sz, de_sz, de3d_sz, final_sz, *args, **kwargs):\n",
    "        super(Recon2X3D7, self).__init__()\n",
    "        assert len(en_sz)== len(de_sz) , 'These input {en_sz} and {de_sz} can not build Recon3DDenseUNet'\n",
    "        \n",
    "        # Prepare feature resolution\n",
    "        input_res = 256\n",
    "        self.res1 = [ round(input_res*0.5**i) for i in range(len(en_sz)) ]\n",
    "        self.res2 = [ round(input_res*0.5**i) for i in range(len(en_sz)) ]\n",
    "        self.res3 = [ round(input_res*0.5**i) for i in range(len(en_sz)) ]\n",
    "        self.res1.reverse()\n",
    "        self.res2.pop(-1)\n",
    "        self.res2.reverse()\n",
    "        self.res3.pop(-1)\n",
    "        self.res3.pop(-1)\n",
    "        self.res3.reverse()\n",
    "        self.res4 = self.res1.copy()\n",
    "        self.res4.reverse()\n",
    "        self.res4.pop(0)\n",
    "        print('self.res1 = {}'.format(self.res1))\n",
    "        print('self.res2 = {}'.format(self.res2))\n",
    "        print('self.res3 = {}'.format(self.res3))\n",
    "        print('self.res4 = {}\\n'.format(self.res4))\n",
    "        \n",
    "        self.en_sz = en_sz\n",
    "        #print('en_sz = {}\\n'.format(self.en_sz))\n",
    "        cat1 = [ x[0]+x[1]*x[2] for x in en_sz]\n",
    "        cat1.reverse()\n",
    "        cat1.pop(0)\n",
    "        #cat2 = [ en_sz[-1][0]+en_sz[-1][1]*en_sz[-1][2] , *[ x[-1] for x in de_sz ] ]\n",
    "        cat2 = [ x[-1] for x in de_sz ]\n",
    "        cat = [ x1+x2 for x1,x2 in zip(cat1,cat2) ]\n",
    "        cat = [en_sz[-1][0]+en_sz[-1][1]*en_sz[-1][2], *cat]\n",
    "        '''print('cat1 = {}'.format(cat1))\n",
    "        print('cat2 = {}'.format(cat2))\n",
    "        print('cat = {}\\n'.format(cat))'''\n",
    "        \n",
    "        self.de_sz = [ [x1,*x2] for x1,x2 in zip(cat,de_sz)]\n",
    "        #print('self.de_sz = {}\\n'.format(self.de_sz))\n",
    "        self.de3d_sz = de3d_sz\n",
    "        print('self.de3d_sz = {}\\n'.format(self.de3d_sz))\n",
    "        self.final_sz = final_sz\n",
    "        #print('self.final_sz = {}\\n'.format(self.final_sz))\n",
    "        \n",
    "        ############################################\n",
    "        # Class Layer description : MyDenseLayer >> MyDecoder >> UpConv2d >> MyDecoder3d >> UpConv3d >> nn.Conv3d\n",
    "        # Starting layer\n",
    "        self.first_layer1 = nn.Conv2d(in_f, en_sz[0][0], kernel_size=3, stride=1, padding=1)\n",
    "        self.first_layer2 = nn.Conv2d(in_f, en_sz[0][0], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Dense Connection Encode layer\n",
    "        self.dense_layer1 = nn.ModuleList([ MyDenseLayer(en_sz[i][0],en_sz[i][1],en_sz[i][2], \n",
    "                                                         kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(en_sz))] )\n",
    "        self.dense_layer2 = nn.ModuleList([ MyDenseLayer(en_sz[i][0],en_sz[i][1],en_sz[i][2], \n",
    "                                                         kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(en_sz))] )\n",
    "        # Pooling2D\n",
    "        self.pool2d = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, return_indices=True)\n",
    "        #self.adaptpool2d = nn.ModuleList([ nn.AdaptiveMaxPool2d((x,x)) for x in self.res4])\n",
    "        \n",
    "        # Decode layer for 2D\n",
    "        self.decode_layer1 = nn.ModuleList([ MyDecoder(self.de_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(self.de_sz))] )\n",
    "        self.decode_layer2 = nn.ModuleList([ MyDecoder(self.de_sz[i], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                           for i in range(len(self.de_sz))] )\n",
    "\n",
    "        # UpConv2d for Decoder\n",
    "        self.upconv2d1 = nn.ModuleList( [ UpConv2d([self.de_sz[i][-1],self.de_sz[i][-1]], self.res1[i], \n",
    "                                                   kernel_size=2, stride=2, padding=0) \n",
    "                                          for i in range(len(self.de_sz)) ] )\n",
    "        self.upconv2d2 = nn.ModuleList( [ UpConv2d([self.de_sz[i][-1],self.de_sz[i][-1]], self.res1[i], \n",
    "                                                   kernel_size=2, stride=2, padding=0) \n",
    "                                          for i in range(len(self.de_sz)) ] )\n",
    "        \n",
    "        # AxialInceptionFusion\n",
    "        self.inceptPool2d = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.axialD1 = nn.ModuleList([ AxialFusion(self.de3d_sz[i][0:2], self.res1[i], kernel_size=1, stride=1, padding=0) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        self.axialH1 = nn.ModuleList([ AxialFusion(self.de3d_sz[i][0:2], self.res1[i], kernel_size=1, stride=1, padding=0) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        self.axialW1 = nn.ModuleList([ AxialFusion(self.de3d_sz[i][0:2], self.res1[i], kernel_size=1, stride=1, padding=0) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        self.axialD3 = nn.ModuleList([ AxialFusion(self.de3d_sz[i][0:2], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        self.axialH3 = nn.ModuleList([ AxialFusion(self.de3d_sz[i][0:2], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        self.axialW3 = nn.ModuleList([ AxialFusion(self.de3d_sz[i][0:2], self.res1[i], kernel_size=3, stride=1, padding=1) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        self.axialD5 = nn.ModuleList([ AxialFusion(self.de3d_sz[i][0:2], self.res1[i], kernel_size=5, stride=1, padding=2) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        self.axialH5 = nn.ModuleList([ AxialFusion(self.de3d_sz[i][0:2], self.res1[i], kernel_size=5, stride=1, padding=2) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        self.axialW5 = nn.ModuleList([ AxialFusion(self.de3d_sz[i][0:2], self.res1[i], kernel_size=5, stride=1, padding=2) \n",
    "                                      for i in range(len(self.de3d_sz))])\n",
    "        test = [self.de3d_sz[0][1]*5,self.de3d_sz[0][-1] ]\n",
    "        print(test)\n",
    "        self.axialFusionD = nn.ModuleList([ AxialFusion([self.de3d_sz[i][1]*5,self.de3d_sz[i][-1]], self.res1[i], kernel_size=1, stride=1, padding=0) \n",
    "                                           for i in range(len(self.de3d_sz)) ] )\n",
    "        self.axialFusionH = nn.ModuleList([ AxialFusion([self.de3d_sz[i][1]*5,self.de3d_sz[i][-1]], self.res1[i], kernel_size=1, stride=1, padding=0)\n",
    "                                           for i in range(len(self.de3d_sz)) ] )\n",
    "        self.axialFusionW = nn.ModuleList([ AxialFusion([self.de3d_sz[i][1]*5,self.de3d_sz[i][-1]], self.res1[i], kernel_size=1, stride=1, padding=0) \n",
    "                                           for i in range(len(self.de3d_sz)) ] )\n",
    "        \n",
    "        # for 3D connection pyramid for 3D-Fusion\n",
    "        self.final_layer = nn.ModuleList([ MyDecoder3d(self.final_sz[i], self.res1[i], kernel_size=1, stride=1, padding=0) \n",
    "                                          for i in range(len(self.final_sz))])\n",
    "        \n",
    "        # UpConv3d for 3D-Fusion\n",
    "        self.upconv3d = nn.ModuleList( [ UpConv3d([self.final_sz[i][-1], 3], self.res2[i], \n",
    "                                                  kernel_size=2, stride=2, padding=0) \n",
    "                                          for i in range(len(self.de_sz)-1) ] )   # upsample to 1-volume\n",
    "        '''self.upconv3d = nn.ModuleList( [ UpConv3d( [self.final_sz[i][-1],4], self.res2[i], \n",
    "                                                      kernel_size=2, stride=2, padding=0) \n",
    "                                           for i in range(len(self.de_sz)-1) ] )   # upsample to N-sub-volumes'''\n",
    "                                         \n",
    "        # Final classification by Conv3d(1x1x1)\n",
    "        self.final_layer2 = nn.Sequential(nn.Conv3d(self.final_sz[-1][-1], 3, kernel_size=1, stride=1, padding=0),\n",
    "                                          nn.Softmax(dim=1) )  # Change output channel to {2,3} upto ToTensor version\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        #print('\\n Running Recon2X3D7 \\n')\n",
    "        #print('\\n--- Encode loop ---')\n",
    "        x1 = self.first_layer1(x1)\n",
    "        x2 = self.first_layer2(x2)\n",
    "        encode_trace1 = []\n",
    "        encode_trace2 = []\n",
    "        i = 0\n",
    "        for layer1 , layer2 in zip(self.dense_layer1 , self.dense_layer2):   # for loop on each nn.Sequential layer\n",
    "            x1 = layer1(x1)         # layer = each dense blocks\n",
    "            x2 = layer2(x2)         # layer = each dense blocks\n",
    "            #print('Encode1 layer:{}   |  h1 = {}   {}   {}'.format(i, x1.size(), x1.dtype, x1.get_device()))\n",
    "            #print('Encode2 layer:{}   |  h2 = {}   {}   {}'.format(i, x2.size(), x2.dtype, x2.get_device()))\n",
    "            encode_trace1.append(x1)    # trace x for concatenation with decode layer\n",
    "            encode_trace2.append(x2)    # trace x for concatenation with decode layer\n",
    "            if i != len(self.dense_layer1)-1:\n",
    "                x1 , _ = self.pool2d(x1)        # for MaxPool2d or AvgPool2d\n",
    "                x2 , _ = self.pool2d(x2)        # for MaxPool2d or AvgPool2d\n",
    "                #x1 = self.adaptpool2d[i](x1)        # for AdaptivePooling\n",
    "                #x2 = self.adaptpool2d[i](x2)        # for AdaptivePooling\n",
    "                #print('                  |  h1_pool = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "                #print('                  |  h2_pool = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "            i += 1\n",
    "        #[print('encode_trace1 = {}'.format(x.size())) for x in encode_trace1]\n",
    "        #[print('encode_trace2 = {}'.format(x.size())) for x in encode_trace2]\n",
    "        \n",
    "        #print('\\n--- Decode loop and Fusion loop ---')\n",
    "        res = [ round(x1[-1]/x2) for x1,x2 in zip(self.de_sz,self.res1) ]\n",
    "        #print('inside loop res = {}'.format(res))\n",
    "        i = 0\n",
    "        for layer1,layer2,axialD1,axialH1,axialW1,axialD3,axialH3,axialW3,axialD5,axialH5,axialW5,axialFuseD,axialFuseH,axialFuseW,fusion3dlayer in zip(self.decode_layer1, self.decode_layer2,\n",
    "                                                                                                                                                        self.axialD1, self.axialH1, self.axialW1,\n",
    "                                                                                                                                                        self.axialD3, self.axialH3, self.axialW3,\n",
    "                                                                                                                                                        self.axialD5, self.axialH5, self.axialW5,\n",
    "                                                                                                                                                        self.axialFusionD,self.axialFusionH,self.axialFusionW,\n",
    "                                                                                                                                                        self.final_layer):\n",
    "            #print('Level: {}'.format(i))\n",
    "            if i==0:\n",
    "                x1 = encode_trace1[len(self.dense_layer1)-1-i]\n",
    "                x2 = encode_trace2[len(self.dense_layer2)-1-i]\n",
    "            else:   # encoder and decoder fusion\n",
    "                x1 = torch.cat( (encode_trace1[len(self.dense_layer1)-1-i] , x1) , dim=1)\n",
    "                x2 = torch.cat( (encode_trace2[len(self.dense_layer1)-1-i] , x2) , dim=1)\n",
    "            N,C,H,W = x1.size()\n",
    "            #print('            |  x1_cat = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "            #print('            |  x2_cat = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "            x1 = layer1(x1)\n",
    "            x2 = layer2(x2)\n",
    "            #print('            |  x1_conv2d = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "            #print('            |  x2_conv2d = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "            \n",
    "            # extract feature come out from final conv2d encoder-decoder\n",
    "            #feature1.append(x1)\n",
    "            #feature2.append(x2)\n",
    "            \n",
    "            # Axial + Inception + Fusion along D, H, W\n",
    "            xd = torch.cat( (x1,x2.transpose(3,1).flip(3)), dim=1 )                     # N,2C,H,W >>[Conv2d]>> N,C,H,W\n",
    "            xh = torch.cat( (x1,x2.transpose(3,1).flip(3)), dim=2 ).transpose(2,1)      # N,C,2H,W >> N,2H,C,W >>[Conv2d]>> N,H,C,W >> N,C,H,W\n",
    "            xw = torch.cat( (x1,x2.transpose(3,1).flip(3)), dim=3 ).transpose(3,1)      # N,C,H,2W >> N,C,H,2W >>[Conv2d]>> N,W,H,C >> N,C,H,W\n",
    "            #print('            |  xd_cat = {}   {}   {}'.format(xd.size(), xd.dtype, xd.get_device()))\n",
    "            #print('            |  xh_cat = {}   {}   {}'.format(xh.size(), xh.dtype, xh.get_device()))\n",
    "            #print('            |  xw_cat = {}   {}   {}'.format(xw.size(), xw.dtype, xw.get_device()))\n",
    "            xd = torch.cat([self.inceptPool2d(xd),axialD1(xd),axialD3(xd),axialD5(xd)], dim=1)\n",
    "            xh = torch.cat([self.inceptPool2d(xh),axialH1(xh),axialH3(xh),axialH5(xh)], dim=1)\n",
    "            xw = torch.cat([self.inceptPool2d(xw),axialW1(xw),axialW3(xw),axialW5(xw)], dim=1)\n",
    "            #print('            |  xd_axial_inception = {}   {}   {}'.format(xd.size(), xd.dtype, xd.get_device()))\n",
    "            #print('            |  xh_axial_inception = {}   {}   {}'.format(xh.size(), xh.dtype, xh.get_device()))\n",
    "            #print('            |  xw_axial_inception = {}   {}   {}'.format(xw.size(), xw.dtype, xw.get_device()))\n",
    "            xd = axialFuseD(xd)\n",
    "            xh = axialFuseH(xh).transpose(2,1)\n",
    "            xw = axialFuseW(xw).transpose(3,1)\n",
    "            #print('            |  xd_fusion2d = {}   {}   {}'.format(xd.size(), xd.dtype, xd.get_device()))\n",
    "            #print('            |  xh_fusion2d = {}   {}   {}'.format(xh.size(), xh.dtype, xh.get_device()))\n",
    "            #print('            |  xw_fusion2d = {}   {}   {}'.format(xw.size(), xw.dtype, xw.get_device()))\n",
    "            # Covert to 3D volumes\n",
    "            XD = xd.view(-1, res[i], H, H, W)\n",
    "            XH = xh.view(-1, res[i], H, H, W)\n",
    "            XW = xw.view(-1, res[i], H, H, W)\n",
    "            #print('            |  XD_view3d = {}   {}   {}'.format(XD.size(), XD.dtype, XD.get_device()))\n",
    "            #print('            |  XH_view3d = {}   {}   {}'.format(XH.size(), XH.dtype, XH.get_device()))\n",
    "            #print('            |  XW_view3d = {}   {}   {}'.format(XW.size(), XW.dtype, XW.get_device()))\n",
    "            \n",
    "            if i == 0:\n",
    "                X = torch.cat( (XD,XH,XW), dim=1 )                # cat everything  and  # avg of cat\n",
    "            else:\n",
    "                X = torch.cat( (XD,XH,XW,X), dim=1 )               # cat everything\n",
    "            #print('            |  X_cat3d = {}   {}   {}'.format(X.size(), X.dtype, X.get_device()))\n",
    "            \n",
    "            X = fusion3dlayer(X)\n",
    "            #print('            |  X_fusion3d = {}   {}   {}'.format(X.size(), X.dtype, X.get_device()))\n",
    "            if i!=len(self.decode_layer1)-1:\n",
    "                x1 = self.upconv2d1[i](x1)\n",
    "                x2 = self.upconv2d1[i](x2)\n",
    "                X = self.upconv3d[i](X)\n",
    "                #print('               |  x1_upconv2d = {}   {}   {}'.format(x1.size(), x1.dtype, x1.get_device()))\n",
    "                #print('               |  x2_upconv2d = {}   {}   {}'.format(x2.size(), x2.dtype, x2.get_device()))\n",
    "                #print('               |  X_upconv3d = {}   {}   {}'.format(X.size(), X.dtype, X.get_device()))\n",
    "            i+=1\n",
    "            \n",
    "        #print('\\n--- Final classify ---')\n",
    "        X = self.final_layer2(X)\n",
    "        #print('   X final = {}   {}   {} \\n'.format(X.size(), X.dtype, X.get_device()))\n",
    "        return X\n",
    "        \n",
    "def test_Recon2X3D7():\n",
    "    torch.cuda.empty_cache()\n",
    "    # AxialInceptionFusion of AP & LAT features\n",
    "    in_c = 1\n",
    "    en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4]]\n",
    "    de_sz = [[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "    de3d_sz = [[16,8,8],[32,16,16],[64,32,32],[128,64,64],[256,128,128],[512,256,256]] # 2-axial layer per level\n",
    "    final_sz = [[3,16,16],[6,16,16],[6,16,16],[6,16,16],[6,16,16],[6,16,16]] # 2-fusion layer per level\n",
    "    model = Recon2X3D7( in_c, en_sz, de_sz, de3d_sz, final_sz).to(device=device)\n",
    "    #model = nn.DataParallel(model)\n",
    "    print(model,'\\n')\n",
    "    #print(model.__dict__.keys())\n",
    "    n = 1\n",
    "    x1 = torch.randn(n,1,256,256).to(device=device)\n",
    "    x2 = torch.randn(n,1,256,256).to(device=device)\n",
    "    time1 = time.time()\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        output = model(x1,x2)\n",
    "        #torch.cuda.synchronize()\n",
    "    time2 = time.time()\n",
    "    assert output.size()==torch.Size([n,3,256,256,256]) , 'output size error!'\n",
    "    print('\\n Total running time = {} sec. \\n'.format(time2-time1))\n",
    "    \n",
    "test_Recon2X3D7()\n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Discriminator3D for GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator3D(nn.Module):\n",
    "    #def __init__(self, ngpu):\n",
    "    def __init__(self, nc, ndf, ):\n",
    "        super(Discriminator3D, self).__init__()\n",
    "        #self.ngpu = ngpu\n",
    "        self.nc = nc\n",
    "        self.ndf = ndf\n",
    "        self.main = nn.Sequential(\n",
    "            # input is [bz,nc,256,256,256]\n",
    "            nn.Conv3d( nc, ndf, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            # [bz,ndf,128,128,128]\n",
    "            nn.Conv3d(ndf, ndf*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(ndf*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            # [bz,ndf*2,64,64,64]\n",
    "            nn.Conv3d(ndf*2, ndf*4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(ndf*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            # [bz,ndf*4,32,32,32]\n",
    "            nn.Conv3d(ndf*4, ndf*8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(ndf*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            # [bz,ndf*8,16,16,16]\n",
    "            nn.Conv3d(ndf*8, ndf*8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(ndf*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            # [bz,ndf*8,8,8,8]\n",
    "            nn.Conv3d(ndf*8, ndf*16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(ndf*16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            # [bz,ndf*16,4,4,4]\n",
    "            nn.Conv3d(ndf*16, ndf*32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(ndf*32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            # [bz,ndf*32,2,2,2]\n",
    "            )\n",
    "        \n",
    "        # Flattening\n",
    "        self.classify = nn.Sequential(nn.Conv3d(ndf*32*2*2*2, 512, kernel_size=1, stride=1, padding=0, bias=False) ,\n",
    "                                      nn.Conv3d(512, 1, kernel_size=1, stride=1, padding=0, bias=False) ,\n",
    "                                      nn.Sigmoid())\n",
    "    def forward(self, input):\n",
    "        #print(' ### Discriminator3D ###')\n",
    "        #print('input = {} '.format(input.size()))\n",
    "        n = input.size(0)\n",
    "        #print('n = {}'.format(n))\n",
    "        x = self.main(input)\n",
    "        #print('x = {} '.format(x.size()))\n",
    "        x = x.reshape(n, -1,1,1,1)\n",
    "        #print('x = {} '.format(x.size()))\n",
    "        x = self.classify(x)\n",
    "        #print('x = {} '.format(x.size()))\n",
    "        return x\n",
    "\n",
    "def test_Discriminator3D():\n",
    "    #torch.cuda.empty_cache()\n",
    "    n = 8\n",
    "    nc = 3\n",
    "    ndf = 32\n",
    "    model = Discriminator3D(nc, ndf).to(device=device)\n",
    "    #print(model,'\\n')\n",
    "    input = torch.randn(n,nc,256,256,256).to(device=device)\n",
    "    print('input = {}'.format(input.size()))\n",
    "    time1 = time.time()\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        output = model(input)\n",
    "        print('output = {}'.format(output.size()))\n",
    "    time2 = time.time()\n",
    "    print('\\n Total running time = {} sec. \\n'.format(time2 - time1))\n",
    "    assert output.size()==torch.Size([n,1,1,1,1]) , 'output size error!'\n",
    "    \n",
    "#test_Discriminator3D()\n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Traing Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### Mixed precision - Simply and DataParallel (Version2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "criterion = None\n",
    "\n",
    "def train_mixed(model, trainLoader, valLoader, optimizer, scheduler, criterion, batch_sz, epochs, saved_name, saved_dict):\n",
    "    ''' Mixed precision training Version2 \n",
    "        Build-in model saver, saved_dict\n",
    "    '''\n",
    "    print('Train on: {}'.format(device))\n",
    "    print('Optimizer = {} \\n'.format(optimizer))\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for e in range(epochs):\n",
    "        time1 = time.time()\n",
    "        print('----- Epoch = {} ----- # Learning rate = {:.4e}'.format(e+1, optimizer.param_groups[0][\"lr\"]))\n",
    "        train_loss, train_acc, val_loss, val_acc = 0, 0, 0, 0   # reset every epoch\n",
    "        \n",
    "        for t, train_sample in enumerate(trainLoader):\n",
    "            #print('trainLoader = {}'.format(len(trainLoader)))\n",
    "            #print('batch size = {} \\n'.format(train_sample['Target'].size()))\n",
    "            if train_sample['Target'].size(0)%batch_sz != 0 or t==len(trainLoader)-1:  # exclude inequal batch\n",
    "                print('Final training accuracy = {:.4f}'.format(acc))\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                model.train(mode=True)    # put model to training mode\n",
    "                target = train_sample['Target'].to(device=device, dtype=dtype1)\n",
    "                view1 = train_sample['view1'].to(device=device, dtype=dtype1)\n",
    "                view2 = train_sample['view2'].to(device=device, dtype=dtype1)\n",
    "                #print('target autocast = {}   {}\\nap autocast = {}   {}'.format(target.size(),target.dtype, view1.size(),view1.dtype))\n",
    "                \n",
    "                # calculation of output\n",
    "                output = model(view1, view2)\n",
    "                #print('output autocast 1 = {}   {}\\n'.format(output.size(),output.dtype))\n",
    "                \n",
    "                # Calculate loss of this sample batch\n",
    "                loss = criterion(output, target.long())          # for ToTensor6-9 with FocalLossMulticlass\n",
    "                #print('loss = {}   {}'.format(loss,loss.dtype))\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # Check accuracy\n",
    "                acc = iou((output[:,1]>=0.5).float(), (target==1).float())                       # for ToTensor6-9\n",
    "                #acc2 = iou((output[:,2]>=0.5).float(), (target==2).float())                      # for ToTensor6-9\n",
    "                #acc2 = hausdorff_voxel((output[:,2]>=0.5).float(), (target==2).float())          # for ToTensor6-9\n",
    "                train_acc += acc.detach().item()\n",
    "                \n",
    "                if t%(round(len(trainLoader)/5)) == 0:\n",
    "                    #print('Iteration: {}   |   Loss = {:.4f}   |   Accuracy = {:.4f} {:.4f}'.format(t, loss.item(), acc, acc2))\n",
    "                    print('Iteration: {}   |   Loss = {:.4f}   |   Accuracy = {:.4f}'.format(t, loss.item(), acc))\n",
    "                if t==len(trainLoader)-1:\n",
    "                    print('Final training accuracy = {:.4f}'.format(acc))\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            torch.cuda.synchronize()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        # Append Loss and Accuracy history\n",
    "        train_loss /= round(len(trainLoader))    # mean\n",
    "        train_acc /= round(len(trainLoader))     # mean\n",
    "        saved_dict['train_loss_history'].append(train_loss)\n",
    "        saved_dict['train_acc_history'].append(train_acc)\n",
    "        print('Training loss = {:.4f}'.format(train_loss))\n",
    "        print('Training accuracy = {:.4f}'.format(train_acc))\n",
    "        print('Max. of [Mean Training accuracy] = {:.4f}'.format(max(saved_dict['train_acc_history'])))\n",
    "        time2 = time.time()\n",
    "        print('Duration training time = {} Min.\\n'.format((time2-time1)/60))\n",
    "        \n",
    "###########################################################################################################################\n",
    "        \n",
    "        print('### Validation loop ###')\n",
    "        model.eval()\n",
    "        time3 = time.time()\n",
    "        \n",
    "        for t, val_sample in enumerate(valLoader):\n",
    "            if val_sample['Target'].size(0)%batch_sz%batch_sz != 0 or t==len(valLoader)-1: # exclude inequal batch\n",
    "                print('Final validation accuracy = {:,.4f}'.format(acc))\n",
    "                break\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    target = val_sample['Target'].to(device=device, dtype=dtype1)\n",
    "                    view1 = val_sample['view1'].to(device=device, dtype=dtype1)\n",
    "                    view2 = val_sample['view2'].to(device=device, dtype=dtype1)\n",
    "                    output = model(view1,view2)\n",
    "                    \n",
    "                    loss = criterion(output ,target.long())         # for ToTensor6-9\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    acc = iou((output[:,1]>=0.5).float(), (target==1).float())                        # for ToTensor6-9\n",
    "                    #acc2 = iou((output[:,2]>=0.5).float(), (target==2).float())                       # for ToTensor6-9\n",
    "                    #acc2 = hausdorff_voxel((output[:,2]>=0.5).float(), (target==2).float())           # for ToTensor6-9\n",
    "                    val_acc += acc.detach().item()\n",
    "                    \n",
    "                    if t%(round(len(valLoader)/5)) == 0:\n",
    "                        #print('Iteration: {}   |   Loss = {:,.4f}   |   Accuracy = {:.4f} {:.4f}'.format(t, loss.item(), acc, acc2))\n",
    "                        print('Iteration: {}   |   Loss = {:.4f}   |   Accuracy = {:.4f}'.format(t, loss.item(), acc))\n",
    "                    if t==len(valLoader)-1:\n",
    "                        print('Final validation accuracy = {:,.4f}'.format(acc))\n",
    "        \n",
    "        val_loss /= round(len(valLoader))    # mean\n",
    "        val_acc /= round(len(valLoader))     # mean\n",
    "        saved_dict['val_loss_history'].append(val_loss)\n",
    "        saved_dict['val_acc_history'].append(val_acc)\n",
    "        saved_dict['timestamp'] = str(datetime.datetime.now())     # update timestamp\n",
    "        torch.cuda.synchronize()\n",
    "        scheduler.step(val_acc)\n",
    "        print('# Validation loss = {:.4f}'.format(val_loss))\n",
    "        print('Validation accuracy = {:.4f}'.format(val_acc))\n",
    "        print('Max. of [Mean Validation accuracy] = {:.4f}'.format(max(saved_dict['val_acc_history'])))\n",
    "        time4 = time.time()\n",
    "        print('Duration validation time = {} Min. \\n'.format((time4-time3)/60))\n",
    "        \n",
    "        ### Save the best trained model and training history ###\n",
    "        if val_acc >= max(saved_dict['val_acc_history']):   # save the best trained\n",
    "            print(' *** Update the best model state dict at epoch: {}  at time: {} ***'.format(e+1, str(datetime.datetime.now())))\n",
    "            saved_dict['model_state_dict'] = model.module.state_dict()    # update the best model\n",
    "        else:   # just save training history and keep the best trained model\n",
    "            print('\\n *** Update training history and the last model state dict at epoch = {}  at time: {} ***\\n'.format(e+1, str(datetime.datetime.now())))\n",
    "            # don't update any params in the trained model\n",
    "            \n",
    "        ### End of training ###\n",
    "        ### Save trained parameters at the last epoch and training history, keeping the best trained model   \n",
    "        saved_dict['optimizer_state_dict'] = optimizer.state_dict()\n",
    "        saved_dict['scheduler_state_dict'] = scheduler.state_dict()\n",
    "        torch.save(saved_dict, saved_name)\n",
    "        print(' *** Saved End ***\\n')\n",
    "        print('-'*120,'\\n\\n')\n",
    "    \n",
    "    return saved_dict\n",
    "    \n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-fold cross validation ###\n",
    "\n",
    "# Training and validation loop\n",
    "num_workers = 4\n",
    "history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "batch_sz = 3\n",
    "learning_rate = 1e-4             # deflaut = 5e-4\n",
    "weight = None\n",
    "epochs = 10\n",
    "\n",
    "# Select mode\n",
    "toTensorMode = int(input('Select ToTensor : [8] Auxiliary class  [9] Native = '))\n",
    "if toTensorMode == 8:\n",
    "    print('ToTensor8 selected')\n",
    "    ToTensor = ToTensor8()\n",
    "    weight = torch.tensor([0.15,0.25,0.6], device=device)     # For ToTensor8\n",
    "elif toTensorMode == 9:\n",
    "    print('Totensor9 selected')\n",
    "    ToTensor = ToTensor9()\n",
    "    weight = torch.tensor([0.5,0.5], device=device)           # For ToTensor9\n",
    "elif toTensorMode < 8:\n",
    "    print('Totensor1-7 selected  change NormalizeSample() and FemurDataset')\n",
    "    ToTensor = ToTensor7()\n",
    "else:\n",
    "    raise ValueError ('Invalid ToTensor input')\n",
    "print('ToTensor = {}'.format(ToTensor))\n",
    "criterion = FocalLossMulticlass(weight=weight, gamma=2.0, reduction='sum')\n",
    "\n",
    "root_dir = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2'\n",
    "\n",
    "for fold in range(1,6):\n",
    "    \n",
    "    # Preparing dataset\n",
    "    saved_name = 'trained\\FracReconNet_Fold{}.pt'.format(fold)\n",
    "    train_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\With augmentation - Fold{} Training.xlsx'.format(fold)\n",
    "    test_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\With augmentation - Fold{} Testing.xlsx'.format(fold)\n",
    "    print('Training on: {}', train_file)\n",
    "    print('Testing on: {}', test_file)\n",
    "    train_transformedFemur = FemurDataset2(csv_file=training_file, root_dir=root_dir, \n",
    "                                           transform=transforms.Compose([NormalizeSample2(), ToTensor]))\n",
    "    test_transformedFemur = FemurDataset2(csv_file=test_file, root_dir=root_dir, \n",
    "                                          transform=transforms.Compose([NormalizeSample2(), ToTensor]))\n",
    "    trainLoader = DataLoader(train_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=num_workers)\n",
    "    testLoader = DataLoader(val_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    # Building model\n",
    "    in_c = 1\n",
    "    en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "    de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "    de3d_sz = None\n",
    "    final_sz = [[2,32,32],[5,32,32],[5,32,32],[5,16,16],[5,16,16],[5,16,16],[5,16,16]]\n",
    "    saved_dict = {'timestamp':None ,'note':note ,'in_c':in_c ,'en_sz':en_sz,'de_sz':de_sz,'de3d_sz':de3d_sz ,'final_sz':final_sz,\n",
    "                  'train_loss_history':list(),'train_acc_history':list(),'val_loss_history':list(),'val_acc_history':list(),\n",
    "                  'model_state_dict':None,'optimizer_state_dict':None,'scheduler_state_dict':None }\n",
    "    model = Recon2X3D5(in_c, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, threshold=1e-3, \n",
    "                                  threshold_mode='rel', cooldown=0, min_lr=1e-12, verbose=True)\n",
    "    \n",
    "    # Training and Testing\n",
    "    timeT1 = time.time()\n",
    "    saved_dict = train_mixed(model, trainLoader, testLoader, optimizer, scheduler, criterion, batch_sz, epochs, saved_name, saved_dict)\n",
    "    timeT2 = time.time()\n",
    "    print('Save model = {}'.format(saved_name))\n",
    "    print('Total training time = {} hours'.fomat((timeT2-timeT1)/3600))\n",
    "    print('############################################################### Clear parameters \\n\\n')\n",
    "    del model, train_transformedFemur, test_transformedFemur, trainLoader, testLoader, optimizer, scheduler, saved_dict\n",
    "\n",
    "torch.save(model,'k_cross_CNN.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generative and Adversarial Training - Mixed Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### GAN without Autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Without Autocast ###\n",
    "# Dataset\n",
    "batch_sz = 1\n",
    "train_transformedFemur = FemurDataset2(csv_file=training_file, root_dir=root_dir, \n",
    "                                      transform=transforms.Compose([NormalizeSample2(),ToTensor8()]))\n",
    "val_transformedFemur = FemurDataset2(csv_file=val_file, root_dir=root_dir, \n",
    "                                    transform=transforms.Compose([NormalizeSample2(),ToTensor8()]))\n",
    "trainLoader = DataLoader(train_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "valLoader = DataLoader(val_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "print('   Training set : {}'.format(len(trainLoader)))\n",
    "print('   Validation set : {}'.format(len(valLoader)))\n",
    "\n",
    "# Defind the models\n",
    "\n",
    "nc = 3\n",
    "ndf = 4   # ndf should higher than nc\n",
    "netD = Discriminator3D(nc, ndf).to(device=device)\n",
    "'''in_c = 1\n",
    "en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "de3d_sz = None\n",
    "final_sz = [[2,32,32] , [5,32,32] , [5,32,32] , [5,16,16] , [5,16,16] , [5,16,16] , [5,16,16]]\n",
    "netG = Recon2X3D5(in_c, en_sz, de_sz, de3d_sz, final_sz).to(device=device)'''\n",
    "netG = model.to(device=device)\n",
    "#netG = model.to(device=device)\n",
    "\n",
    "netD = nn.DataParallel(netD)\n",
    "netG = nn.DataParallel(netG)\n",
    "\n",
    "# Training setup\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "lr = 1e-4   # default = 0.0002\n",
    "beta1 = 0.5 # default = 0.5\n",
    "#weight = torch.tensor([0.15,0.25,0.6], device=device)     # For ToTensor8   \n",
    "#weight = torch.tensor([0.5,0.5], device=device)           # For ToTensor9   \n",
    "#criterion = FocalLossMulticlass(weight=weight, gamma=2.0, reduction='sum')\n",
    "criterion_gan = nn.BCELoss()\n",
    "#criterion_gan = nn.BCEWithLogitsLoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "epochs = 1\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "Recon_losses = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "D_Gx1 = []\n",
    "D_Gx2 = []\n",
    "D_X = []\n",
    "IoU = []\n",
    "acc = 0\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "t1 = time.time()\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    # For each batch in the dataloader\n",
    "    #for i, data in enumerate(dataloader, 0):\n",
    "    for i, train_sample in enumerate(trainLoader,0):\n",
    "        netD.train(mode=True)\n",
    "        netG.train(mode=True)\n",
    "        ##################################################################################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))  to reach zero\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        optimizerD.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "\n",
    "        # Format batch\n",
    "        target = train_sample['Target'].to(device=device, dtype=dtype1)\n",
    "        view1 = train_sample['view1'].to(device=device, dtype=dtype1)\n",
    "        view2 = train_sample['view2'].to(device=device, dtype=dtype1)\n",
    "        #print('target = {} {} {}'.format(target.size(), target.dtype, target.get_device()))\n",
    "        targetc = torch.zeros((target.size(0), nc, target.size(1), target.size(2), target.size(3)),device=device)\n",
    "        targetc[:,0] = target==0\n",
    "        targetc[:,1] = target==1\n",
    "        targetc[:,2] = target==2\n",
    "        targetc = targetc.to(device=device, dtype=dtype1)\n",
    "        #print('targetc = {} {} {}'.format(targetc.size(),targetc.dtype,targetc.get_device()))\n",
    "        #print('view1 = {} {} {}'.format(view1.size(),view1.dtype,view1.get_device()))\n",
    "        #print('view2 = {} {} {}'.format(view2.size(),view2.dtype,view2.get_device()))\n",
    "        b_size = target.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        #print('label-real = {}'.format(label))\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(targetc).view(-1)\n",
    "        #print('Real-output = {} {}'.format(output, output.size()))\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion_gan(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        ## Train with all-fake batch\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(view1, view2)      # Reconstruct the output 3D shape\n",
    "        label.fill_(fake_label)\n",
    "        #print('label-fake = {}'.format(label))\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)   # detach for only updating D without updating G\n",
    "        #print('Fake-output-z1 = {} {}'.format(output, output.size(), output.dtype))\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion_gan(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        #errD.backward()\n",
    "        # Update D for fake data\n",
    "        optimizerD.step()\n",
    "        \n",
    "        ########################################################################################\n",
    "        # (2) Update G network: maximize log(D(G(z))) to reach zero\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        #print('label-fake2 = {}'.format(label))\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        # Classify of G'output by D\n",
    "        output = netD(fake).view(-1)\n",
    "        #print('Fake-output-z2 = {} {}'.format(output, output.size(), output.dtype))\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion_gan(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()   # or loss_recon.backword\n",
    "        D_G_z2 = output.mean().item()\n",
    "        acc = iou((fake[:,1]>=0.5).float(), (targetc[:,1]).float())\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i%(round(len(trainLoader)/50)) == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\t  D(x): %.4f\\tD(G(z)): %.4f / %.4f\\tIoU: %.4f'\n",
    "                  % (epoch, epochs, i, len(trainLoader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2, acc))   # len(dataloader)\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        D_X.append(D_x)\n",
    "        D_Gx1.append(D_G_z1)\n",
    "        D_Gx2.append(D_G_z2)\n",
    "        IoU.append(acc)\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        '''\n",
    "        if (iters % 500 == 0) or ((epoch == epochs-1) and (i == len(trainLoader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(view1,view2).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        '''\n",
    "        \n",
    "t2 = time.time()\n",
    "print('\\n ### Total GAN training time = {} min.'.format((t2-t1)/60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### GAN with Autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Autocast ###\n",
    "# Device\n",
    "torch.cuda.empty_cache()\n",
    "cuda0 = torch.device('cuda:0')\n",
    "cuda1 = torch.device('cuda:1')\n",
    "\n",
    "# Dataset\n",
    "batch_sz = 2\n",
    "train_transformedFemur = FemurDataset2(csv_file=training_file, root_dir=root_dir, \n",
    "                                      transform=transforms.Compose([NormalizeSample2(),ToTensor8()]))\n",
    "val_transformedFemur = FemurDataset2(csv_file=val_file, root_dir=root_dir, \n",
    "                                    transform=transforms.Compose([NormalizeSample2(),ToTensor8()]))\n",
    "trainLoader = DataLoader(train_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=4)\n",
    "valLoader = DataLoader(val_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=4)\n",
    "print('   Training set : {}'.format(len(trainLoader)))\n",
    "print('   Validation set : {}'.format(len(valLoader)))\n",
    "\n",
    "# Defind the models and Training setup\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "lr = 0.0001\n",
    "beta1 = 0.5\n",
    "running_mode = int(input('Running mode: [1] Training from scratch  [2] Transfer model  [3] Resume previous GAN model = '))\n",
    "if running_mode == 1:\n",
    "    print(' ### Training from scratch ###')\n",
    "    nc = 3\n",
    "    ndf = 4  #  ndf should higher than nc\n",
    "    netD = Discriminator3D(nc, ndf).to(device=cuda1)\n",
    "    in_c = 1\n",
    "    en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "    de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "    de3d_sz = None\n",
    "    final_sz = [[2,32,32],[5,32,32],[5,32,32],[5,16,16],[5,16,16],[5,16,16],[5,16,16]]\n",
    "    netG = Recon2X3D5(in_c, en_sz, de_sz, de3d_sz, final_sz).to(device=cuda0)\n",
    "elif running_mode == 2:  # go to 3.2 Main Execution, then load Recon2X3D for transfered model\n",
    "    print(' ### Transfer model ###')\n",
    "    nc = 3\n",
    "    ndf = 4  #ndf should higher than nc\n",
    "    netD = Discriminator3D(nc, ndf).to(device=cuda1)\n",
    "    netG = model.to(device=cuda0)\n",
    "elif running_mode == 3:\n",
    "    print(' ### Resume previous GAN model ###')\n",
    "    saved_name = 'trained\\Recon2X3D5GAN_22022201.pt'      # Recon2X3D5GAN_22022101\n",
    "    confirm_saved_name = str(input('Confirm Save : ' + saved_name + ' [y/n] ? '))\n",
    "    while confirm_saved_name!='y':\n",
    "        saved_name = str('trained\\\\') + str(input('Enter saved_name = trained\\ ')) + str('.pt')\n",
    "        confirm_saved_name = str(input('Confirm Save : ' + saved_name + ' [y/n] ? '))\n",
    "    print('*** Confirm saved_name = {} ***\\n'.format(saved_name))\n",
    "    saved_dict = torch.load(saved_name, map_location=device)\n",
    "    nc = saved_dict['nc']\n",
    "    ndf = saved_dict['ndf']\n",
    "    in_c = saved_dict['in_c']\n",
    "    en_sz = saved_dict['en_sz']\n",
    "    de_sz = saved_dict['de_sz']\n",
    "    de3d_sz = saved_dict['de3d_sz']\n",
    "    final_sz = saved_dict['final_sz']\n",
    "    print('NetD.parameter: \\n\\tnc={} \\n\\tndf={}'.format(saved_dict['nc'],saved_dict['ndf']))\n",
    "    print('NetG.parameter: \\n\\tin_c={} \\n\\ten_sz={} \\n\\tde_sz={} \\n\\tfinal_sz={}'.format(saved_dict['in_c'],saved_dict['en_sz'],saved_dict['de_sz'],saved_dict['final_sz']))\n",
    "    netD = Discriminator3D(saved_dict['nc'],saved_dict['ndf']).to(device=cuda1)\n",
    "    netG = Recon2X3D5(saved_dict['in_c'], saved_dict['en_sz'], saved_dict['de_sz'], \n",
    "                      saved_dict['de3d_sz'], saved_dict['final_sz']).to(device=cuda0)\n",
    "    netD.load_state_dict(saved_dict['netD_state_dict'])\n",
    "    netG.load_state_dict(saved_dict['netG_state_dict'])\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optimizerD.load_state_dict(saved_dict['optimizerD_state_dict'])\n",
    "    optimizerG.load_state_dict(saved_dict['optimizerG_state_dict'])\n",
    "    D_losses = saved_dict['D_losses']\n",
    "    G_losses = saved_dict['G_losses']\n",
    "    Recon_losses = saved_dict['Recon_losses']\n",
    "    D_Gx1 = saved_dict['D_Gx1']\n",
    "    D_Gx2 = saved_dict['D_Gx2']\n",
    "    D_X = saved_dict['D_X']\n",
    "    IoU = saved_dict['IoU']\n",
    "    print('Timestamp = {}'.format(saved_dict['timestamp']))\n",
    "    print('Total iterations = {:,}'.format(len(saved_dict['G_losses'])))\n",
    "else:\n",
    "    raise ValueError(\"Invalid running_mode input !!! \\n\")\n",
    "\n",
    "#netD = nn.DataParallel(netD)\n",
    "#netG = nn.DataParallel(netG)\n",
    "\n",
    "#criterion_gan = nn.BCELoss(reduction='sum')\n",
    "criterion_gan = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "weight = torch.tensor([0.15,0.25,0.6], device=device)     # For ToTensor8\n",
    "#weight = torch.tensor([0.5,0.5], device=device)           # For ToTensor9\n",
    "criterion_recon = FocalLossMulticlass(weight=weight, gamma=2.0, reduction='mean')\n",
    "\n",
    "if running_mode == 1 or running_mode == 2:\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    # Lists to keep track of progress\n",
    "    img_list = []\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "    Recon_losses = []\n",
    "    D_Gx1 = []\n",
    "    D_Gx2 = []\n",
    "    D_X = []\n",
    "    IoU = []\n",
    "    iters = 0\n",
    "    \n",
    "\n",
    "training_logit = int(input('\\nDo you want to start training? [1] Yes  [0] No = '))\n",
    "if training_logit == 1:\n",
    "    print(\"\\n\\t... Starting Training Loop ...\")\n",
    "    epochs = int(input('[Input] epochs = '))\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    # For each epoch\n",
    "    for epoch in range(epochs):\n",
    "        # For each batch in the dataloader\n",
    "        #for i, data in enumerate(dataloader, 0):\n",
    "        errD_sum, errG_gan_sum, errG_recon_sum, D_x_sum, D_G_z1_sum, D_G_z2_sum, IoU_sum = 0, 0, 0, 0, 0, 0, 0\n",
    "        for i, train_sample in enumerate(trainLoader,0):\n",
    "            if i==len(trainLoader)-1:  # exclude inequal batch\n",
    "                print('Final training accuracy = {:.4f} \\n'.format(acc))\n",
    "                break\n",
    "            netD.train(mode=True)\n",
    "            netG.train(mode=True)\n",
    "            ##################################################################################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))  to reach zero\n",
    "            ###########################\n",
    "            ## Train with all-real batch\n",
    "            netD.zero_grad()\n",
    "            optimizerD.zero_grad()\n",
    "            optimizerG.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                # Format batch\n",
    "                target = train_sample['Target'].to(device=cuda1, dtype=dtype1)\n",
    "                view1 = train_sample['view1'].to(device=cuda0, dtype=dtype1)\n",
    "                view2 = train_sample['view2'].to(device=cuda0, dtype=dtype1)\n",
    "                #print('target = {} {} {}'.format(target.size(), target.dtype, target.get_device()))\n",
    "                targetc = torch.zeros((target.size(0), nc, target.size(1), target.size(2), target.size(3)),device=device)\n",
    "                targetc[:,0] = target==0\n",
    "                targetc[:,1] = target==1\n",
    "                targetc[:,2] = target==2\n",
    "                targetc = targetc.to(device=cuda1, dtype=dtype1)\n",
    "                #print('targetc = {} {} {}'.format(targetc.size(),targetc.dtype,targetc.get_device()))\n",
    "                #print('view1 = {} {} {}'.format(view1.size(),view1.dtype,view1.get_device()))\n",
    "                #print('view2 = {} {} {}'.format(view2.size(),view2.dtype,view2.get_device()))\n",
    "                b_size = target.size(0)\n",
    "                label = torch.full((b_size,), real_label, dtype=torch.float, device=cuda1)\n",
    "\n",
    "                # Forward pass real batch through D\n",
    "                output = netD(targetc).view(-1)\n",
    "                #print('Real-output = {} {}'.format(output, output.size()))\n",
    "                # Calculate loss on all-real batch\n",
    "                errD_real = criterion_gan(output, label)\n",
    "                # Calculate gradients for D in backward pass\n",
    "                #errD_real.backward()\n",
    "                D_x = output.mean().item()\n",
    "                D_x_sum += D_x\n",
    "            # Update D for real data\n",
    "            scaler.scale(errD_real).backward()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            ## Train with all-fake batch\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                # Generate fake image batch with G\n",
    "                fake = netG(view1, view2)      # Reconstruct the output 3D shape\n",
    "                label.fill_(fake_label)\n",
    "                # Classify all fake batch with D\n",
    "                output = netD(fake.detach().to(device=cuda1)).view(-1)   # detach for updating D without updating G\n",
    "                #print('Fake-output = {} {} {}'.format(output, output.size(), output.dtype))\n",
    "                # Calculate D's loss on the all-fake batch\n",
    "                errD_fake = criterion_gan(output, label)\n",
    "                # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "                #errD_fake.backward()\n",
    "                D_G_z1 = output.mean().item()\n",
    "                D_G_z1_sum += D_G_z1\n",
    "                # Compute error of D as sum over the fake and the real batches\n",
    "                errD = errD_real + errD_fake\n",
    "                errD_sum += errD.item()\n",
    "            # Update D for fake data\n",
    "            #optimizerD.step()\n",
    "            scaler.scale(errD_fake).backward()\n",
    "            torch.cuda.synchronize()\n",
    "            scaler.step(optimizerD)\n",
    "\n",
    "            ########################################################################################\n",
    "            # (2) Update G network: maximize log(D(G(z))) to reach zero\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            # Classify of G'output by D\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                output = netD(fake.to(device=cuda1)).view(-1)\n",
    "                # Calculate G's loss based on this output\n",
    "                errG_gan = criterion_gan(output, label)\n",
    "                errG_recon = criterion_recon(fake.to(device=device),target.long().to(device=device))\n",
    "                errG = errG_gan.to(device=device) + 50*errG_recon.to(device=device)\n",
    "                errG_gan_sum += errG_gan.item()\n",
    "                errG_recon_sum += errG_recon.item()\n",
    "                # Calculate gradients for G\n",
    "                #errG.backward()   # or loss_recon.backword\n",
    "                D_G_z2 = output.mean().item()\n",
    "                D_G_z2_sum += D_G_z2\n",
    "                acc = iou((fake[:,1]>=0.5).float().to(device=cuda1), (targetc[:,1]).float().to(device=cuda1)).item()\n",
    "                IoU_sum += acc\n",
    "            # Update G\n",
    "            #optimizerG.step()\n",
    "            scaler.scale(errG).backward()\n",
    "            torch.cuda.synchronize()\n",
    "            scaler.step(optimizerG)\n",
    "            scaler.update()   # update for the next iteration\n",
    "            \n",
    "            # Output training stats\n",
    "            if i%(round(len(trainLoader)/20)) == 0:\n",
    "                print('Time: {}'.format(str(datetime.datetime.now())))\n",
    "                print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_recon: %.4f\\t D(x): %.4f D(G(z)): %.4f/%.4f IoU: %.4f'\n",
    "                      % (epoch, epochs-1, i, len(trainLoader)-1,\n",
    "                         errD.item(), errG_gan.item(), errG_recon.item(), D_x, D_G_z1, D_G_z2, acc))    # len(dataloader)\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        D_losses.append(errD_sum/len(trainLoader))\n",
    "        G_losses.append(errG_gan_sum/len(trainLoader))\n",
    "        Recon_losses.append(errG_recon_sum/len(trainLoader))\n",
    "        D_X.append(D_x_sum/len(trainLoader))\n",
    "        D_Gx1.append(D_G_z1_sum/len(trainLoader))\n",
    "        D_Gx2.append(D_G_z2_sum/len(trainLoader))\n",
    "        IoU.append(IoU_sum/len(trainLoader))\n",
    "        \n",
    "        saved_dict2 = {'timestamp':str(datetime.datetime.now()),\n",
    "                       'note':'GAN training model transfered from Scratch using lossG = log(D(G(x))) + 50*FocalLoss([0.15,0.25,0.6])',\n",
    "                       'in_c':in_c,\n",
    "                       'en_sz':en_sz,\n",
    "                       'de_sz':de_sz,\n",
    "                       'de3d_sz':de3d_sz,\n",
    "                       'final_sz':final_sz,\n",
    "                       'netG_state_dict':netG.state_dict(),\n",
    "                       'nc':nc,\n",
    "                       'ndf':ndf,\n",
    "                       'netD_state_dict':netD.state_dict(),\n",
    "                       'G_losses':G_losses,\n",
    "                       'D_losses':D_losses,\n",
    "                       'Recon_losses':Recon_losses,\n",
    "                       'D_Gx1':D_Gx1,\n",
    "                       'D_Gx2':D_Gx2,\n",
    "                       'D_X':D_X,\n",
    "                       'IoU':IoU,\n",
    "                       'optimizerG_state_dict': optimizerG.state_dict(),\n",
    "                       'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "                      }\n",
    "        #saved_name2 = 'trained\\\\Recon2X3D5GAN_22022201.pt'\n",
    "        saved_name2 = saved_name\n",
    "        print('Save as: {}\\n'.format(saved_name2))\n",
    "        #save_logic = int(input('Confirm saved name = {} : [1] Save [0] Not Save ?'.format(saved_name2)))\n",
    "        save_logic = 1\n",
    "        if save_logic==1:\n",
    "            torch.save(saved_dict2, saved_name2)     \n",
    "            \n",
    "print(' ### End Session ### ')\n",
    "\n",
    "# Check how the generator is doing by saving G's output on fixed_noise\n",
    "'''\n",
    "if (iters % 500 == 0) or ((epoch == epochs-1) and (i == len(trainLoader)-1)):\n",
    "    with torch.no_grad():\n",
    "        fake = netG(view1,view2).detach().cpu()\n",
    "    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Visualize and Save GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,8))\n",
    "plt.plot(G_losses, 'r', label='G losses')\n",
    "plt.plot(D_losses, 'b', label='D losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,8))\n",
    "plt.plot(D_X,'b', label='D_X')\n",
    "plt.plot(D_Gx1, 'r', label='D_Gx1')\n",
    "plt.plot(D_Gx2, 'g', label='D_Gx2')\n",
    "plt.plot(IoU, 'o-', label='IoU')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dict2 = {'timestamp':str(datetime.datetime.now()),\n",
    "               'note':'GAN training model transfered from Scratch using lossG = log(D(G(x))) + 50*FocalLoss([0.15,0.25,0.6])',\n",
    "               'in_c':in_c,\n",
    "               'en_sz':en_sz,\n",
    "               'de_sz':de_sz,\n",
    "               'de3d_sz':de3d_sz,\n",
    "               'final_sz':final_sz,\n",
    "               'netG_state_dict':netG.state_dict(),\n",
    "               'nc':nc,\n",
    "               'ndf':ndf,\n",
    "               'netD_state_dict':netD.state_dict(),\n",
    "               'G_losses':G_losses,\n",
    "               'D_losses':D_losses,\n",
    "               'Recon_losses':Recon_losses,\n",
    "               'D_Gx1':D_Gx1,\n",
    "               'D_Gx2':D_Gx2,\n",
    "               'D_X':D_X,\n",
    "               'IoU':IoU,\n",
    "               'optimizerG_state_dict': optimizerG.state_dict(),\n",
    "               'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "              }\n",
    "\n",
    "saved_name2 = 'trained\\\\Recon2X3D5GAN_22022201.pt'\n",
    "print('Save as: {}'.format(saved_name2))\n",
    "#save_logic = int(input('Confirm saved name = {} : [1] Save [0] Not Save ?'.format(saved_name2)))\n",
    "save_logic = 1\n",
    "if save_logic==1:\n",
    "    torch.save(saved_dict2, saved_name2)\n",
    "    print(' --- END --- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main Execution\n",
    "\n",
    "* mode = 1:   Transfer learning\n",
    "* mode = 2:   Training from scratch\n",
    "* mode = 3:   Evaluation model\n",
    "* mode = 4:   Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Main Execution (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T04:36:53.736689Z",
     "start_time": "2020-10-23T04:36:51.866422Z"
    },
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "runningMode = int(input('Select running mode: '))\n",
    "if runningMode==1 or runningMode==2:\n",
    "    epoch_number = int(input('Number of epoch: '))\n",
    "    batch_sz = int(input('Batch size: '))\n",
    "elif runningMode==3:\n",
    "    batch_sz = 1\n",
    "    \n",
    "# Define dataLoader\n",
    "train_transformedFemur = FemurDataset2(csv_file=training_file, root_dir=root_dir, \n",
    "                                      transform=transforms.Compose([NormalizeSample2(),ToTensor8()]))\n",
    "val_transformedFemur = FemurDataset2(csv_file=val_file, root_dir=root_dir, \n",
    "                                    transform=transforms.Compose([NormalizeSample2(),ToTensor8()]))\n",
    "trainLoader = DataLoader(train_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=4)\n",
    "valLoader = DataLoader(val_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=4)\n",
    "print('   Training set : {}'.format(len(trainLoader)))\n",
    "print('   Validation set : {}'.format(len(valLoader)))\n",
    "learning_rate = 1e-4             # deflaut = 5e-4\n",
    "\n",
    "#criterion = BCEDiceLoss(0.75,0.25).to(device=device)\n",
    "'''posWeight = (0.005,0.015,0.98)   # tunning classes imbalance\n",
    "criterion = MulticlassBCEDiceLoss(0.75, 0.25, posWeight, 'mean').to(device=device)'''\n",
    "'''criterion = FocalBCETverskyLoss(ratio=0.8 ,alpha1=1, gamma1=2, \n",
    "                                alpha2=1, beta2=1, gamma2=1,\n",
    "                                reduction='mean', smooth=1e-4,\n",
    "                               )'''\n",
    "#criterion = BinaryTverskyLossV2(alpha=0.35, beta=0.65, reduction='mean')\n",
    "#criterion = FocalBinaryTverskyLoss(alpha=0.5, beta=0.5, gamma=1.0, reduction='mean')\n",
    "#criterion = FEWFocalLoss2(alpha=0.5, gamma=5, reduction='mean')    # alpha + (1-alpha) = 1\n",
    "#criterion = BinaryFocalLoss(alpha=1, gamma=2, reduction='mean')    # alpha > positive real number\n",
    "weight = torch.tensor([0.15,0.25,0.6], device=device)\n",
    "#weight = torch.tensor([0.5805325277511839, 0.8178515226802764, 0.853697532760545], device=device)  # Tuning\n",
    "criterion = FocalLossMulticlass(weight=weight, gamma=2.0, reduction='sum')\n",
    "#criterion = HausdorffERLoss(alpha=2.0)\n",
    "\n",
    "# Select running mode\n",
    "if runningMode==1 or runningMode==3:      # Transfer learning\n",
    "    model_name = 'trained\\Recon2X3D5_21100402.pt'\n",
    "    print('   ### Loading trained model : {} ### \\n'.format(model_name))\n",
    "    checkpoint = torch.load(model_name, map_location=device)\n",
    "    ### Load checkpoint ###\n",
    "    timestamp = checkpoint['timestamp']\n",
    "    note = checkpoint['note']\n",
    "    en_sz = checkpoint['en_sz']\n",
    "    de_sz = checkpoint['de_sz']\n",
    "    de3d_sz = checkpoint['de3d_sz']\n",
    "    final_sz = checkpoint['final_sz']\n",
    "    train_loss_history = checkpoint['train_loss_history']\n",
    "    train_acc_history = checkpoint['train_acc_history']\n",
    "    val_loss_history = checkpoint['val_loss_history']\n",
    "    val_acc_history = checkpoint['val_acc_history']\n",
    "    \n",
    "    ### Print state ###\n",
    "    print('Model: en_sz = {}'.format(en_sz))\n",
    "    print('Model: de_sz = {}'.format(de_sz))\n",
    "    print('Model: de3d_sz = {}'.format(de3d_sz))\n",
    "    print('Model: final_sz = {}'.format(final_sz))\n",
    "    print('Note: {} \\n'.format(note))\n",
    "    print('Timestamp = {}'.format(timestamp))\n",
    "    print('Total iterations = {:,}'.format(len(train_loss_history)))\n",
    "    print('Average training accuracy = {:.4f}'.format(max(train_acc_history)))\n",
    "    print('Average validation accuracy = {:.4f}'.format(max(val_acc_history)))\n",
    "    \n",
    "    ### Create Model ###\n",
    "    model_select = int(input('Select model \\n1) Recon3DUNet Old\\n2) Recon3DUNet\\n3) Recon3DDenseUNet'\\\n",
    "                             '\\n4) Recon3DDenseUNet2 \\n5) Recon2X3D \\n6) Recon2X3D2 \\n7) Recon2X3D3 '\\\n",
    "                             '\\n8) Recon2X3D4 \\n9) Recon2X3D5 \\n10) Recon2X3D6 \\n\\n INPUT: '))\n",
    "    if model_select==2:\n",
    "        model = Recon3DUNet(1, en_sz, de_sz, final_sz)\n",
    "    elif model_select==3:\n",
    "        model = Recon3DDenseUNet(1, en_sz, de_sz, final_sz)\n",
    "    elif model_select==4:\n",
    "        de3d_sz = checkpoint['de3d_sz']\n",
    "        model = Recon3DDenseUNet2(1, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    elif model_select==5:\n",
    "        model = Recon2X3D(1, en_sz, de_sz, final_sz)\n",
    "    elif model_select==6:\n",
    "        de3d_sz = checkpoint['de3d_sz']\n",
    "        model = Recon2X3D2(1, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    elif model_select==7:\n",
    "        de3d_sz = checkpoint['de3d_sz']\n",
    "        model = Recon2X3D3(1, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    elif model_select==8:\n",
    "        de3d_sz = checkpoint['de3d_sz']\n",
    "        model = Recon2X3D4(1, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    elif model_select==9:\n",
    "        model = Recon2X3D5(1, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    elif model_select==10:\n",
    "        model = Recon2X3D6(1, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    \n",
    "    ### Load state dict to Model ###\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, threshold=1e-3, \n",
    "                              threshold_mode='rel', cooldown=0, min_lr=0, verbose=True)\n",
    "    model.to(device=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])                 # default: strict=False\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])         # default: strict=False\n",
    "    #print('Model parameters: \\n   {}'.format(model.parameters))\n",
    "    #print('optimizer parameters: {}\\n'.format(optimizer))\n",
    "    print('\\n----- Finished loading -----\\n')\n",
    "    \n",
    "    \n",
    "elif runningMode == 2:    # Training from scratch\n",
    "    print('   ### Start training from scratch ### \\n')\n",
    "    train_loss_history = list()\n",
    "    train_acc_history = list()\n",
    "    val_loss_history = list()\n",
    "    val_acc_history = list()\n",
    "    \n",
    "    # Define new\n",
    "    model_select = int(input('Select model \\n1) Recon3DUNet Old\\n2) Recon3DUNet\\n3) Recon3DDenseUNet'\\\n",
    "                             '\\n4) Recon3DDenseUNet2 \\n5) Recon2X3D \\n6) Recon2X3D2 \\n7) Recon2X3D3 '\\\n",
    "                             '\\n8) Recon2X3D4 \\n9) Recon2X3D5 \\n10) Recon2X3D6 \\n\\n INPUT: '))\n",
    "    if model_select==2:    # Recon3DUNet\n",
    "        print('model = Recon3DUNet\\n')\n",
    "        en_sz = [[2,4],[8,16],[32,64],[128,256],[256,256]]      # Encode-dimension\n",
    "        de_sz = [[256,256],[256,256],[256,256],[256,256]]           # Decode-dimension\n",
    "        final_sz = [256]                                        # Classify-dimension\n",
    "        model = Recon3DUNet(1, en_sz, de_sz, final_sz)\n",
    "    elif model_select==3:    # Recon3DDenseUNet\n",
    "        print('model = Recon3DDenseUNet\\n')\n",
    "        in_c = 1\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4]]       # encoder with dense connection\n",
    "        de_sz = [[256,256],[256,256],[256,256],[256,256]]                    # decoder dimension\n",
    "        #final_sz = [256]           # For 2D-classifier dimension use with self.final_sz\n",
    "        final_sz = [16,16]        # For 3D-classifier dimension use with self.final_sz2 and self.final_sz3\n",
    "        model = Recon3DDenseUNet(in_c, en_sz, de_sz ,final_sz)\n",
    "    elif model_select==4:    # Recon3DDenseUNet2\n",
    "        print('model = Recon3DDenseUNet2 (with 3D-Conv Decoder) \\n')\n",
    "        in_c = 1\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4]]   # Encoder with dense-connection [input_C,deep,level]\n",
    "        de_sz = [[256,256],[256,256],[256,256],[256,256]]             # Decoder dimension\n",
    "        de3d_sz = [[1,8,8],[1,8,8],[1,8,8],[1,8,8]]                   # 3D-decoder\n",
    "        final_sz = [[16,8],[16,8],[16,8]]                     # 3D-Reconstruction\n",
    "        model = Recon3DDenseUNet2( in_c, en_sz , de_sz , de3d_sz, final_sz)\n",
    "    elif model_select==5:    # Recon2X3D\n",
    "        print('model = Recon2X3D\\n')\n",
    "        in_c = 1\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4]]         # encoder with dense connection\n",
    "        de_sz = [[256,256],[256,256],[256,256],[256,256]]                       # decoder dimension\n",
    "        final_sz = [[2,1],[3,1],[3,1],[3,3]]                              # classifier by nn.Conv3d\n",
    "        #final_sz = [[256],[256],[256],[256]]                               # classifier by nn.Conv2d\n",
    "        model = Recon2X3D(in_c, en_sz, de_sz, final_sz)\n",
    "    elif model_select==6:    # Recon2X3D2\n",
    "        print('model = Recon2X3D2')\n",
    "        in_c = 1\n",
    "        en_sz = [[16,16,4],[80,16,4],[144,16,4],[208,16,4],[272,16,4],[336,16,4]]    # encoder with dense connection\n",
    "        de_sz = [[128,16],[128,32],[128,64],[128,128],[256,256]]              # decoder dimension\n",
    "        de3d_sz = [[1,4],[1,4],[1,4],[1,4],[1,4]]\n",
    "        final_sz = [[8,4],[12,4],[12,4],[12,4],[12,12]]\n",
    "        model = Recon2X3D2(in_c, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    elif model_select==7:\n",
    "        print('model = Recon2X3D3')\n",
    "        in_c = 1\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4]]    # encoder with dense connection\n",
    "        de_sz = [[128,32],[128,64],[128,128],[256,256]]                # decoder dimension\n",
    "        de3d_sz = [[1,4],[1,4],[1,4],[1,4]]\n",
    "        final_sz = [[4,8],[4,8],[4,8],[4,8]]                           # 3D Tensor pyramid connection\n",
    "        model = Recon2X3D3(in_c, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    elif model_select==8:\n",
    "        print('model = Recon2X3D4')\n",
    "        in_c = 1\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4]]    # encoder with dense connection\n",
    "        de_sz = [[128,32],[128,64],[128,64],[256,128]]               # decoder dimension\n",
    "        de3d_sz = [[256*256, 1024*4, 256*256]]             # Linear fusion of AP and LAT view\n",
    "        final_sz = [[1,8]]                                        # 3D Tensor pyramid connection\n",
    "        model = Recon2X3D4(in_c, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    elif model_select==9:\n",
    "        print('model = Recon2X3D5')\n",
    "        in_c = 1\n",
    "        de3d_sz = [[32,8],[24,6],[14,4],[8,2]] # don't use now\n",
    "        \n",
    "        # Six level (Averaging feature)\n",
    "        '''en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4]]    # encoder with dense connection\n",
    "        de_sz = [[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]              # decoder dimension\n",
    "        final_sz = [[1,32,32],[1,32,32],[1,16,16],[1,16,16],[1,16,16],[1,16,16]]'''\n",
    "        \n",
    "        # Seven level (Averaging feature)\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "        de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "        final_sz = [[1,32,32],[1,32,32],[1,32,32],[1,16,16],[1,16,16],[1,16,16],[1,16,16]] # 2-fusion layer\n",
    "        #final_sz = [[1,16,16,16],[1,16,16,16],[1,16,16,16],[1,16,16,16],[1,16,16,16],[1,16,16,16],[1,16,16]] # 3-fusion layer\n",
    "        \n",
    "        # Concatenation wtih arithmetic features (+,*,**2)\n",
    "        '''en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "        #de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]               # work\n",
    "        #final_sz = [[2,32,32],[2,32,32],[2,32,32],[2,16,16],[2,16,16],[2,16,16],[2,16,16]]   # work\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "        de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "        #final_sz = [[2,32],[3,32],[3,32],[3,16],[3,16],[3,16],[3,16]]                                  # 1-fusion layer\n",
    "        final_sz = [[2,32,32],[3,32,32],[3,32,32],[3,16,16],[3,16,16],[3,16,16],[3,16,16]]              # 2-fusion layer\n",
    "        #final_sz = [[2,16,16,16],[3,16,16,16],[3,16,16,16],[3,16,16,16],[3,16,16,16],[3,16,16,16],[3,16,16,16]] # 3-fusion layer\n",
    "        '''\n",
    "        # Concatenation of Sub-feature-volume\n",
    "        '''en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "        de_sz = [[64,4*8],[128,8*8],[128,16*4],[256,32*4],[256,64*2],[256,128*2],[256,256]] \n",
    "        final_sz = [[16,32,32],[16+4,32,32],[8+4,32,32],[8+4,16,16],[4+4,16,16],[4+4,16,16],[2+4,16,16]]'''\n",
    "        \n",
    "        model = Recon2X3D5(in_c, en_sz, de_sz, de3d_sz, final_sz)\n",
    "        \n",
    "    elif model_select==10:\n",
    "        print('model = Recon2X3D6')\n",
    "        in_c = 1\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "        de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "        de3d_sz = [[8,8,4],[16,16,8],[32,32,16],[64,64,32],[128,128,64],[256,256,128],[512,512,256]] # 2-axial layer per level\n",
    "        final_sz = [[3,32,32],[6,32,32],[6,32,32],[6,16,16],[6,16,16],[6,16,16],[6,16,16]]   # cat(X, XD,XH,XW)\n",
    "        #final_sz = [[3,32,32],[3,32,32],[3,32,32],[3,16,16],[3,16,16],[3,16,16],[3,16,16]]   #  avg[X ,cat(XD,XH,Xw)]\n",
    "        \n",
    "        # 6-Levels\n",
    "        '''en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4]]\n",
    "        de_sz = [[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "        de3d_sz = [[16,16,8],[32,32,16],[64,64,32],[128,128,64],[256,256,128],[512,512,256]] # 2-axial layer per level\n",
    "        final_sz = [[3,32,32],[11,32,32],[11,32,32],[11,16,16],[11,16,16],[11,16,16]]   # cat(X, XD,XH,XW)'''\n",
    "        \n",
    "        model = Recon2X3D6( in_c, en_sz, de_sz, de3d_sz, final_sz)\n",
    "        \n",
    "    \n",
    "    # Create new model\n",
    "    #model = nn.DataParallel(model, device_ids=[0, 1])         # Training on all available GPU\n",
    "    model.to(device=device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, threshold=1e-3, \n",
    "                              threshold_mode='rel', cooldown=0, min_lr=1e-12, verbose=True)\n",
    "    # Print state\n",
    "    print('Model parameters: {}\\n'.format(model.parameters))\n",
    "\n",
    "elif runningMode == 4:\n",
    "    print('   ### Hyperparameter Tuning ### \\n')\n",
    "    train_loss_history = list()\n",
    "    train_acc_history = list()\n",
    "    val_loss_history = list()\n",
    "    val_acc_history = list()\n",
    "    model = define_model_trial(trial).to(device=device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, threshold=1e-3, \n",
    "                              threshold_mode='rel', cooldown=0, min_lr=1e-12, verbose=True)\n",
    "    \n",
    "\n",
    "if runningMode==1 or runningMode==2:\n",
    "    data_parallel_mode = int(input('Activate nn.DataParallet  [0] No  [1] Yes  : '))\n",
    "    if data_parallel_mode == 1:\n",
    "        model = nn.DataParallel(model)\n",
    "else:\n",
    "    data_parallel_mode = 0\n",
    "\n",
    "# Execution loop\n",
    "if runningMode == 1 or runningMode == 2:\n",
    "    print('\\nTotal iterations = {:,}'.format(len(train_loss_history)))\n",
    "    timeStr = str(datetime.datetime.now())\n",
    "    print('Start training at : {}'.format(timeStr))\n",
    "    \n",
    "    timeT1 = time.time()\n",
    "    if USE_GPU and torch.cuda.is_available():\n",
    "        print('\\n --- Use mixed precision training ---\\n')\n",
    "        #train_loss_history,train_acc_history,val_loss_history,val_acc_history = train_mixed(model, trainLoader, valLoader, optimizer, scheduler, criterion, epochs=epoch_number)\n",
    "        saved_dict = train_mixed(model, trainLoader, valLoader, optimizer, scheduler, criterion, epochs=epoch_number)\n",
    "        #train_mixed(model, trainLoader, valLoader, optimizer, scheduler, criterion, epochs=1):\n",
    "    else:\n",
    "        assert '!!! Training with CPU not available !!!'\n",
    "        train(model, trainLoader, valLoader, optimizer, scheduler, criterion, epochs=epoch_number)\n",
    "    timeT2 = time.time()\n",
    "    print('\\nTotal training time = {} min. \\nTotal epoch = {}\\n'.format((timeT2-timeT1)/60, len(val_acc_history)))\n",
    "    timeStr = str(datetime.datetime.now()) \n",
    "    print('Finish task at : {}\\n'.format(timeStr))\n",
    "elif runningMode == 4:\n",
    "    print('\\n##### Hyperameter Tuning #####')\n",
    "    timeStr = str(datetime.datetime.now())\n",
    "    print('Start training at : {}'.format(timeStr))\n",
    "    timeT1 = time.time()\n",
    "    #Running Hyperparameter Search\n",
    "    timeT2 = time.time()\n",
    "    print('\\nTotal training time = {} min. \\nTotal epoch = {}\\n'.format((timeT2-timeT1)/60, len(val_acc_history)))\n",
    "    timeStr = str(datetime.datetime.now()) \n",
    "    print('Finish task at : {}\\n'.format(timeStr))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recon2X3D5_21101901\t\tFracReconNet\n",
    "\n",
    "Recon2X3D5_21102201\t\t3DReconNet-Ac\n",
    "\n",
    "Recon2X3D5_21102101\t\tFracAug only\n",
    "\n",
    "Recon2X3D5_21102102\t\tBare    Recon2X3D5_21100402  3DReconNet\n",
    "\n",
    "NormalizeSample2()  use with FemurDataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T04:36:53.736689Z",
     "start_time": "2020-10-23T04:36:51.866422Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "runningMode = int(input('Select running mode: '))\n",
    "if runningMode==1 or runningMode==2:\n",
    "    epoch_number = int(input('Number of epoch: '))\n",
    "    batch_sz = int(input('Batch size: '))\n",
    "elif runningMode==3:\n",
    "    batch_sz = 1\n",
    "    \n",
    "toTensorMode = int(input('Select ToTensor : [8] Auxiliary class  [9] Native = '))\n",
    "if toTensorMode == 8:\n",
    "    print('ToTensor8 selected')\n",
    "    ToTensor = ToTensor8()\n",
    "elif toTensorMode == 9:\n",
    "    print('Totensor9 selected')\n",
    "    ToTensor = ToTensor9()\n",
    "elif toTensorMode < 8:\n",
    "    print('Totensor1-7 selected  change NormalizeSample() and FemurDataset')\n",
    "    ToTensor = ToTensor7()\n",
    "else:\n",
    "    raise ValueError ('Invalid ToTensor input')\n",
    "print('ToTensor = {}'.format(ToTensor))\n",
    "\n",
    "# Define dataLoader\n",
    "num_workers = 4\n",
    "\n",
    "train_transformedFemur = FemurDataset2(csv_file=training_file, root_dir=root_dir, \n",
    "                                      transform=transforms.Compose([NormalizeSample2(), ToTensor]))\n",
    "val_transformedFemur = FemurDataset2(csv_file=val_file, root_dir=root_dir, \n",
    "                                    transform=transforms.Compose([NormalizeSample2(), ToTensor]))\n",
    "trainLoader = DataLoader(train_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=num_workers)\n",
    "valLoader = DataLoader(val_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=num_workers)\n",
    "print('   Training set : {}'.format(len(trainLoader)))\n",
    "print('   Validation set : {}'.format(len(valLoader)))\n",
    "\n",
    "learning_rate = 1e-4             # deflaut = 5e-4\n",
    "weight = torch.tensor([0.15,0.25,0.6], device=device)     # For ToTensor8\n",
    "#weight = torch.tensor([0.5,0.5], device=device)           # For ToTensor9\n",
    "criterion = FocalLossMulticlass(weight=weight, gamma=2.0, reduction='sum')\n",
    "\n",
    "saved_name = 'trained\\Recon2X3D5_21101901.pt' # Recon2X3D5_21101901 Recon2X3D5GAN_22022201  Recon2X3D6_22060701  unalign{Recon2X3D5_22081001 vs Recon2X3D5_22081701}\n",
    "confirm_saved_name = str(input('Confirm Save : ' + saved_name + ' [y/n] ? '))\n",
    "while confirm_saved_name!='y':\n",
    "    saved_name = str('trained\\\\') + str(input('Enter saved_name = trained\\ ')) + str('.pt')\n",
    "    confirm_saved_name = str(input('Confirm Save : ' + saved_name + ' [y/n] ? '))\n",
    "print('*** Confirm saved_name = {} ***\\n'.format(saved_name))\n",
    "\n",
    "# Select running mode\n",
    "if runningMode==1 or runningMode==3:      # Transfer learning, Testing or Inference\n",
    "    print('### Loading trained model : {} ### \\n'.format(saved_name))\n",
    "    saved_dict = torch.load(saved_name, map_location=device)\n",
    "    \n",
    "    ### Print state ###\n",
    "    saved_dict['in_c'] = 1  # For the Obsolete Model\n",
    "    print('Model: in_c  = {}'.format(saved_dict['in_c']))\n",
    "    print('Model: en_sz = {}'.format(saved_dict['en_sz']))\n",
    "    print('Model: de_sz = {}'.format(saved_dict['de_sz']))\n",
    "    print('Model: de3d_sz = {}'.format(saved_dict['de3d_sz']))\n",
    "    print('Model: final_sz = {}'.format(saved_dict['final_sz']))\n",
    "    print('Note: {} \\n'.format(saved_dict['note']))\n",
    "    print('Timestamp = {}'.format(saved_dict['timestamp']))\n",
    "    #print('Total iterations = {:,}'.format(len(saved_dict['train_loss_history'])))\n",
    "    #print('Average training accuracy = {:.4f}'.format(max(saved_dict['train_acc_history'])))\n",
    "    #print('Average validation accuracy = {:.4f}'.format(max(saved_dict['val_acc_history'])))\n",
    "    \n",
    "    ### Create Model ###\n",
    "    model_select = int(input('Select model \\n1) Recon2X3D5 \\n2) Recon2X3D6 \\n3) Recon2X3D5-GAN \\n\\n INPUT: '))\n",
    "    if model_select==1:\n",
    "        model = Recon2X3D5(saved_dict['in_c'], saved_dict['en_sz'], saved_dict['de_sz'], \n",
    "                           saved_dict['de3d_sz'], saved_dict['final_sz'])  # \n",
    "    elif model_select==2:\n",
    "        model = Recon2X3D6(saved_dict['in_c'], saved_dict['en_sz'], saved_dict['de_sz'], \n",
    "                           saved_dict['de3d_sz'], saved_dict['final_sz'])\n",
    "    elif model_select==3:\n",
    "        netD = Discriminator3D(saved_dict['nc'],saved_dict['ndf']).to(device=device)\n",
    "        netG = Recon2X3D5(saved_dict['in_c'], saved_dict['en_sz'], saved_dict['de_sz'], \n",
    "                          saved_dict['de3d_sz'], saved_dict['final_sz']).to(device=device)\n",
    "    \n",
    "    ### Load state dict to Model ###\n",
    "    \n",
    "    '''if runningMode==1:\n",
    "        model.load_state_dict(saved_dict['model_state_dict_last'])     # Load the last trained model to train again\n",
    "    elif runningMode==3:\n",
    "        model.load_state_dict(saved_dict['model_state_dict'])          # Load the best model to test and inference'''\n",
    "    \n",
    "    model.load_state_dict(saved_dict['model_state_dict'])          # Load the best model to test and inference\n",
    "    model.to(device=device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, threshold=1e-3, \n",
    "                              threshold_mode='rel', cooldown=0, min_lr=0, verbose=True)\n",
    "    optimizer.load_state_dict(saved_dict['optimizer_state_dict'])         # default: strict=False\n",
    "    #scheduler.load_state_dict(saved_dict['scheduler_state_dict'])         # default: strict=False\n",
    "    print('odel parameters: \\n   {}'.format(model.parameters))\n",
    "    print('optimizer parameters: {}\\n'.format(optimizer))\n",
    "    print('scheduler parameters: {}\\n'.format(scheduler))\n",
    "    print('\\n----- Finished loading -----\\n')\n",
    "    \n",
    "elif runningMode == 2:    # Training from scratch\n",
    "    print('   ### Start training from scratch ### \\n')\n",
    "    note = 'Recon2X3D5 (IN/IN/IN) CAT(X1,X2,X)*2-fusion layer \\\n",
    "            (last fusion layer [3,16,16][16,3] + ToTensor8 using FocalMulticlass({0.15,0.25,0.6}, 2, sum)'\n",
    "    #note = 'Recon2X3D5 (IN/IN/IN) CAT[XD,XH,XW,X(C=3))*2-fusion layer \\\n",
    "    #(last fusion layer [3,16,16][16,2] + ToTensor9 using FocalMulticlass({0.5 0.5}, 2, sum)'\n",
    "    \n",
    "    # Define new\n",
    "    model_select = int(input('Select model \\n0) Recon2X3D4 \\n1) Recon2X3D5 \\n2) Recon2X3D6 \\n\\n INPUT: '))\n",
    "    if model_select==0:\n",
    "        print('model = Recon2X3D4')\n",
    "        in_c = 1\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4]]    # encoder with dense connection\n",
    "        de_sz = [[128,32],[128,64],[128,64],[256,128]]               # decoder dimension\n",
    "        de3d_sz = [[256*256, 1024*4, 256*256]]             # Linear fusion of AP and LAT view\n",
    "        final_sz = [[1,8]]                                        # 3D Tensor pyramid connection\n",
    "        model = Recon2X3D4(in_c, en_sz, de_sz, de3d_sz, final_sz)\n",
    "    elif model_select==1:\n",
    "        print('model = Recon2X3D5')\n",
    "        in_c = 1\n",
    "        de3d_sz = None       # don't use now\n",
    "        \n",
    "        # Six level (Averaging feature)\n",
    "        '''en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4]]    # encoder with dense connection\n",
    "        de_sz = [[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]              # decoder dimension\n",
    "        final_sz = [[1,32,32],[1,32,32],[1,16,16],[1,16,16],[1,16,16],[1,16,16]]'''\n",
    "        \n",
    "        # Seven level (Averaging feature)\n",
    "        '''en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "        de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "        final_sz = [[1,32,32],[1,32,32],[1,32,32],[1,16,16],[1,16,16],[1,16,16],[1,16,16]] # 2-fusion layer\n",
    "        #final_sz = [[1,16,16,16],[1,16,16,16],[1,16,16,16],[1,16,16,16],[1,16,16,16],[1,16,16,16],[1,16,16]] # 3-fusion layer'''\n",
    "        \n",
    "        # Concatenation wtih arithmetic features (+,*,**2)\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "        de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "        final_sz = [[2,32,32],[5,32,32],[5,32,32],[5,16,16],[5,16,16],[5,16,16],[5,16,16]]\n",
    "        \n",
    "        # Concatenation of Sub-feature-volume\n",
    "        '''en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "        de_sz = [[64,4*8],[128,8*8],[128,16*4],[256,32*4],[256,64*2],[256,128*2],[256,256]] \n",
    "        final_sz = [[16,32,32],[16+4,32,32],[8+4,32,32],[8+4,16,16],[4+4,16,16],[4+4,16,16],[2+4,16,16]]'''\n",
    "        \n",
    "        model = Recon2X3D5(in_c, en_sz, de_sz, de3d_sz, final_sz)\n",
    "        \n",
    "    elif model_select==2:\n",
    "        print('model = Recon2X3D6')\n",
    "        in_c = 1\n",
    "        en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "        de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "        de3d_sz = [[8,8,4],[16,16,8],[32,32,16],[64,64,32],[128,128,64],[256,256,128],[512,512,256]] # 2-axial layer per level\n",
    "        final_sz = [[3,32,32],[6,32,32],[6,32,32],[6,16,16],[6,16,16],[6,16,16],[6,16,16]]    # cat(X, XD, XH, XW)\n",
    "        #final_sz = [[3,32,32],[3,32,32],[3,32,32],[3,16,16],[3,16,16],[3,16,16],[3,16,16]]   #  avg[X ,cat(XD, XH, Xw)]\n",
    "        \n",
    "        # 6-Levels\n",
    "        '''en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4]]\n",
    "        de_sz = [[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "        de3d_sz = [[16,16,8],[32,32,16],[64,64,32],[128,128,64],[256,256,128],[512,512,256]] # 2-axial layer per level\n",
    "        final_sz = [[3,32,32],[11,32,32],[11,32,32],[11,16,16],[11,16,16],[11,16,16]]   # cat(X, XD,XH,XW)'''\n",
    "        \n",
    "        model = Recon2X3D6(in_c, en_sz, de_sz, de3d_sz, final_sz)\n",
    "        \n",
    "    # Create new model\n",
    "    #model = nn.DataParallel(model, device_ids=[0, 1])         # Training on all available GPU\n",
    "    model.to(device=device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, threshold=1e-3, \n",
    "                              threshold_mode='rel', cooldown=0, min_lr=1e-12, verbose=True)\n",
    "    \n",
    "    saved_dict = {'timestamp':None ,\n",
    "                  'note':note ,\n",
    "                  'in_c':in_c ,\n",
    "                  'en_sz':en_sz ,\n",
    "                  'de_sz':de_sz ,\n",
    "                  'de3d_sz':de3d_sz ,\n",
    "                  'final_sz':final_sz ,\n",
    "                  'train_loss_history':list() ,\n",
    "                  'train_acc_history':list() ,\n",
    "                  'val_loss_history':list() ,\n",
    "                  'val_acc_history':list() ,\n",
    "                  'model_state_dict':None ,\n",
    "                  #'model_state_dict_last':None ,\n",
    "                  'optimizer_state_dict':None,\n",
    "                  'scheduler_state_dict':None }\n",
    "    \n",
    "    # Print state\n",
    "    #print('Model parameters: {}\\n'.format(model.parameters))\n",
    "\n",
    "elif runningMode == 4:\n",
    "    # do not finish\n",
    "    print('   ### Hyperparameter Tuning ### \\n')\n",
    "    model = define_model_trial(trial).to(device=device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, threshold=1e-3, \n",
    "                              threshold_mode='rel', cooldown=0, min_lr=1e-12, verbose=True)\n",
    "    \n",
    "\n",
    "if runningMode==1 or runningMode==2:\n",
    "    data_parallel_mode = int(input('Activate nn.DataParallet  [0] No  [1] Yes  : '))\n",
    "    if data_parallel_mode == 1:\n",
    "        model = nn.DataParallel(model)\n",
    "else:\n",
    "    data_parallel_mode = 0\n",
    "\n",
    "# Execution loop\n",
    "if runningMode == 1 or runningMode == 2:\n",
    "    print('\\nTotal iterations = {:,}'.format(len(saved_dict['train_loss_history'])))\n",
    "    print('Start training at : {}'.format(str(datetime.datetime.now())))\n",
    "    \n",
    "    timeT1 = time.time()\n",
    "    if USE_GPU and torch.cuda.is_available():\n",
    "        print('\\n --- Use mixed precision training ---\\n')\n",
    "        saved_dict = train_mixed(model, trainLoader, valLoader, optimizer, scheduler, criterion, \n",
    "                                 batch_sz, epoch_number, saved_name, saved_dict)\n",
    "    else:\n",
    "        assert USE_GPU and torch.cuda.is_available(), '!!! Training with CPU not available !!!'\n",
    "        \n",
    "    timeT2 = time.time()\n",
    "    print('\\nTotal training time = {} hours \\nTotal epoch = {}\\n'.format((timeT2-timeT1)/3600, len(saved_dict['val_acc_history'])))\n",
    "    print('Saved Name: {}'.format(saved_name))\n",
    "    print('Finish task at : {}\\n'.format(str(datetime.datetime.now()) ))\n",
    "    \n",
    "elif runningMode == 4:\n",
    "    print('\\n##### Hyperameter Tuning #####')\n",
    "    print('Start training at : {}'.format(str(datetime.datetime.now()) ))\n",
    "    timeT1 = time.time()\n",
    "    #Running Hyperparameter Search\n",
    "    timeT2 = time.time()\n",
    "    print('\\nTotal training time = {} hours \\nTotal epoch = {}\\n'.format((timeT2-timeT1)/3600, len(val_acc_history)))\n",
    "    print('Finish task at : {}\\n'.format(str(datetime.datetime.now()) ))   \n",
    "#trainLoader = 43906\n",
    "#batch size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "''' Tuning parameter list:\n",
    "    \n",
    "    network_deep = depth of overall network  (optional)\n",
    "    en_sz = encoder dimension : [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "    de_sz = decoder dimension : [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "    final_sz = fusion dimension : [[1,32,32],[1,32,32],[1,32,32],[1,16,16],[1,16,16],[1,16,16],[1,16,16]]\n",
    "    \n",
    "    weight = [w1,w2,w3] for background, bone and fracture occupancy, respectively in the Focal loss\n",
    "    gamma = focus rate of the Focal loss\n",
    "    lr = learning rate\n",
    "'''\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "epochs = 3\n",
    "batch_sz = 3\n",
    "train_transformedFemur = FemurDataset(csv_file=training_file, root_dir=root_dir, \n",
    "                                      transform=transforms.Compose([NormalizeSample(),ToTensor6()]))\n",
    "val_transformedFemur = FemurDataset(csv_file=val_file, root_dir=root_dir, \n",
    "                                    transform=transforms.Compose([NormalizeSample(),ToTensor6()]))\n",
    "trainLoader = DataLoader(train_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=4)\n",
    "valLoader = DataLoader(val_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=4)\n",
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = list(), list(), list(), list()\n",
    "\n",
    "def define_model_trial(trial):\n",
    "    de3d_sz = [[32,8],[24,6],[14,4],[8,2]]  # ignore\n",
    "    en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "    de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "    final_sz = [[1,32,32],[1,32,32],[1,32,32],[1,16,16],[1,16,16],[1,16,16],[1,16,16]]\n",
    "    return Recon2X3D5(1, en_sz, de_sz, de3d_sz, final_sz)\n",
    "\n",
    "def objective(trial):\n",
    "    # Setup model and hyperparameter\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(3)\n",
    "    model = define_model_trial(trial).to(device=device)\n",
    "    model = nn.DataParallel(model, device_ids=[0,1])\n",
    "    w1 = trial.suggest_float(\"w1\", 1e-2, 1.00, log=False)   # for background occupancy\n",
    "    w2 = trial.suggest_float(\"w2\", 1e-2, 1.00, log=False)   # for bone occupancy\n",
    "    w3 = trial.suggest_float(\"w3\", 1e-2, 1.00, log=False)   # for fracture occupancy\n",
    "    weight = torch.tensor([w1,w2,w3], device=device)\n",
    "    gamma = trial.suggest_discrete_uniform(\"gamma\", 1, 5, 0.25)\n",
    "    criterion = FocalLossMulticlass(weight=weight, gamma=gamma, reduction='sum')\n",
    "    learning_rate = trial.suggest_categorical(\"lr\", [2.5e-4, 1e-4, 5e-5, 2.5e-5, 1e-5])\n",
    "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    #optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, threshold=1e-3, \n",
    "                                  threshold_mode='rel', cooldown=0, min_lr=0, verbose=True)\n",
    "    # Traning and Validation loop\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    stepShow = round(len(trainLoader)/11)\n",
    "    for e in range(epochs):\n",
    "        time1 = time.time()\n",
    "        print('----- Epoch = {} ----- # Learning rate = {:.4e}'.format(e+1, optimizer.param_groups[0][\"lr\"]))\n",
    "        max_train_acc, train_loss, train_acc, train_acc2, max_val_acc, val_loss, val_acc, val_acc2 = 0, 0, 0, 0, 0, 0, 0 ,0\n",
    "        hd = HausdorffDistance()\n",
    "        for t, train_sample in enumerate(trainLoader):\n",
    "            if len(train_sample)%batch_sz != 0 or t==len(trainLoader)-1:  # exclude inequal batch\n",
    "                #print('Final training accuracy = {:.4f}'.format(acc))\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                model.train(mode=True)    # put model to training mode\n",
    "                target = train_sample['Target'].to(device=device, dtype=dtype1)\n",
    "                ap = train_sample['AP'].to(device=device, dtype=dtype1)\n",
    "                lat = train_sample['LAT'].to(device=device, dtype=dtype1)\n",
    "                \n",
    "                # calculation of output\n",
    "                output = model(ap,lat)\n",
    "                \n",
    "                # Calculate loss of this sample batch\n",
    "                #loss = criterion(output,target[0:2]).to(device=device)  # calculate loss from both foreground and background (and fracMask)\n",
    "                loss = criterion(output, target.long())          # for ToTensor6 with FocalLossMulticlass\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # Check accuracy\n",
    "                #acc = iou((output[:,1]>=0.5).float(),target[:,1])                              # for ToTensor1-5\n",
    "                #acc2 = iou((output[:,0]>=0.5).float(), (target[:,0]).float())                  # for ToTensor1-5\n",
    "                #acc2 = hd.compute((output[:,2:3]>=0.5).float(), (target[:,2:3]>=0.5).float())  # for ToTensor1-5\n",
    "                acc = iou((output[:,1]>=0.5).float(), (target==1).float())         # for ToTensor6\n",
    "                acc2 = iou((output[:,2]>=0.5).float(), (target==2).float())        # for ToTensor6\n",
    "                train_acc += acc.detach()\n",
    "                \n",
    "                if acc > max_train_acc:    # record max.accuracy in current batch\n",
    "                    max_train_acc = acc\n",
    "                if t%(round(len(trainLoader)/10)) == 0:\n",
    "                    print('Iteration: {}   |   Loss = {:.4f}   |   Accuracy = {:.4f} {:.4f}'.format(t, loss.item(), acc, acc2))\n",
    "                if t==len(trainLoader)-1:\n",
    "                    print('Final training accuracy = {:.4f}'.format(acc))\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            torch.cuda.synchronize()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        # Append Loss and Accuracy history\n",
    "        train_loss /= round(len(trainLoader)/batch_sz)\n",
    "        train_acc /= round(len(trainLoader)/batch_sz)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        print('# Training loss = {:.4f}'.format(train_loss))\n",
    "        print('Training accuracy = {:.4f}'.format(train_acc))\n",
    "        print('Max. Training accuracy = {:.4f}'.format(max_train_acc))\n",
    "        \n",
    "        time2 = time.time()\n",
    "        print('Duration training time = {} Min.\\n'.format((time2-time1)/60))\n",
    "        \n",
    "###########################################################################################################################\n",
    "        \n",
    "        print('### Validation loop ###')\n",
    "        model.eval()\n",
    "        time3 = time.time()\n",
    "        for t, val_sample in enumerate(valLoader):\n",
    "            if len(val_sample)%batch_sz != 0 or t==len(valLoader)-1: # exclude inequal batch\n",
    "                #print('Final validation accuracy = {:,.4f}'.format(acc))\n",
    "                break\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    target = val_sample['Target'].to(device=device, dtype=dtype1)\n",
    "                    ap = val_sample['AP'].to(device=device, dtype=dtype1)\n",
    "                    lat = val_sample['LAT'].to(device=device, dtype=dtype1)\n",
    "                    output = model(ap,lat)\n",
    "                    \n",
    "                    #loss = criterion(output,target[:,0:2])         # for ToTensor1-5\n",
    "                    loss = criterion(output ,target.long())   # for ToTensor6\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    #acc = iou((output[:,1]>=0.5).float(),target[:,1])                                # for ToTensor1-5\n",
    "                    #acc2 = iou((output[:,0]>=0.5).float(), (target[:,0]).float())                    # for ToTensor1-5\n",
    "                    #acc2 = hd.compute((output[:,2:3]>=0.5).float(), (target[:,2:3]>=0.5).float())    # for ToTensor1-5\n",
    "                    acc = iou((output[:,1]>=0.5).float(), (target==1).float())                        # for ToTensor6\n",
    "                    acc2 = iou((output[:,2]>=0.5).float(), (target==2).float())                       # for ToTensor6\n",
    "                    #acc2 = hausdorff_voxel((output[:,2]>=0.5).float(), (target==2).float())          # for ToTensor6\n",
    "                    val_acc += acc.detach()\n",
    "                    val_acc2 += acc2.detach()\n",
    "                    if acc > max_val_acc:    # record max. validation accuracy in current batch\n",
    "                        max_val_acc = acc\n",
    "                    if t%(round(len(valLoader)/10)) == 0:\n",
    "                        print('Iteration: {}   |   Loss = {:,.4f}   |   Accuracy = {:.4f} {:.4f}'.format(t, loss.item(), acc, acc2))\n",
    "                    if t==len(valLoader)-1:\n",
    "                        print('Final validation accuracy = {:,.4f}'.format(acc))\n",
    "        \n",
    "        val_loss /= round(len(valLoader)/batch_sz)\n",
    "        val_acc /= round(len(valLoader)/batch_sz)\n",
    "        val_acc2 /= round(len(valLoader)/batch_sz)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        torch.cuda.synchronize()\n",
    "        print('# Validation loss = {:.4f}'.format(val_loss))\n",
    "        print('Validation accuracy = {:.4f}'.format(val_acc))\n",
    "        print('Max. Validation accuracy = {:.4f}'.format(max_val_acc))\n",
    "        scheduler.step(val_acc)\n",
    "        time4 = time.time()\n",
    "        print('Duration validation time = {} Min. \\n'.format((time4-time3)/60))\n",
    "        print('                      *****    Total epoch time = {} Min.    ***** \\n'.format((time4-time1)/60))\n",
    "        print('-'*120,'\\n')\n",
    "        \n",
    "        '''trial.report(val_acc, e)   # Trial.report is not supported for multi-objective optimization\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()'''\n",
    "        \n",
    "    return val_acc , val_acc2\n",
    "\n",
    "def callback(study, trial):  # failed\n",
    "    #print(' ##### callback def ##### ')\n",
    "    #if study.best_trial == trial:\n",
    "    #print('trial = {}'.format(trial))\n",
    "    if study.best_trials[0].number == trial.number :   # for multi-objective\n",
    "        print(' ***** Callback : Save Best Trial {} ***** '.format(trial.number))\n",
    "        timeStr = str(datetime.datetime.now()) \n",
    "        note = 'Recon2X3D5 (batch2d/batch2d/batch3d) avg(X1,X2,X)*2-fusion layer (last fusion layer [1,16,16][16,3] + ToTensor6 using FocalMulticlass({0.1 0.3 0.6}, 2, sum)'\n",
    "        save_dict = {'timestamp':timeStr ,\n",
    "                     'note':note ,\n",
    "                     'en_sz':en_sz ,\n",
    "                     'de_sz':de_sz ,\n",
    "                     'de3d_sz':de3d_sz ,\n",
    "                     'final_sz':final_sz ,\n",
    "                     'train_loss_history':train_loss_history ,\n",
    "                     'train_acc_history':train_acc_history ,\n",
    "                     'val_loss_history':val_loss_history ,\n",
    "                     'val_acc_history':val_acc_history ,\n",
    "                     #'model_state_dict':model.state_dict() ,              # without nn.DataParallel\n",
    "                     'model_state_dict':model.module.state_dict() ,        # with    nn.DataParallel\n",
    "                     'optimizer_state_dict': optimizer.state_dict()}\n",
    "        # Save Model state_dict\n",
    "        saved_name ='trained\\hyper_' + study_name +  '.pt'   # file name = hyper_Recon2X3D5_210609.pt  (example)\n",
    "        torch.save(save_dict, saved_name)\n",
    "        print('\\n   ##### Model is saved @ {}  ;\\n   ##### Trial =  {} \\n'.format(timeStr,trial))\n",
    "    \n",
    "    \n",
    "#################### Starting trial ####################\n",
    "torch.cuda.empty_cache()\n",
    "#study_name = str(input(' Input Study Name = '))\n",
    "study_name = 'Recon2X3D5_21060902'   # 'test'\n",
    "de3d_sz = [[32,8],[24,6],[14,4],[8,2]]  # ignore\n",
    "en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "final_sz = [[1,32,32],[1,32,32],[1,32,32],[1,16,16],[1,16,16],[1,16,16],[1,16,16]]\n",
    "\n",
    "hypertuneLogic = int(input('Run Hyperparameter Tuning : [1] Yes  [0] No  = '))\n",
    "storageLogic = int(input('Select saving storage [1] Remote database [2] Local : '))\n",
    "if storageLogic==1:\n",
    "    storage = 'sqlite:///recon2x3d5.db'       # 'sqlite:///example.db'\n",
    "    print('   Use Remote databases (RDB)\\nStudy name = {}   At {}\\n'.format(study_name, storage))\n",
    "    study = optuna.create_study(study_name=study_name , directions=['maximize','maximize'], \n",
    "                                storage=storage, load_if_exists=True)\n",
    "elif storageLogic==2:\n",
    "    print('   Use Local memory')\n",
    "    creatnewstudyLogic = int(input('   Input [1] Create new study: <{}>  [2] Load existing: <{}.pkl> :'.format(study_name,study_name)))\n",
    "    if creatnewstudyLogic==1:   # create new study\n",
    "        study = optuna.create_study(study_name=study_name , direction=['maximize','maximize'])\n",
    "    elif creatnewstudyLogic==2:  # load existing study\n",
    "        study = joblib.load(r'trained\\hyper_' + study_name + '.pkl')\n",
    "        \n",
    "print('   Study = {} | {}'.format(study_name ,study))\n",
    "print(\"   Number of finished trials: \", len(study.trials))\n",
    "print('   Sampler = {} '.format(study.sampler.__class__.__name__))\n",
    "print('   Number of Training set : {}'.format(len(trainLoader)))\n",
    "print('   Number of Validation set : {}'.format(len(valLoader)))\n",
    "\n",
    "if hypertuneLogic==1:   # Hyperparameter Tuning\n",
    "    n_trials = int(input('Input number of trials (n_trials) = '))\n",
    "    print()\n",
    "    time1 = time.time()\n",
    "    #study.optimize(objective, n_trials=n_trials)\n",
    "    study.optimize(objective, n_trials=n_trials, callbacks=[callback])  # Callback to save the best trained pytorch model\n",
    "    if storageLogic==1:\n",
    "        joblib.dump(study, r'trained\\hyper_' + study_name + '.pkl')  # Save into local storage\n",
    "    time2 = time.time()\n",
    "    print(' ##### Total optimizing time = {} Hours #####\\n'.format((time2-time1)/3600))\n",
    "\n",
    "print(\"  Study : {} \".format(study_name))\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(study.get_trials(deepcopy=False, states=[TrialState.PRUNED])))\n",
    "print(\"  Number of complete trials: \", len(study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])))\n",
    "\n",
    "print(\"\\nBest trial number: {}\".format(study.best_trials[0].number))\n",
    "trial = study.best_trials[0]\n",
    "print(\"  Value: \", trial.values)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nBest trial number: {}\".format(study.best_trials[0].number))\n",
    "trial = study.best_trials[0]\n",
    "print(\"  Value: \", trial.values)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "study.best_trials[0].number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial = study.best_trials\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#help(study.trials[2])\n",
    "for i, trial in enumerate(study.trials):\n",
    "    print('Trial:{}   value = {}   Duration={} Hr.Min.Sec.milSec'.format(i, trial.values, trial.duration))\n",
    "    print('Params: {}\\n'.format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#print(\"   Number of finished trials: \", len(study.trials))\n",
    "study.best_trials    # Get best trial's information.\n",
    "#study.trials        # Get all trials' information.\n",
    "\n",
    "#study.best_params   # Get best parameters for the objective function.\n",
    "#study.best_value    # Get best objective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For Deleting The Study from Remoting DataBase\n",
    "delete_logic = int(input('Are you sure to delete The Study: < {} > ? ; [1] Yes  [0] No = '.format(study_name)))\n",
    "if delete_logic==1:\n",
    "    optuna.delete_study(study_name=study_name , storage=storage)\n",
    "    print('*** The study deleted ***')\n",
    "else:\n",
    "    print('*** Do not delete the study ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Visualize Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study,target=['val_acc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plot_contour(study)\n",
    "plot_contour(study, params=['w1','w2','w3'])   # params [Str] = gamma , w1 , w2 , w3 , lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_edf(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Multi-objective\n",
    "optuna.visualization.plot_pareto_front(study, target_names=[\"val_acc\", \"val_acc2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''val_loss_history.append(125478.0054)\n",
    "val_acc_history.append(2.402127273)'''\n",
    "print(saved_dict['train_loss_history'])\n",
    "print(saved_dict['train_acc_history'])\n",
    "print(saved_dict['val_loss_history'])\n",
    "print(saved_dict['val_acc_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning curve and accuracy\n",
    "\n",
    "print('Total iterations = {:}'.format(len(saved_dict['train_loss_history'])))\n",
    "print('Min. Training loss = {:,.4f}'.format(min(saved_dict['train_loss_history'])))\n",
    "print('Max. Training accuracy = {:.4f}'.format(max(saved_dict['train_acc_history'])))\n",
    "print('Min. Validation loss = {:,.4f}'.format(min(saved_dict['val_loss_history'])))\n",
    "print('Max. Validation accuracy = {:.4f}'.format(max(saved_dict['val_acc_history'])))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(saved_dict['train_loss_history'],'r',label='Training')\n",
    "plt.plot(saved_dict['val_loss_history'],'g',label='Validation')\n",
    "plt.ylabel('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(saved_dict['train_acc_history'],'r',label='Training')\n",
    "plt.plot(saved_dict['val_acc_history'],'g',label='Validation')\n",
    "plt.ylabel('Accuracy history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**CNN + FCN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T10:24:31.970374Z",
     "start_time": "2020-10-21T10:24:26.664780Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary to save\n",
    "timeStr = str(datetime.datetime.now()) \n",
    "#note = 'Recon2X3D5 (instanceNorm/instanceNorm/instanceNorm) AVG(X1,X2,X)*2-fusion layer (last fusion layer [3,16,16][16,3] + ToTensor7 using FocalMulticlass({0.1 0.3 0.6}, 2, sum)'\n",
    "note = 'Recon2X3D6 (instanceNorm/instanceNorm/instanceNorm) CAT[XD,XH,XW,X(C=3))*2-fusion layer (last fusion layer [3,16,16][16,3] + ToTensor7 using FocalMulticlass({0.15 0.25 0.6}, 2, sum)'\n",
    "save_dict = {'timestamp':timeStr ,\n",
    "             'note':note ,\n",
    "             'en_sz':en_sz ,\n",
    "             'de_sz':de_sz ,\n",
    "             'de3d_sz':de3d_sz ,\n",
    "             'final_sz':final_sz ,\n",
    "             'train_loss_history':train_loss_history ,\n",
    "             'train_acc_history':train_acc_history ,\n",
    "             'val_loss_history':val_loss_history ,\n",
    "             'val_acc_history':val_acc_history ,\n",
    "             #'model_state_dict':model.state_dict() ,              # without nn.DataParallel\n",
    "             'model_state_dict':model.module.state_dict() ,        # with    nn.DataParallel\n",
    "             'optimizer_state_dict': optimizer.state_dict()}\n",
    "\n",
    "# Save Model state_dict\n",
    "saved_name = 'trained\\Recon2X3D6_21101501.pt'\n",
    "confirm_saved_name = str(input('Confirm Save : ' + saved_name + ' [y/n] ? '))\n",
    "#confirm_saved_name = 'y'\n",
    "if confirm_saved_name=='y':\n",
    "    torch.save(save_dict, saved_name)\n",
    "    print('model.parameters:')\n",
    "    print(model.parameters)\n",
    "    print('\\n   ### Model is saved @ {}  ; Total Epoch =  {} ### \\n'.format(timeStr,len(train_loss_history)))\n",
    "else:\n",
    "    print('\\n   ### Model is not saved !!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**AutoEncoder Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "note = 'Autoencoder: Encode3d using AdaptiveMaxPool3d'\n",
    "timeStr = str(datetime.datetime.now())\n",
    "save_dict = {'timestamp':timeStr , \n",
    "             'note':note , \n",
    "             'en3d_sz':en3d_sz , \n",
    "             'de3d_sz':de3d_sz ,\n",
    "             'en_sz' : en_sz ,\n",
    "             'final_sz': final_sz ,\n",
    "             'train_loss_history':train_loss_history , \n",
    "             'train_acc_history':train_acc_history , \n",
    "             'val_loss_history':val_loss_history ,\n",
    "             'val_acc_history':val_acc_history , \n",
    "             'model1_state_dict':model1.state_dict() ,    # for encode3d or encode2d\n",
    "             'model2_state_dict':model2.state_dict() ,    # for encode3d\n",
    "             'optimizer_state_dict': optimizer.state_dict()}\n",
    "\n",
    "# Save Model state_dict\n",
    "saved_name = 'trained\\TLNetAuto_21012502.pt'\n",
    "confirm_saved_name = str(input('Confirm save : ' + saved_name + ' [y/n] ? '))\n",
    "#confirm_saved_name = 'y'\n",
    "if confirm_saved_name=='y':\n",
    "    torch.save(save_dict, saved_name)\n",
    "    print('model1.parameters: \\n{}'.format(model1.parameters))\n",
    "    print()\n",
    "    print('model2.parameters: \\n{}'.format(model2.parameters))\n",
    "    print()\n",
    "    print('\\n   ### Model is saved @ {}  ; Total Epoch =  {} ### \\n'.format(timeStr,len(train_loss_history)))\n",
    "else:\n",
    "    print('\\n   ### Model is not saved !!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T04:37:07.924921Z",
     "start_time": "2020-10-23T04:37:07.905926Z"
    }
   },
   "outputs": [],
   "source": [
    "# For anlign dataset2\n",
    "\n",
    "first = True\n",
    "\n",
    "test_mode = int(input('Select test mode : [1] All sample  [2] Specify sample : '))\n",
    "\n",
    "if test_mode == 1:\n",
    "    test_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\Validation_set.xlsx'\n",
    "    shuffle_logic = bool(input('Input Shuffle : [1] True  [0] False '))\n",
    "    \n",
    "elif test_mode == 2:\n",
    "    shuffle_logic = False\n",
    "    sampletype = int(input('Select sample type : [1] Intact  [2] Nondisplaced  [3] Displaced : '))\n",
    "    if sampletype == 1:\n",
    "        print('Intact        id = {0 5 8 14 16} ')\n",
    "        test_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\IntactLogTotal0.xlsx'\n",
    "    elif sampletype == 2:\n",
    "        print('Nondisplaced  id = {0 1 2 3 4 5*} ')\n",
    "        test_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\NondisplaceLogTotal0.xlsx'\n",
    "    elif sampletype == 3:\n",
    "        print('Displaced     id = {0 3 4* 5 8 9} ')\n",
    "        test_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\DisplaceLogTotal0.xlsx'\n",
    "    elif sampletype not in (1,3):\n",
    "        raise ValueError('Error sampletype input : invalid integer input !!! ')\n",
    "elif test_mode not in (1,2):\n",
    "    raise ValueError('Error test_model input')\n",
    "    \n",
    "'''test_transformedFemur = FemurDataset(csv_file=test_set, root_dir=root_dir, \n",
    "                                     transform=transforms.Compose([NormalizeSample(),ToTensor7()]))  # For ToTensor7'''\n",
    "\n",
    "\n",
    "test_transformedFemur = FemurDataset2(csv_file=test_set, root_dir=root_dir, \n",
    "                                    transform=transforms.Compose([NormalizeSample2(),ToTensor8()]))  # For ToTensor8-9\n",
    "                                    \n",
    "testLoader = DataLoader(test_transformedFemur, batch_size=1, shuffle=shuffle_logic, num_workers=4)\n",
    "\n",
    "print(' --- END ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## k3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_dir = r'D:\\FEW PhD\\Datasets\\Siriraj Dataset\\Cleaning Data\\Siriraj_UnalignData'\n",
    "#training_file = r'D:\\FEW PhD\\Datasets\\Siriraj Dataset\\Cleaning Data\\Siriraj_UnalignData\\Siriraj_testset.xlsx'\n",
    "#val_file = r'D:\\FEW PhD\\Datasets\\Siriraj Dataset\\Cleaning Data\\Siriraj_UnalignData\\Siriraj_testset.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unanlign dataset4\n",
    "first = True\n",
    "\n",
    "test_mode = int(input('Select test mode : [1] All sample  [2] Specify sample : '))\n",
    "if test_mode == 1:\n",
    "    test_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TrainingSet_Scale12.xlsx'      # TestingSet_Scale12.xlsx\n",
    "    shuffle_logic = bool(input('Input Shuffle : [1] True  [0] False '))\n",
    "    sampletype = int(input('Select sample type : [1] Intact  [2] Nondisplaced  [3] Displaced [4] Siriraj: '))\n",
    "    \n",
    "elif test_mode == 2:\n",
    "    shuffle_logic = False\n",
    "    sampletype = int(input('Select sample type : [1] Intact  [2] Nondisplaced  [3] Displaced [4] Siriraj: '))\n",
    "    if sampletype == 1:\n",
    "        print('Intact {0 - 1625}          id = unalign{} ')    # 0 - 1625\n",
    "        test_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TestingSet_IntactLog2.xlsx'\n",
    "    elif sampletype == 2:\n",
    "        print('Nondisplaced {1626 - 2321} id = unalign{} ')   # 1626 - 2321\n",
    "        test_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TestingSet_NondisplaceLog2.xlsx'\n",
    "    elif sampletype == 3:\n",
    "        print('Displaced {2322 - 3365}    id = unalign{} ')   # 2322 - 3365\n",
    "        test_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TestingSet_DisplaceLog2.xlsx'\n",
    "    elif sampletype == 4:\n",
    "        print('Displaced {2322 - 3365}    id = unalign{} ')   # 2322 - 3365\n",
    "        test_set = r'D:\\FEW PhD\\Datasets\\Siriraj Dataset\\Cleaning Data\\Siriraj_UnalignData\\Siriraj_testset.xlsx'\n",
    "    elif sampletype not in (1,3):\n",
    "        raise ValueError('Error sampletype input : invalid integer input !!! ')\n",
    "elif test_mode not in (1,2):\n",
    "    raise ValueError('Error test_model input')\n",
    "\n",
    "'''test_transformedFemur = FemurDataset(csv_file=test_set, root_dir=root_dir, \n",
    "                                        transform=transforms.Compose([NormalizeSample(),ToTensor7()]))  # For ToTensor7'''\n",
    "\n",
    "test_transformedFemur = FemurDataset2(csv_file=test_set, root_dir=root_dir, \n",
    "                                      transform=transforms.Compose([NormalizeSample2(),ToTensor8()]))  # For ToTensor8-9\n",
    "                                    \n",
    "testLoader = DataLoader(test_transformedFemur, batch_size=1, shuffle=shuffle_logic, num_workers=4)\n",
    "\n",
    "print(' --- END ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "torch.cuda.empty_cache()\n",
    "test_mode = 2\n",
    "\n",
    "if test_mode == 1:\n",
    "    print('Testing on Shuffle sample')\n",
    "    if first==True:\n",
    "        print('First testing sample')\n",
    "        femurIter = iter(testLoader)\n",
    "        first = False\n",
    "    else:\n",
    "        print('Next testing sample')\n",
    "    sample = next(femurIter)\n",
    "    target , ap , lat = sample['Target'] , sample['view1'] , sample['view2']\n",
    "    #target , ap , lat = sample['Target'] , sample['AP'] , sample['LAT']\n",
    "    print('Raw: target={}  ap={}  lat={}'.format(target.size(),ap.size(),lat.size()))\n",
    "    #target,ap,lat=target,ap[0].unsqueeze(0).to(device=device),lat[0].unsqueeze(0).to(device=device) # for ToTensor1-5\n",
    "    target, ap, lat = target[0],ap[0].unsqueeze(0).to(device=device),lat[0].unsqueeze(0).to(device=device)  # for ToTensor6-8\n",
    "    print('Final: target={}  ap={}  lat={}'.format(target.size(), ap.size(), lat.size()))\n",
    "    \n",
    "elif test_mode == 2:\n",
    "    print('Choose Sample Number : 0 - {} ?'.format(len(testLoader)-1))\n",
    "    sample_number = int(input('Sample number : '))    # using 123\n",
    "    #valLoader = DataLoader(val_transformedFemur, batch_size=1, shuffle=False, num_workers=0)\n",
    "    sample = test_transformedFemur[sample_number]\n",
    "    target , ap , lat = sample['Target'] , sample['view1'] , sample['view2']\n",
    "    #target , ap , lat = sample['Target'] , sample['AP'] , sample['LAT']\n",
    "    print('Raw Dataset = {} {} {}'.format(target.size(),ap.size(),lat.size()))\n",
    "    #target,ap,lat=target,ap[0].unsqueeze(0).to(device=device),lat[0].unsqueeze(0).to(device=device) # for ToTensor1-5\n",
    "    target,ap,lat=target,ap.unsqueeze(0).to(device=device),lat.unsqueeze(0).to(device=device)  # for ToTensor6\n",
    "    print('Final = {} {} {}'.format(target.size(), ap.size(), lat.size()))\n",
    "    \n",
    "with torch.cuda.amp.autocast(enabled=True):\n",
    "    t1 = time.time()\n",
    "    #output = model(ap).detach()\n",
    "    #model = nn.DataParallel(model, device_ids=[0,1])\n",
    "    #model = model.module\n",
    "    #model.to(device='cpu')\n",
    "    #model.to(device='cuda:0')\n",
    "    #model = netG.to(device=device)\n",
    "    model.eval()\n",
    "    output = model(ap,lat).detach()\n",
    "    #torch.cuda.synchronize()\n",
    "    print('\\nOutput = {}   {}'.format(output.size(), output.dtype))\n",
    "    #print('output.device: {} \\ntarget.device: {} '.format(output.device,target.device))\n",
    "    t2 = time.time()\n",
    "    print('Calculating time = {:,.4f} sec.'.format(t2-t1))\n",
    "    \n",
    "    \n",
    "##### OUTPUT: Bone Visualization #####\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,20))\n",
    "ax[0].imshow(ap.detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "ax[0].set_title('Input view 1')\n",
    "ax[1].imshow(lat.detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "ax[1].set_title('Input view 2')\n",
    "plt.show()\n",
    "\n",
    "gt = (target==1).detach().cpu().numpy()\n",
    "ot = (output[0,1,:]>0.5).detach().cpu().numpy()\n",
    "#print('IOU = {:.4f} '.format(iou((output[0,1]>=0.5).float(), (target==1).float().to(device=device)))) # ToTensor6-9\n",
    "print('IoU = {:,.3f}'.format(iou((target==1).float().cpu(), (output[0,1,:]>0.5).float().cpu())), end=', ')\n",
    "dist_metrics = surface_distance_measurement(output[0,1].cpu().numpy(), (target==1).cpu().numpy(), res=0.5, verbose=False)\n",
    "print('ASSD = {:.3f} mm'.format(dist_metrics['ASSD']))\n",
    "\n",
    "### Ground Truth : Bone Class ###\n",
    "# Camera view\n",
    "cam_view1 = [0,0,-1.5, 0,0,0 ,0,-1,0]   # view1\n",
    "cam_view2 = [1.5,0,0, 0,0,0 ,1,-1,0]    # view2\n",
    "\n",
    "plot1 = k3d.plot(name='Plot 1 : [view1]')\n",
    "obj = k3d.volume(gt, name='Ground Truth', \n",
    "                 color_map=k3d.colormaps.matplotlib_color_maps.Bone,\n",
    "                 gradient_step=0.005,\n",
    "                 shadow='dynamic',\n",
    "                 shadow_delay=10,\n",
    "                )\n",
    "plot1 += obj + k3d.text2d(text='Ground Truth (view1)', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "plot1.display()\n",
    "plot1.camera = cam_view1\n",
    "\n",
    "plot3 = k3d.plot(name='Plot 1 : [view2]')\n",
    "obj3 = k3d.volume(gt, name='Ground Truth', \n",
    "                  color_map=k3d.colormaps.matplotlib_color_maps.Bone,\n",
    "                  gradient_step=0.005,\n",
    "                  shadow='dynamic',\n",
    "                  shadow_delay=10,\n",
    "                 )\n",
    "plot3 += obj3 + k3d.text2d(text='Ground Truth (view2)', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "plot3.display()\n",
    "plot3.camera = cam_view2\n",
    "\n",
    "\n",
    "### Output : Bone Class ###\n",
    "plot2 = k3d.plot(name='Plot 2 : [view1]')\n",
    "obj = k3d.volume(ot, name='Output',\n",
    "                 color_map=k3d.colormaps.matplotlib_color_maps.Bone,\n",
    "                 gradient_step=0.005,\n",
    "                 shadow='dynamic',\n",
    "                 shadow_delay=10,\n",
    "                )\n",
    "plot2 += obj + k3d.text2d(text='Output (view1)', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "plot2.display()\n",
    "plot2.camera = cam_view1\n",
    "\n",
    "\n",
    "plot4 = k3d.plot(name='Plot 2 : [view2]')\n",
    "obj = k3d.volume(ot, name='Output',\n",
    "                 color_map=k3d.colormaps.matplotlib_color_maps.Bone,\n",
    "                 gradient_step=0.005,\n",
    "                 shadow='dynamic',\n",
    "                 shadow_delay=10,\n",
    "                )\n",
    "plot4 += obj + k3d.text2d(text='Output (view2)', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "plot4.display()\n",
    "plot4.camera = cam_view2\n",
    "print(' --- END --- ')\n",
    "### For align dataset2 ###\n",
    "# Intact        id = {0 5 8 14 16}\n",
    "# Nondisplaced  id = {6 1 2 3 4 5'}\n",
    "# Displaced     id = {0 3 4' 5 8 9}\n",
    "\n",
    "### For unalign dataset4 ###\n",
    "### For Training set ###\n",
    "# Intact {0 - 1625}          id = {14771(0),14772(+10),14773(+7.5),14774(+5),14775(+2.5)}           {(0),(+10),(+7.5),(+5),(+2.5)}\n",
    "# Nondisplaced {1626 - 2321} id = {16453(0),16454(+10),16455(+7.5),16456(+5),16457(+2.5)}\n",
    "#                               = {(0),(+10),(+7.5),(+5),(+2.5)}\n",
    "#                               = {(0),(+10),(+7.5),(+5),(+2.5)}\n",
    "# Displaced {2322 - 3365}    id = {17700(0),17701(+10),17702(+7.5),17703(+5),17704(+2.5)}\n",
    "#                            id = {18077(0),18078(+10),18079(+7.5),18080(+5),18081(+2.5)}\n",
    "### For Testing set ###\n",
    "# Intact                     id = {39(0),40(+10),41(+7.5),42(+5),43(+2.5)}           {(0),(+10),(+7.5),(+5),(+2.5)}\n",
    "#                                 {(0),(+10),(+7.5),(+5),(+2.5)}\n",
    "# Nondisplace                id = {39(0),40(+10),41(+7.5),42(+5),43(+2.5)}   ไม่เอา\n",
    "#                            id = {10(0),11(+10),12(+7.5),13(+5),14(+2.5)}\n",
    "#                            id = {68(0),69(+10),70(+7.5),71(+5),72(+2.5)}\n",
    "# Displace                   id = {10(0),11(+10),12(+7.5),13(+5),14(+2.5)}\n",
    "#                            id = {155(0),156(+10),157(+7.5),158(+5),159(+2.5)}#         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Update upper display: camera view of ground truth to corresponding camera view of output\n",
    "plot3.camera = plot4.camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Update lower display: camera view of output to corresponding camera view of ground truth\n",
    "plot4.camera = plot3.camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show Volume rotation360 ###\n",
    "N = 1   # number of round\n",
    "r_orbit = 1.75\n",
    "camera_rotate = list([-r_orbit*np.sin(t), -0.2,r_orbit*np.cos(t), 0,0,0, 0,-1,0] for t in np.linspace(0, 2*np.pi*N, num=360*N) )  # 360*N\n",
    "k3d.plot()\n",
    "plot3.grid_visible = False\n",
    "plot4.grid_visible = False\n",
    "\n",
    "for i, view in enumerate(camera_rotate):\n",
    "    #print('Order:{} | {}'.format(i,view))\n",
    "    plot3.camera = view\n",
    "    plot4.camera = view\n",
    "    time.sleep(8/360)\n",
    "\n",
    "print('--- Rotation END ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the result in k3d-widget.html files ###\n",
    "from PIL import Image\n",
    "# Normalize the intensity of DRRs\n",
    "drr1_file = ap.detach().cpu().numpy().squeeze()\n",
    "drr2_file = lat.detach().cpu().numpy().squeeze()\n",
    "drr1_file = (drr1_file - drr1_file.min())*65535/(drr1_file.max() - drr1_file.min())\n",
    "drr2_file = (drr2_file - drr2_file.min())*65535/(drr2_file.max() - drr2_file.min())\n",
    "drr1_file = drr1_file.astype(np.uint16())   # float > uint16 for .png\n",
    "drr2_file = drr2_file.astype(np.uint16())   # float > uint16 for .png\n",
    "drr1_file = cv2.resize(drr1_file, dsize=(1024, 1024), interpolation=cv2.INTER_CUBIC)\n",
    "drr2_file = cv2.resize(drr2_file, dsize=(1024, 1024), interpolation=cv2.INTER_CUBIC)\n",
    "drr1_file = Image.fromarray(drr1_file)\n",
    "drr2_file = Image.fromarray(drr2_file)\n",
    "\n",
    "# Name the files\n",
    "k3dResultLocation = r'trained\\K3DResult\\\\FracReconNet_Recon2X3D5_21101901_unalignTrain (feak)'   # root directory\n",
    "confirm_savePath = int(input('Confirm savePath = {}\\n    [1] Yes  [0] No = '.format(k3dResultLocation)))\n",
    "if confirm_savePath==1:\n",
    "    drr1_filename = k3dResultLocation + r'\\DRRs\\drr1-type-' + str(sampletype) + '-NO-' + str(sample_number) + '.png'\n",
    "    drr2_filename = k3dResultLocation + r'\\DRRs\\drr2-type-' + str(sampletype) + '-NO-' + str(sample_number) + '.png'\n",
    "    gt_filename = k3dResultLocation + r'\\Volume\\volumeGT-type-' + str(sampletype) + '-NO-' + str(sample_number) + '.html' # For only 0.0 degree alignment\n",
    "    ot_filename = k3dResultLocation + r'\\Volume\\volumeOT-type-' + str(sampletype) + '-NO-' + str(sample_number) + '.html'\n",
    "\n",
    "    # save ผิด folder FracReconNet_Recon2X3D5_22081001_unalignTest \n",
    "    # Save the files\n",
    "    drr1_file.save(drr1_filename)\n",
    "    drr2_file.save(drr2_filename)\n",
    "    with open(gt_filename,'w') as fp:     # For only 0.0 degree alignment\n",
    "        fp.write(plot3.get_snapshot()) \n",
    "    with open(ot_filename,'w') as fp:\n",
    "        fp.write(plot4.get_snapshot())\n",
    "    print(' Saving complete :\\n      {}\\n      {}\\n      {}\\n      {}'.format(drr1_filename,drr2_filename,gt_filename, ot_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Save RAW volumetric data in .numpy files \n",
    "gt_np = (target==1).detach().cpu().numpy()\n",
    "ot_np = (output[0,1,:]>0.5).detach().cpu().numpy()\n",
    "print('gt = {}\\not = {}'.format(gt_np.shape, ot_np.shape))\n",
    "\n",
    "# Name the file\n",
    "gt_array_filename = k3dResultLocation + r'\\Numpy Array\\gtArray-type-' + str(sampletype) + '-NO-' + str(sample_number) + '.npy'\n",
    "ot_array_filename = k3dResultLocation + r'\\Numpy Array\\otArray-type-' + str(sampletype) + '-NO-' + str(sample_number) + '.npy'\n",
    "\n",
    "# Saving the .numpy files\n",
    "np.save(gt_array_filename, gt_np)\n",
    "np.save(ot_array_filename, ot_np)\n",
    "print(' Saving complete :\\n      {}\\n      {}'.format(gt_array_filename, ot_array_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Snapshot view.png files\n",
    "folder_dir = k3dResultLocation + r\"\\GIF_Volume\"\n",
    "\n",
    "N = 1\n",
    "r_orbit = 1.75\n",
    "camera_rotate = list([r_orbit*np.sin(t), -0.2,r_orbit*np.cos(t), 0,0,0, 0,-1,0] for t in np.linspace(0, 2*np.pi*N, num=45*N) )  # 360*N\n",
    "\n",
    "out = ipywidgets.Output()\n",
    "gt_or_ot = int(input('Select: [1] Ground-truth [2] Output: '))\n",
    "if gt_or_ot == 1:\n",
    "    plot3.grid_visible = False\n",
    "    @plot3.yield_screenshots   # running in the background  change={plot3, plot4}\n",
    "    def coroutine():\n",
    "        print('Running coroutine for ground-truth ...')\n",
    "        global camera_rotate   # use global parameters\n",
    "        for i, view in enumerate(camera_rotate):\n",
    "            print(i)\n",
    "            # For ground-truth ####\n",
    "            plot3.camera = view     \n",
    "            plot3.fetch_screenshot()\n",
    "            screenshot = yield \n",
    "            with open(folder_dir + r'\\GT\\viewGT_%03d.png'%i, 'wb') as f:\n",
    "                f.write(screenshot)\n",
    "            with out:\n",
    "                print('viewGT_%03d.png saved.'%i)\n",
    "        with out:\n",
    "            print('Done!!! \\n')\n",
    "    \n",
    "elif gt_or_ot == 2:\n",
    "    plot4.grid_visible = False\n",
    "    @plot4.yield_screenshots   # running in the background  change={plot3, plot4}\n",
    "    def coroutine():\n",
    "        print('Running coroutine for output ...')\n",
    "        global camera_rotate   # use global parameters\n",
    "        for i, view in enumerate(camera_rotate):\n",
    "            print(i)\n",
    "            ### For output ###\n",
    "            plot4.camera = view\n",
    "            plot4.fetch_screenshot()\n",
    "            screenshot = yield\n",
    "            with open(folder_dir + r'\\OT\\viewOT_%03d.png'%i, 'wb') as f:\n",
    "                f.write(screenshot)\n",
    "            with out:\n",
    "                print('viewOT_%03d.png '%i, end=' ')\n",
    "        with out:\n",
    "            print('Done!!! \\n')\n",
    "        \n",
    "coroutine()\n",
    "out\n",
    "print('Saving folder: {}'.format(folder_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate the Volume_GIF file\n",
    "# get the path/directory\n",
    "multi_GTimgs = list()\n",
    "multi_OTimgs = list()\n",
    "t1 = time.time()\n",
    "if gt_or_ot == 1:\n",
    "    print('Generating .GIF for Ground-truth')\n",
    "    for image_name in os.listdir(folder_dir + r'\\GT'):    \n",
    "        # check if the image ends with png\n",
    "        if (image_name.endswith(\".png\")):   # Ground-truth\n",
    "            print('Name: ',image_name, end=' ')\n",
    "            img = Image.open(os.path.join(folder_dir,'GT',image_name))\n",
    "            multi_GTimgs.append(img)\n",
    "\n",
    "    gif_GTname = folder_dir + r'\\GT\\GIFVolumeGT-type-' + str(sampletype) + '-NO-' + str(sample_number) +'.gif'\n",
    "    multi_GTimgs[0].save(gif_GTname, format='GIF', append_images=multi_GTimgs[1:], save_all=True, duration=250, loop=0 )\n",
    "    print('\\n ### Save .gif name: {}'.format(gif_GTname))\n",
    "elif gt_or_ot == 2:\n",
    "    print('Generating .GIF for Output')\n",
    "    for image_name in os.listdir(folder_dir + r'\\OT'):   \n",
    "        if (image_name.endswith(\".png\")):   # Output\n",
    "            print(image_name, end=' ')\n",
    "            img = Image.open(os.path.join(folder_dir,'OT',image_name))\n",
    "            multi_OTimgs.append(img)\n",
    "    gif_OTname = folder_dir + r'\\OT\\GIFVolumeOT-type-' + str(sampletype) + '-NO-' + str(sample_number) +'.gif'\n",
    "    multi_OTimgs[0].save(gif_OTname, format='GIF', append_images=multi_OTimgs[1:], save_all=True, duration=250, loop=0 )\n",
    "    print('\\n ### Save .gif name: {}'.format(gif_OTname))\n",
    "t2 = time.time()\n",
    "print('Done in {} sec.'.format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ASSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "surf_dist_result = surface_distance_measurement(gt, ot, res=0.5, return_vert_dist=True, verbose=False)\n",
    "t2 = time.time()\n",
    "print('\\nTotal time of calculation = {} sec.'.format(t2-t1))\n",
    "print('key result = {}'.format(surf_dist_result.keys()))\n",
    "\n",
    "background_color = 65536*255 + 256*255 + 255\n",
    "pltmesh = k3d.plot(background_color=background_color)\n",
    "title_text = k3d.text2d(text='Min. Surface Distance (GroundTruth-based)', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "meshsurf = k3d.mesh(surf_dist_result['surface_dist_gt']['target_vert'], surf_dist_result['surface_dist_gt']['target_face'],\n",
    "                    name='surface distance (mm)',\n",
    "                    color_map=k3d.colormaps.basic_color_maps.Jet,\n",
    "                    #color_map=k3d.colormaps.paraview_color_maps.Bone_Matlab\n",
    "                    color_range=[0,3],   # [0,3] for align dataset   |   [0,6] for unalign dataset\n",
    "                    attribute=surf_dist_result['surface_dist_gt']['distances'].astype(np.float32())\n",
    "                   )   #     -target_vert[:,2]\n",
    "pltmesh += title_text + meshsurf\n",
    "pltmesh.display()\n",
    "pltmesh.camera = [128,-100,512, 128,-128,128,  0,1,0]\n",
    "\n",
    "\n",
    "pltmesh3 = k3d.plot(background_color=background_color)\n",
    "title_text3 = k3d.text2d(text='Min. Surface Distance (GroundTruth-based2)', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "meshsurf3 = k3d.mesh(surf_dist_result['surface_dist_gt']['target_vert'], surf_dist_result['surface_dist_gt']['target_face'],\n",
    "                    name='surface distance (mm)',\n",
    "                    color_map=k3d.colormaps.basic_color_maps.Jet,\n",
    "                    #color_map=k3d.colormaps.paraview_color_maps.Bone_Matlab\n",
    "                    color_range=[0,3],   # [0,3] for align dataset   |   [0,6] for unalign dataset\n",
    "                    attribute=surf_dist_result['surface_dist_gt']['distances'].astype(np.float32())\n",
    "                   )   #     -target_vert[:,2]\n",
    "pltmesh3 += title_text3 + meshsurf3\n",
    "pltmesh3.display()\n",
    "pltmesh3.camera = [-224,-100,128, 128,-128,128,  0,1,0]\n",
    "\n",
    "\n",
    "pltmesh2 = k3d.plot(background_color=background_color)\n",
    "title_text2 = k3d.text2d(text='Min. Surface Distance (Output-based)', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "meshsurf2 = k3d.mesh(surf_dist_result['surface_dist_ot']['target_vert'], surf_dist_result['surface_dist_ot']['target_face'],\n",
    "                    name='surface distance (mm)',\n",
    "                    color_map=k3d.colormaps.basic_color_maps.Jet, \n",
    "                    color_range=[0,3],  # [0,3] for align dataset   |   [0,6] for unalign dataset\n",
    "                    attribute=surf_dist_result['surface_dist_ot']['distances'].astype(np.float32())\n",
    "                   )   #     -target_vert[:,2]\n",
    "pltmesh2 += title_text2 + meshsurf2\n",
    "pltmesh2.display()\n",
    "pltmesh2.camera = [128,-100,512, 128,-128,128,  0,1,0]\n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltmesh.fetch_screenshot()\n",
    "base64_decoded = base64.b64decode(pltmesh.screenshot)\n",
    "img_snap = Image.open(io.BytesIO(base64_decoded))\n",
    "img_np = np.array(img_snap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(img_np[:,:,0:3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshsurf.attribute = {str(t): 0.5*surface_dist_gt['distances']*t for t in np.linspace(0, 1.5, num=100)}\n",
    "meshsurf2.attribute = {str(t): 0.5*surface_dist_ot['distances']*t for t in np.linspace(0, 1.5, num=100)}\n",
    "pltmesh.start_auto_play()\n",
    "pltmesh2.start_auto_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltmesh.stop_auto_play()\n",
    "pltmesh2.stop_auto_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show Mesh rotation360 ###\n",
    "N = 1   # number of round\n",
    "r_orbit = 384   #384\n",
    "camera_rotate = list([128+r_orbit*np.sin(t),-90,128-r_orbit*np.cos(t), 128,-128,128, 0,1,0] \n",
    "                     for t in np.linspace(0+np.pi/2, 2*np.pi*N+np.pi/2, num=360*N) )  # 360*N\n",
    "#k3d.plot()\n",
    "pltmesh.grid_visible = False\n",
    "pltmesh2.grid_visible = False\n",
    "#pltmesh2.camera_mode = 'orbit'\n",
    "#pltmesh2.camera_auto_fit = False\n",
    "\n",
    "for i, view in enumerate(camera_rotate):\n",
    "    #print('Order:{} | {}'.format(i,view))\n",
    "    pltmesh.camera = view\n",
    "    pltmesh2.camera = view\n",
    "    time.sleep(8/360)\n",
    "\n",
    "print('--- Rotation END ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the result in k3d-widget.html files ###\n",
    "gt_filename = k3dResultLocation + r'\\Mesh\\meshGT-type-' + str(sampletype) + '-NO-' + str(sample_number) + '.html'\n",
    "ot_filename = k3dResultLocation + r'\\Mesh\\meshOT-type-' + str(sampletype) + '-NO-' + str(sample_number) + '.html'\n",
    "with open(gt_filename,'w') as fp:\n",
    "    fp.write(pltmesh.get_snapshot())\n",
    "with open(ot_filename,'w') as fp:\n",
    "    fp.write(pltmesh2.get_snapshot())\n",
    "print(' Saving complete :\\n      {}\\n      {}'.format(gt_filename, ot_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Snapshot view.png files\n",
    "folder_dir2 = k3dResultLocation + r\"\\GIF_Mesh\"\n",
    "print('Saving folder: {}'.format(folder_dir2))\n",
    "N = 1\n",
    "r_orbit = 384\n",
    "camera_rotate = list([128+r_orbit*np.sin(t),-90,128-r_orbit*np.cos(t), 128,-128,128, 0,1,0] \n",
    "                     for t in np.linspace(0+np.pi/2, 2*np.pi*N+np.pi/2, num=45*N) )  # 360*N\n",
    "\n",
    "pltmesh.grid_visible = False\n",
    "pltmesh2.grid_visible = False\n",
    "\n",
    "out = ipywidgets.Output()\n",
    "@pltmesh.yield_screenshots\n",
    "def coroutine():\n",
    "    print('coroutine is running ...')\n",
    "    global camera_rotate   # use global parameters\n",
    "    for i, view in enumerate(camera_rotate):\n",
    "        print(i)\n",
    "        pltmesh.camera = view\n",
    "        pltmesh.fetch_screenshot()\n",
    "        screenshot = yield\n",
    "        with open(folder_dir2 + r'\\view_%03d.png'%i, 'wb') as f:\n",
    "            f.write(screenshot)\n",
    "        with out:\n",
    "            print('view_%03d.png'%i, end=' ')\n",
    "    with out:\n",
    "        print('Done!!! \\n')\n",
    "\n",
    "coroutine()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate the GIF file\n",
    "# get the path/directory\n",
    "multi_imgs = list()\n",
    "print('Converting images.png to .gif')\n",
    "for image_name in os.listdir(folder_dir2):\n",
    "    # check if the image ends with png\n",
    "    if (image_name.endswith(\".png\")):\n",
    "        print(image_name, end=' ')\n",
    "        img = Image.open(os.path.join(folder_dir2,image_name))\n",
    "        multi_imgs.append(img) # imageio.imread(image_name)\n",
    "    #print('multi_imgs len = {}'.format(len(multi_imgs)))\n",
    "\n",
    "t1 = time.time()\n",
    "gif_name = folder_dir2 + r'\\GIFMeshGT-type-' + str(sampletype) + '-NO-' + str(sample_number) +'.gif'\n",
    "multi_imgs[0].save(gif_name, format='GIF', append_images=multi_imgs[1:], save_all=True, duration=250, loop=0 )   # duration = time to the next picture\n",
    "t2 = time.time()\n",
    "print('\\n\\n### Done in {} sec. : {}'.format(t2-t1, gif_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obsolete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not work !!!\n",
    "import imageio \n",
    "import base64\n",
    "import io\n",
    "\n",
    "gif_name = k3dResultLocation + r'\\GIF\\meshGT-type-' + str(sampletype) + '-NO-' + str(sample_number) + '.gif'\n",
    "print('GIF file name = {}\\n'.format(gif_name))\n",
    "pltmesh2.camera_auto_fit = False\n",
    "N = 1   # number of round\n",
    "r_orbit = 384\n",
    "camera_rotate = list([128-r_orbit*np.sin(t),-90,128-r_orbit*np.cos(t), 128,-128,128, 0,1,0] for t in np.linspace(0, 2*np.pi*N, num=5*N) )  # 360*N\n",
    "#k3d.plot()\n",
    "#pltmesh2.camera_mode = 'orbit'\n",
    "pltmesh.grid_visible = False\n",
    "pltmesh2.grid_visible = False\n",
    "\n",
    "\n",
    "@pltmesh.yield_screenshots\n",
    "def save_gif(camera_rotate):\n",
    "    gif = []   # for save .gif files\n",
    "    for i, view in enumerate(camera_rotate):\n",
    "        print('Order:{} | {}'.format(i,view))\n",
    "        pltmesh.camera = view\n",
    "        pltmesh2.camera = view\n",
    "        time.sleep(8/360)   \n",
    "\n",
    "        # Generating .gif file\n",
    "        pltmesh.fetch_screenshot()\n",
    "        #time.sleep(10)  # Waiting for the widgets to synchronize behind the scenes, before calling the next cell.\n",
    "        try:\n",
    "            print('method1')\n",
    "            img_snap = pltmesh.screenshot.decode('base64')\n",
    "        except:\n",
    "            print('method2')\n",
    "            #base64_decoded = base64.b64decode(pltmesh.screenshot)\n",
    "            base64_decoded = base64.b64decode(pltmesh.fetch_screenshot().screenshot)\n",
    "            img_snap = Image.open(io.BytesIO(base64_decoded))\n",
    "\n",
    "        img_np = np.array(img_snap)\n",
    "        print(img_np.shape, type(img_np), img_np.dtype)\n",
    "        gif.append(img_np[:,:,0:3])\n",
    "        \n",
    "    imageio.mimwrite(gif_name, gif, duration=1/30)\n",
    "    print('### Save .GIF is done!!! ###')\n",
    "    return gif\n",
    "\n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ipyvolume ploting\n",
    "\n",
    "model = netG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "#torch.cuda.empty_cache()\n",
    "encode_feat1 = list()\n",
    "encode_feat2 = list()\n",
    "decode_feat1 = list()\n",
    "decode_feat2 = list()\n",
    "fusion_feat = list()\n",
    "fusionUp_feat = list()\n",
    "test_mode = 2\n",
    "\n",
    "if test_mode==1:\n",
    "    print('Testing on Shuffle sample')\n",
    "    if first==True:\n",
    "        print('First testing sample')\n",
    "        femurIter = iter(testLoader)\n",
    "        first = False\n",
    "    else:\n",
    "        print('Next testing sample')\n",
    "    sample = next(femurIter)\n",
    "    target , ap , lat = sample['Target'] , sample['view1'] , sample['view2']\n",
    "    #target , ap , lat = sample['Target'] , sample['AP'] , sample['LAT']\n",
    "    print('Raw: target={}  ap={}  lat={}'.format(target.size(),ap.size(),lat.size()))\n",
    "    #target,ap,lat=target,ap[0].unsqueeze(0).to(device=device),lat[0].unsqueeze(0).to(device=device) # for ToTensor1-5\n",
    "    target,ap,lat=target[0],ap[0].unsqueeze(0).to(device=device),lat[0].unsqueeze(0).to(device=device)  # for ToTensor6-8\n",
    "    print('Final: target={}  ap={}  lat={}'.format(target.size(), ap.size(), lat.size()))\n",
    "    \n",
    "elif test_mode==2:\n",
    "    print('Choose Sample Number : 0 - {} ?'.format(len(testLoader)-1))\n",
    "    sample_number = int(input('Sample number : '))    # using 123\n",
    "    #valLoader = DataLoader(val_transformedFemur, batch_size=1, shuffle=False, num_workers=0)\n",
    "    sample = test_transformedFemur[sample_number]\n",
    "    target , ap , lat = sample['Target'] , sample['view1'] , sample['view2']\n",
    "    #target , ap , lat = sample['Target'] , sample['AP'] , sample['LAT']\n",
    "    print('Raw Dataset = {} {} {}'.format(target.size(),ap.size(),lat.size()))\n",
    "    #target,ap,lat=target,ap[0].unsqueeze(0).to(device=device),lat[0].unsqueeze(0).to(device=device) # for ToTensor1-5\n",
    "    target,ap,lat=target,ap.unsqueeze(0).to(device=device),lat.unsqueeze(0).to(device=device)  # for ToTensor6\n",
    "    print('Final = {} {} {}'.format(target.size(), ap.size(), lat.size()))\n",
    "    \n",
    "with torch.cuda.amp.autocast(enabled=True):\n",
    "    t1 = time.time()\n",
    "    #output = model(ap).detach()\n",
    "    #model = nn.DataParallel(model, device_ids=[0,1])\n",
    "    #model = model.module\n",
    "    #model.to(device='cpu')\n",
    "    #model.to(device='cuda:0')\n",
    "    model.eval()\n",
    "    output = model(ap,lat).detach()\n",
    "    #torch.cuda.synchronize()\n",
    "    print('\\nOutput = {}   {}'.format(output.size(), output.dtype))\n",
    "    #print('output.device: {} \\ntarget.device: {} '.format(output.device,target.device))\n",
    "    t2 = time.time()\n",
    "    print('Calculating time = {:,.4f} sec.'.format(t2-t1))\n",
    "    '''fig, ax = plt.subplots(1,2, figsize=(16,16))\n",
    "    ax[0].imshow(ap.detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "    ax[0].set_title('Input view 1')\n",
    "    ax[1].imshow(lat.detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "    ax[1].set_title('Input view 2')\n",
    "    plt.show()'''\n",
    "    #print(' --- END ---')\n",
    "    \n",
    "    \n",
    "### OUTPUT: Bone Visualization ###\n",
    "print('\\n ### OUTPUT: Bone Visualization ###')\n",
    "print('### Bone ###')\n",
    "print('IOU = {:.4f} '.format(iou((output[0,1]>=0.5).float(),(target==1).float().to(device=device)))) # ToTensor6-9\n",
    "dist_metrics = surface_distance_measurement(output[0,1].cpu().numpy(), (target==1).cpu().numpy(), res=0.5, verbose=False)\n",
    "print('ASSD = {:.3f} mm'.format(dist_metrics['ASSD']))\n",
    "print('BHD = {:.3f} mm'.format(dist_metrics['BHD']))\n",
    "#print('IOU = {:.4f} '.format(iou((output[0].argmax(dim=0)).float(),(target==1).float().to(device=device)))) # ToTensor6-9\n",
    "#print('HD = {:.3f} \\n'.format(hausdorff_voxel((output[0,1]>=0.5).float(),(target==1).float().to(device=device))))\n",
    "\n",
    "\n",
    "#fig = ipv.figure()\n",
    "'''control = pythreejs.OrbitControls(controlling=fig.camera)\n",
    "fig.controls = control                # assigning to fig.controls will overwrite the builtin controls\n",
    "control.autoRotate = True\n",
    "control.autoRotateSpeed = 10.0\n",
    "fig.render_continuous = True          # the controls does not update itself, ut if we toggle this setting, ipyvolume will update the controls\n",
    "'''\n",
    "\n",
    "### Ground Truth : Bone Class ###\n",
    "'''ipv.figure()   # for ToTensor7 view1\n",
    "ipv.volshow((target==1).transpose(2,1).flip(0).cpu().detach().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1], max_opacity=3)\n",
    "ipv.style.use(['minimal','light'])\n",
    "ipv.show()\n",
    "time.sleep(2)\n",
    "\n",
    "ipv.figure()   # for ToTensor7 view2\n",
    "ipv.volshow((target==1).float().detach().cpu().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1],)\n",
    "ipv.view(270,90)\n",
    "ipv.style.use(['minimal','light'])   \n",
    "ipv.show()\n",
    "time.sleep(2)'''\n",
    "\n",
    "\n",
    "'''ipv.figure()   # for ToTensor8-9 view1\n",
    "ipv.volshow((target==1).float().flip(0).flip(1).float().detach().cpu().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1],)\n",
    "#ipv.view(270,90)\n",
    "ipv.style.use(['minimal','light'])   \n",
    "ipv.show()\n",
    "time.sleep(2)\n",
    "ipv.figure()\n",
    "\n",
    "ipv.figure()   # for ToTensor8-9 view2\n",
    "ipv.volshow((target==1).transpose(2,0).flip(1).float().detach().cpu().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1],)\n",
    "#ipv.view(270,90)\n",
    "ipv.style.use(['minimal','light'])   \n",
    "ipv.show()\n",
    "time.sleep(2)'''\n",
    "\n",
    "### Output : Bone Class ###\n",
    "'''ipv.figure()   # for ToTensor7\n",
    "ipv.volshow((output[0,1]>=0.5).cpu().detach().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1], max_opacity=3)\n",
    "#ipv.volshow(((output[0].argmax(dim=0)==1).float().transpose(2,0).flip(1)).cpu().detach().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1], max_opacity=3)\n",
    "#ipv.view(270,90)\n",
    "ipv.style.use(['minimal','light'])\n",
    "ipv.show()\n",
    "time.sleep(2)'''\n",
    "\n",
    "\n",
    "ipv.figure()     # for ToTensor8-9   view1\n",
    "ipv.volshow((output[0,1].flip(0).flip(1)>=0.5).cpu().detach().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1], max_opacity=3)\n",
    "#ipv.volshow(((output[0].argmax(dim=0)==1).float().flip(0).flip(1)).cpu().detach().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1], max_opacity=3)\n",
    "#ipv.volshow((output[0,1].transpose(2,1).flip(0)>=0.5).cpu().detach().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1], max_opacity=3)\n",
    "ipv.style.use(['minimal','light']) \n",
    "ipv.show()\n",
    "time.sleep(2)\n",
    "ipv.figure()   # for ToTensor8-9 view2\n",
    "ipv.volshow((output[0,1].transpose(2,0).flip(1).float()>=0.5).detach().cpu().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1],)\n",
    "#ipv.view(270,90)\n",
    "ipv.style.use(['minimal','light'])   \n",
    "ipv.show()\n",
    "print(' --- END --- ')\n",
    "\n",
    "# Intact        id = {0 5 8 14 16}\n",
    "# Nondisplaced  id = {0 1 2 3 4 5'}\n",
    "# Displaced     id = {0 3 4' 5 8 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = target.detach().cpu().numpy()\n",
    "ot = output.detach().cpu().numpy()\n",
    "np.save('dataset\\gt_displaced8.npy', gt)\n",
    "np.save('dataset\\ot_displaced8.npy', ot)\n",
    "print(' --- END --- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''fig2 = ipv.figure()\n",
    "control2 = pythreejs.OrbitControls(controlling=fig2.camera)\n",
    "fig.controls = control2\n",
    "control2.autoRotate = True\n",
    "control2.autoRotateSpeed = 10.0\n",
    "fig2.render_continuous = True''' \n",
    "\n",
    "\n",
    "ipv.figure()   # for ToTensor7 view1\n",
    "ipv.volshow((output[0,1].transpose(2,1).flip(0)>=0.5).cpu().detach().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1], max_opacity=3)\n",
    "ipv.style.use(['minimal','light'])\n",
    "ipv.show()\n",
    "time.sleep(2)\n",
    "\n",
    "ipv.figure()   # for ToTensor7 view2\n",
    "ipv.volshow((output[0,1]>=0.5).cpu().detach().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1], max_opacity=3)\n",
    "ipv.view(270,90)\n",
    "ipv.style.use(['minimal','light'])\n",
    "ipv.show()\n",
    "\n",
    "\n",
    "'''ipv.figure()    # for ToTensor8-9 view1\n",
    "ipv.volshow((output[0,1].flip(0).flip(1)>=0.5).cpu().detach().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1], max_opacity=3)\n",
    "ipv.style.use(['minimal','light']) \n",
    "ipv.show()\n",
    "\n",
    "ipv.figure()    # for ToTensor8-9 view2\n",
    "ipv.volshow((output[0,1].transpose(2,0).flip(1)>=0.5).cpu().detach().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1], max_opacity=3)\n",
    "ipv.style.use(['minimal','light']) \n",
    "ipv.show()\n",
    "time.sleep(2)'''\n",
    "print(' --- END --- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('***  IOU Fracture = {:.4f}  *** \\n'.format(iou((output[0,2]>=0.5).float(),(target[0,2]).float().to(device=device))))  # ToTensor1-5\n",
    "print('### Fracture ###')\n",
    "print('IOU = {:.4f} '.format(iou((output[0,2]>=0.5).float(),(target==2).float().to(device=device))))  # ToTensor6\n",
    "t1 = time.time()\n",
    "'''hd = HausdorffDistance()\n",
    "print('Exact HD Fracture = {:.2f}\\n'.format(hd.compute((output[:,2:3]>=0.5).float(),\n",
    "                                                                      (target==2).unsqueeze(0).unsqueeze(0).float().to(device=device) )))'''\n",
    "dist_metrics = surface_distance_measurement(output[0,2].cpu().numpy(), (target==2).cpu().numpy(),verbose=False)\n",
    "print('ASSD = {:.3f} mm'.format(dist_metrics['ASSD']))\n",
    "print('BHD = {:.3f} mm'.format(dist_metrics['BHD']))\n",
    "t2 = time.time()\n",
    "print('      Times calculating surface distance metrices = {:.3f} sec.'.format(t2-t1))\n",
    "ipv.figure()\n",
    "#ipv.volshow((target[0,2]).float().detach().cpu().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1],)\n",
    "ipv.volshow((target==2).transpose(2,0).flip(1).float().detach().cpu().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1],)\n",
    "#ipv.view(270,90)\n",
    "ipv.style.use(['minimal','light']) \n",
    "ipv.show()\n",
    "time.sleep(2)\n",
    "\n",
    "ipv.figure()\n",
    "#ipv.volshow(output[0,2].detach().cpu().numpy().squeeze())\n",
    "ipv.volshow((output[0,2].transpose(2,0).flip(1)>=0.5).detach().cpu().numpy().squeeze(), level=[0.1, 0.5, 0.67], opacity=[0.01, 0.1, 0.1],)\n",
    "#ipv.view(270,90)\n",
    "ipv.style.use(['minimal','light']) \n",
    "ipv.show()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify Hausdorff-distance\n",
    "t1 = time.time()\n",
    "output_vert,_,_,_ = measure.marching_cubes((output[0,2]>=0.5).detach().cpu().numpy().squeeze())\n",
    "target_vert,_,_,_ = measure.marching_cubes((target==2).detach().cpu().numpy().squeeze())\n",
    "t2 = time.time()\n",
    "print('output_vert = {}   |   target_vert = {}'.format(output_vert.shape, target_vert.shape))\n",
    "print('   Time marching : {:,.5f} sec.'.format(t2-t1))\n",
    "h1 = directed_hausdorff(output_vert,target_vert)\n",
    "h2 = directed_hausdorff(target_vert,output_vert)\n",
    "line1 = np.vstack((output_vert[h1[1],:] , target_vert[h1[2],:]))\n",
    "line2 = np.vstack((target_vert[h2[1],:] , output_vert[h2[2],:]))\n",
    "print('h1 = {:,.4f}   |   h2 = {:,.4f}'.format(h1[0], h2[0]))\n",
    "t3 = time.time()\n",
    "print('   Time hausdorff distance : {:,.5f} sec'.format(t3-t2))\n",
    "\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "ax = fig.add_subplot(221, projection='3d')\n",
    "ax.scatter(output_vert[0::100,0],output_vert[0::100,1],output_vert[0::100,2],c='r',marker='x')\n",
    "ax.plot(output_vert[h1[1],0],output_vert[h1[1],1],output_vert[h1[1],2],c='r',marker='x')\n",
    "ax.plot(output_vert[h2[2],0],output_vert[h2[2],1],output_vert[h2[2],2],c='r',marker='x')\n",
    "ax.set_title('Output fracture')\n",
    "ax = fig.add_subplot(222, projection='3d')\n",
    "ax.scatter(target_vert[0::100,0],target_vert[0::100,1],target_vert[0::100,2],c='g',marker='.')\n",
    "ax.plot(target_vert[h1[2],0],target_vert[h1[2],1],target_vert[h1[2],2],c='g',marker='.')\n",
    "ax.plot(target_vert[h2[1],0],target_vert[h2[1],1],target_vert[h2[1],2],c='g',marker='.')\n",
    "ax.set_title('Ground truth fracture')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "for angle in range(0, 1):\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(output_vert[0::150,0],output_vert[0::150,1],output_vert[0::150,2],c='r', marker='x',label='output')\n",
    "    ax.scatter(target_vert[0::150,0],target_vert[0::150,1],target_vert[0::150,2],c='g', marker='.',label='target')\n",
    "    ax.plot(line1[:,0],line1[:,1],line1[:,2],'black',label='h1')\n",
    "    ax.plot(line2[:,0],line2[:,1],line2[:,2],'black',label='h2')\n",
    "    ax.plot(output_vert[h1[1],0],output_vert[h1[1],1],output_vert[h1[1],2],c='r',marker='x')\n",
    "    ax.plot(target_vert[h1[2],0],target_vert[h1[2],1],target_vert[h1[2],2],c='g',marker='.')\n",
    "    ax.plot(target_vert[h2[1],0],target_vert[h2[1],1],target_vert[h2[1],2],c='g',marker='.')\n",
    "    ax.plot(output_vert[h2[2],0],output_vert[h2[2],1],output_vert[h2[2],2],c='r',marker='x')\n",
    "    ax.legend()\n",
    "    ax.set_title('Comparing Output vs Ground truth fracture')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    #ax.view_init(20, 30)\n",
    "    #plt.draw()\n",
    "    #plt.pause(.001)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0=background  1=boneMask  2=fracMask\n",
    "vidx = 1\n",
    "\n",
    "shp = output.size()\n",
    "# ToTensor1-5\n",
    "'''tp = TP((output[0,vidx]>=0.5).float(), (target[0,vidx]).float().to(device=device))\n",
    "tn = TN((output[0,vidx]>=0.5).float(), (target[0,vidx]).float().to(device=device))\n",
    "fp = FP((output[0,vidx]>=0.5).float(), (target[0,vidx]).float().to(device=device))\n",
    "fn = FN((output[0,vidx]>=0.5).float(), (target[0,vidx]).float().to(device=device))'''\n",
    "# ToTensor6-9\n",
    "tp = TP((output[0,vidx]>=0.5).float(), (target==vidx).float().to(device=device))\n",
    "tn = TN((output[0,vidx]>=0.5).float(), (target==vidx).float().to(device=device))\n",
    "fp = FP((output[0,vidx]>=0.5).float(), (target==vidx).float().to(device=device))\n",
    "fn = FN((output[0,vidx]>=0.5).float(), (target==vidx).float().to(device=device))\n",
    "tp_sum = torch.sum(tp.view(-1))\n",
    "tn_sum = torch.sum(tn.view(-1))\n",
    "fp_sum = torch.sum(fp.view(-1))\n",
    "fn_sum = torch.sum(fn.view(-1))\n",
    "\n",
    "# Confusion matrix\n",
    "precision = tp_sum/(tp_sum+fp_sum)\n",
    "recall = tp_sum/(tp_sum+fn_sum)    # = true positive rate(TPR) = Sensivity\n",
    "accuracy = (tp_sum+tn_sum)/(shp[-3]*shp[-2]*shp[-1])\n",
    "Specificity = tn_sum/(tn_sum+fp_sum)\n",
    "#fpr = 1.0 - Specificity.\n",
    "fpr = fp_sum/(tn_sum+fp_sum)\n",
    "F1 = 2*(precision*recall)/(precision+recall)\n",
    "print('True positive = {:,.1f}'.format(tp_sum))\n",
    "print('False positive = {:,.1f}'.format(fp_sum))\n",
    "print('False negative = {:,.1f}'.format(fn_sum))\n",
    "print('True negative = {:,.1f}\\n'.format(tn_sum))\n",
    "print('Precision = {:,.4f}'.format(precision))\n",
    "print('Recall = {:,.4f}'.format(recall))\n",
    "print('F1-score = {:,.4f}\\n'.format(F1))\n",
    "#print('Accuracy = {:,.4f}'.format(accuracy))\n",
    "print('True positive rate = {:,.4f}'.format(recall))\n",
    "print('False positive rate = {:,.4f}\\n'.format(fpr))\n",
    "\n",
    "\n",
    "print('Number of <False positive> voxel = {:,.1f}/16.7M'.format(fp.view(-1).sum()))\n",
    "fp = fp.transpose(2,0).flip(1).detach().cpu().numpy().squeeze()\n",
    "ipv.figure()\n",
    "ipv.volshow(fp)\n",
    "ipv.style.use(['minimal','light']) \n",
    "#ipv.view(270,90)\n",
    "ipv.show()\n",
    "time.sleep(2)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Number of <False negative> voxel = {:,.1f}/16.7M'.format(fn.view(-1).sum()))\n",
    "fn = fn.transpose(2,0).flip(1).detach().cpu().numpy().squeeze()\n",
    "ipv.figure()\n",
    "ipv.volshow(fn)\n",
    "ipv.style.use(['minimal','light']) \n",
    "#ipv.view(270,90)\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "def sliceviewer2(voxel1, voxel2, voxel3, voxel4, x):\n",
    "    fig , ax = plt.subplots(2,2,figsize=(14,14))\n",
    "    #fig.suptitle('Comparing result')\n",
    "    ax[0,0].imshow(voxel1[:,:,x] , cmap='bone')\n",
    "    ax[0,0].set_title('Ground truth')\n",
    "    ax[0,1].imshow(voxel2[:,:,x] , cmap='bone')\n",
    "    ax[0,1].set_title('Output')\n",
    "    \n",
    "    ax[1,0].imshow(voxel3[:,:,x] , cmap='seismic')\n",
    "    ax[1,0].set_title('False positive')\n",
    "    ax[1,1].imshow(voxel4[:,:,x] , cmap='seismic')\n",
    "    ax[1,1].set_title('False negative')\n",
    "    \n",
    "    return x\n",
    "\n",
    "#voxel1 = sampleOutput['Target']\n",
    "#voxel2 = sampleOutput['Output']\n",
    "voxel1 = target==1\n",
    "voxel2 = (output[0,1]>=0.5).cpu()\n",
    "print(voxel1.size(), type(voxel1), voxel1.dtype)\n",
    "print(voxel2.size(), type(voxel2), voxel2.dtype)\n",
    "widgets.interact( sliceviewer2, \n",
    "                 voxel1 = widgets.fixed(voxel1), \n",
    "                 voxel2 = widgets.fixed(voxel2), \n",
    "                 voxel3 = widgets.fixed(fp), \n",
    "                 voxel4 = widgets.fixed(fn), \n",
    "                 x=(0,voxel1.shape[2]-1)\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encode_feat1[0].size())\n",
    "print(decode_feat1[6].size())\n",
    "print(fusion_feat[6].size(), fusion_feat[6].dtype)\n",
    "print(fusionUp_feat[5].size(), fusion_feat[6].dtype)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "def enc_dec_feature_visualize(enc1, enc2, dec1, dec2, x1, x2, x3, x4):\n",
    "    \n",
    "    fig , ax = plt.subplots(2,2,figsize=(14,14))\n",
    "    fig.suptitle('Feature map visulization ')\n",
    "    im1 = ax[0,0].imshow(enc1[0,x1,:,:], cmap='seismic')\n",
    "    ax[0,0].set_title('Encode_feat1')\n",
    "    divider = make_axes_locatable(ax[0,0])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im1, cax=cax)\n",
    "    #plt.show()\n",
    "    \n",
    "    im2 = ax[0,1].imshow(enc2[0,x2,:,:], cmap='seismic')\n",
    "    ax[0,1].set_title('Encode_feat2')\n",
    "    divider = make_axes_locatable(ax[0,1])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im2, cax=cax)\n",
    "    #plt.show()\n",
    "    \n",
    "    im3 = ax[1,0].imshow(dec1[0,x3,:,:], cmap='seismic')\n",
    "    ax[1,0].set_title('Decode_feat1')\n",
    "    divider = make_axes_locatable(ax[1,0])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im3, cax=cax)\n",
    "    #plt.show()\n",
    "    \n",
    "    im4 = ax[1,1].imshow(dec2[0,x4,:,:], cmap='seismic')\n",
    "    ax[1,1].set_title('Decode_feat2')\n",
    "    divider = make_axes_locatable(ax[1,1])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im4, cax=cax)\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    return x1, x2, x3, x4\n",
    "\n",
    "def fusion_feature_visualize(fusion, fusionUp, x1, x2):\n",
    "    ipv.figure()\n",
    "    ipv.volshow(fusion[0,x1,:,:,:])\n",
    "    ipv.style.use(['minimal','light']) \n",
    "    #ipv.view(270,90)\n",
    "    ipv.show()\n",
    "    \n",
    "    ipv.figure()\n",
    "    ipv.volshow(fusionUp[0,x2,:,:,:])\n",
    "    ipv.style.use(['minimal','light']) \n",
    "    #ipv.view(270,90)\n",
    "    ipv.show()\n",
    "    return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Encode Level = 0 - {}'.format(len(encode_feat1)-1))\n",
    "print('Decode Level = 0 - {}'.format(len(decode_feat1)-1))\n",
    "print('Fusion Level Up = 0 - {}'.format(len(fusion_feat)-1))\n",
    "print('FusionUp Level Up = 0 - {}'.format(len(fusionUp_feat)-1))\n",
    "print('---------------------------------- \\n')\n",
    "level = int(input('Level select : '))\n",
    "\n",
    "\n",
    "widgets.interact(enc_dec_feature_visualize, \n",
    "                 enc1 = widgets.fixed(encode_feat1[level].cpu().detach().numpy().astype(float)), \n",
    "                 enc2 = widgets.fixed(encode_feat2[level].cpu().detach().numpy().astype(float)), \n",
    "                 dec1 = widgets.fixed(decode_feat1[len(decode_feat1)-1-level].cpu().detach().numpy().astype(float)), \n",
    "                 dec2 = widgets.fixed(decode_feat2[len(decode_feat1)-1-level].cpu().detach().numpy().astype(float)), \n",
    "                 x1=(0,encode_feat1[level].shape[1]-1),\n",
    "                 x2=(0,encode_feat2[level].shape[1]-1),\n",
    "                 x3=(0,decode_feat1[len(decode_feat1)-1-level].shape[1]-1),\n",
    "                 x4=(0,decode_feat2[len(decode_feat2)-1-level].shape[1]-1),\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interact(fusion_feature_visualize, \n",
    "                 fusion = widgets.fixed(fusion_feat[len(fusion_feat)-1-level].cpu().detach().numpy().astype(float)), \n",
    "                 fusionUp = widgets.fixed(fusionUp_feat[len(fusionUp_feat)-1-level].cpu().detach().numpy().astype(float)), \n",
    "                 x1=(0,fusion_feat[len(fusion_feat)-1-level].shape[1]-1),\n",
    "                 x2=(0,fusionUp_feat[len(fusionUp_feat)-1-level].shape[1]-1),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if first==True:\n",
    "    print('First testing sample')\n",
    "    femurIter = iter(valLoader)\n",
    "    first = False\n",
    "else:\n",
    "    print('Next testing sample')\n",
    "    \n",
    "sample = next(femurIter)\n",
    "\n",
    "target , ap , lat = sample['Target'] , sample['AP'] , sample['LAT']\n",
    "#print('Raw1 = {} {} {}'.format(target.size(),ap.size(),lat.size()))\n",
    "target , ap , lat= target[0,1] , ap[0] , lat[0]\n",
    "#print('Raw2 = {} {} {}'.format(target.size(), ap.size(), lat.size()))\n",
    "target = target.unsqueeze(0).unsqueeze(0).to(device=device, dtype=dtype1)\n",
    "ap = ap.unsqueeze(0).to(device=device, dtype=dtype1)\n",
    "lat = lat.unsqueeze(0).to(device=device, dtype=dtype1)\n",
    "#print('Final = {} {} {}'.format(target.size(), ap.size(), lat.size()))\n",
    "\n",
    "with torch.cuda.amp.autocast(enabled=True):\n",
    "    embeded = model1(target)\n",
    "    output = model2(embeded)\n",
    "    print('output = {}   {}'.format(output.size(), output.dtype))\n",
    "    output = output[:,1].unsqueeze(1)\n",
    "    print('output = {}   {}'.format(output.size(), output.dtype))\n",
    "    #print('output.device: {} \\ntarget.device: {} '.format(output.device,target.device))\n",
    "    output = (output>=0.25).float()\n",
    "    print('***  Accuracy = {:.4f}  *** \\n'.format( iou(output,target.to(device=device)) ))\n",
    "\n",
    "    output2 =output.detach().cpu().numpy().squeeze()\n",
    "    #print('output2 type: {}   output2.shape = {} \\n'.format(type(output2),output2.shape))\n",
    "\n",
    "sampleOutput = {'Target':target.cpu().numpy().squeeze() ,\n",
    "                'AP':ap.cpu().numpy().squeeze() , \n",
    "                'LAT':lat.cpu().numpy().squeeze() , \n",
    "                'Output':output2}\n",
    "\n",
    "show_sample(sampleOutput, view='all', showOutput=True, detail=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.max())\n",
    "print(output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,C,D,H,W = output.size()\n",
    "print(output.size(), target.size())\n",
    "tp = TP(output,target.to(device=device))\n",
    "tn = TN(output,target.to(device=device))\n",
    "fp = FP(output,target.to(device=device))\n",
    "fn = FN(output,target.to(device=device))\n",
    "tp_sum = torch.sum(tp.view(-1))\n",
    "tn_sum = torch.sum(tn.view(-1))\n",
    "fp_sum = torch.sum(fp.view(-1))\n",
    "fn_sum = torch.sum(fn.view(-1))\n",
    "\n",
    "# Confusion matrix\n",
    "precision = tp_sum/(tp_sum+fp_sum)\n",
    "recall = tp_sum/(tp_sum+fn_sum)    # = true positive rate(TPR) = Sensivity\n",
    "accuracy = (tp_sum+tn_sum)/(D*H*W)\n",
    "Specificity = tn_sum/(tn_sum+fp_sum)\n",
    "#fpr = 1.0 - Specificity.\n",
    "fpr = fp_sum/(tn_sum+fp_sum)\n",
    "F1 = 2*(precision*recall)/(precision+recall)\n",
    "print('True positive = {:,.1f}'.format(tp_sum))\n",
    "print('False positive = {:,.1f}'.format(fp_sum))\n",
    "print('False negative = {:,.1f}'.format(fn_sum))\n",
    "print('True negative = {:,.1f}\\n'.format(tn_sum))\n",
    "print('Precision = {:,.4f}'.format(precision))\n",
    "print('Recall(TPR or Sensivity) = {:,.4f}'.format(recall))\n",
    "print('Accuracy = {:,.4f}'.format(accuracy))\n",
    "print('True positive rate = {:,.4f}'.format(recall))\n",
    "print('False positive rate = {:,.4f}'.format(fpr))\n",
    "print('F1-score = {:,.4f}\\n\\n'.format(F1))\n",
    "\n",
    "print('Number of <False positive> voxel = {:,.1f}/16.7M'.format(fp.view(-1).sum()))\n",
    "#print(fp.size())\n",
    "#fp = fp.cpu().numpy().squeeze()\n",
    "fp = fp.detach().cpu().numpy().squeeze()\n",
    "#print(fp.shape)\n",
    "ipv.figure()\n",
    "ipv.volshow(fp)\n",
    "ipv.view(270,90)\n",
    "ipv.show()\n",
    "time.sleep(2)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Number of <False negative> voxel = {:,.1f}/16.7M'.format(fn.view(-1).sum()))\n",
    "fn = fn.detach().cpu().numpy().squeeze()\n",
    "ipv.figure()\n",
    "ipv.volshow(fn)\n",
    "ipv.view(270,90)\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "**Evaluate on Testing set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## On Perfect Orthogonal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = netG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\Validation_set.xlsx'\n",
    "eva_transformedFemur = FemurDataset2(csv_file=eva_set, root_dir=root_dir, \n",
    "                                    transform=transforms.Compose([NormalizeSample2(),ToTensor8()]))\n",
    "evaLoader = DataLoader(eva_transformedFemur, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor([0.15,0.25,0.6], device=device)     # For ToTensor8: Auxiliary Class\n",
    "#weight = torch.tensor([0.5,0.5], device=device)           # For ToTensor9: Augmentation only \n",
    "criterion = FocalLossMulticlass(weight=weight, gamma=2.0, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('### Evaluation loop ###')\n",
    "torch.cuda.empty_cache()\n",
    "model.eval()\n",
    "time1 = time.time()\n",
    "column_name = ['loss','IoU1','TP1','FP1','FN1','TN1','ASD_gt','ASD_ot','ASSD','RMSD_gt','RMSD_ot','HD_gt','HD_ot','BHD','IoU2','TP2','FP2','FN2','TN2']   # Auxiliary Class\n",
    "#column_name = ['loss','IoU1','TP1','FP1','FN1','TN1','ASD_gt','ASD_ot','ASSD','RMSD_gt','RMSD_ot','HD_gt','HD_ot','BHD']                                  # Without Auxiliary Class\n",
    "result = np.zeros((len(evaLoader),len(column_name)))\n",
    "progressbar = tqdm(enumerate(evaLoader), total=len(evaLoader), desc=\"Process:   \")\n",
    "for  t, eva_sample in progressbar:\n",
    "#for t, val_sample in tqdm(enumerate(valLoader)):\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            target = eva_sample['Target'].to(device=device, dtype=dtype1)\n",
    "            view1 = eva_sample['view1'].to(device=device, dtype=dtype1)\n",
    "            view2 = eva_sample['view2'].to(device=device, dtype=dtype1)\n",
    "            #view1 = eva_sample['AP'].to(device=device, dtype=dtype1)\n",
    "            #view2 = eva_sample['LAT'].to(device=device, dtype=dtype1)\n",
    "            output = model(view1, view2)\n",
    "            \n",
    "            loss = criterion(output,target.long())   # for ToTensor6-7 with FocalMultiClass\n",
    "            #loss_eva.append(loss.item())\n",
    "            \n",
    "            target_aux = (target[0,:]==2).float().detach().cpu()       # for Auxiliary (only) or Auxiliary + FracAugmentation\n",
    "            output_aux = (output[0,2]>=0.5).float().detach().cpu()     # for Auxiliary (only) or Auxiliary + FracAugmentation\n",
    "            target = (target[0,:]==1).float().detach().cpu()\n",
    "            output = (output[0,1]>=0.5).float().detach().cpu()\n",
    "            \n",
    "            metrices = overlap_based_metrices(output, target, sum_of_matrix=True)                        # for ToTensor7-9\n",
    "            metrices2 = overlap_based_metrices(output_aux, target_aux, sum_of_matrix=True)               # for ToTensor7-9\n",
    "            bound_metrices = surface_distance_measurement(target.numpy(), output.numpy(), res=0.5, return_vert_dist=False, verbose=False)\n",
    "            \n",
    "            result[t,:] = [loss,\n",
    "                           *[ i.item() for i in metrices.values()],\n",
    "                           *bound_metrices.values(),\n",
    "                           *[ i.item() for i in metrices2.values()]\n",
    "                          ]\n",
    "            progressbar.set_description('Loss:{:,.0f}   IOU:{:.3f}   ASSD:{:,.2f}'.format(loss,metrices['IoU'],bound_metrices['ASSD']))\n",
    "            \n",
    "time2 = time.time()\n",
    "eva_loss_acc = pd.DataFrame(data=result, columns=column_name)\n",
    "print('   Duration Testing time = {} Min. '.format((time2-time1)/60))\n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Recon2X3D5_21101901\t\tBoth\n",
    "Recon2X3D5_21102201\t\tAux only\n",
    "Recon2X3D5_21102101\t\tFracAug only\n",
    "Recon2X3D5_21102102\t\tBare\n",
    "Recon2X3D5GAN_22022201\t\tFracReconNet-GAN\n",
    "'''\n",
    "print(saved_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_loss_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_loss_acc.to_csv(path_or_buf='trained\\Recon2X3D5GAN_22022201.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bound_metrices.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [ i.item() for i in metrices2.values()]\n",
    "print(x, len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(result[:,0], bins=100)\n",
    "plt.xlabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "ax[0].hist(result[:,1], bins=100)    # IoU_bone\n",
    "ax[0].set_title('Intersection-Over-Union of Bone')\n",
    "ax[0].set_xlabel('Samples')\n",
    "ax[1].hist(result[:,8], bins=100)      # ASSD\n",
    "ax[1].set_title('Average Symmetrical Surface Distances')\n",
    "ax[1].set_xlabel('Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = np.random.randint(0, 10, size=(1000))\n",
    "print(cc.shape)\n",
    "#cc_hist = np.histogram(cc, bins=100)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(cc, bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipv.figure()\n",
    "ipv.volshow(target==2)\n",
    "ipv.show()\n",
    "time.sleep(3)\n",
    "ipv.figure()\n",
    "ipv.volshow(output[2]>=0.5)\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.size())\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.detach().cpu().numpy().squeeze()\n",
    "target = target.detach().cpu().numpy().squeeze()\n",
    "print(type(output), output.shape, output.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('output.npy', output)\n",
    "np.save('target.npy', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[2].shape)\n",
    "print((target==2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ETC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument('-i', '--image', required=True,\n",
    "    help='path to image')\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "#model = models.resnet50(pretrained=True)\n",
    "print(model)\n",
    "model_weights = [] # we will save the conv layer weights in this list\n",
    "conv_layers = [] # we will save the 49 conv layers in this list\n",
    "# get all the model children as list\n",
    "model_children = list(model.children())\n",
    "print(model_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter to keep count of the conv layers\n",
    "counterConv2d = 0\n",
    "counterConv3d = 0\n",
    "counterConvTranspose3d = 0\n",
    "# append all the conv layers and their respective weights to the list\n",
    "for i,level in enumerate(model_children):\n",
    "    print('Level:{}'.format(i))\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Conv3d) or isinstance(module, nn.ConvTranspose3d):\n",
    "        model_weights.append(level.weight)\n",
    "        conv_layers.append(level)\n",
    "    for j,layer in enumerate(level):\n",
    "        print('   Layer:{}'.format(j))\n",
    "        for k,module in enumerate(layer):\n",
    "            print('      Module:{}'.format(k))\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Conv3d) or isinstance(module, nn.ConvTranspose3d):\n",
    "                model_weights.append(module.weight)\n",
    "                conv_layers.append(module)\n",
    "print('Total Conv2d = {}'.format(counterConv2d))\n",
    "print('Total Conv3d = {}'.format(counterConv3d))\n",
    "print('Total ConvTranspose3d = {}'.format(counterConvTranspose3d))\n",
    "\n",
    "'''\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        counter += 1\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    elif type(model_children[i]) == nn.Sequential:\n",
    "        for j in range(len(model_children[i])):\n",
    "            for child in model_children[i][j].children():\n",
    "                if type(child) == nn.Conv2d:\n",
    "                    counter += 1\n",
    "                    model_weights.append(child.weight)\n",
    "                    conv_layers.append(child)\n",
    "print(f\"Total convolutional layers: {counter}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first conv layer filters\n",
    "plt.figure(figsize=(20, 17))\n",
    "for i, filter in enumerate(model_weights[0]):\n",
    "    plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "    plt.imshow(filter[0, :, :].detach(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('../outputs/filter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the image through all the layers\n",
    "results = [conv_layers[0](img)]\n",
    "for i in range(1, len(conv_layers)):\n",
    "    # pass the result from the last layer to the next layer\n",
    "    results.append(conv_layers[i](results[-1]))\n",
    "# make a copy of the `results`\n",
    "outputs = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## On Non-orthogonal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eva_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\Validation_set.xlsx'\n",
    "eva_set = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\TestingSet_Scale12.xlsx'\n",
    "eva_transformedFemur = FemurDataset2(csv_file=eva_set, root_dir=root_dir, \n",
    "                                     transform=transforms.Compose([NormalizeSample3(),ToTensor8Plus()]))\n",
    "evaLoader = DataLoader(eva_transformedFemur, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor([0.15,0.25,0.6], device=device)     # For ToTensor8: Auxiliary Class\n",
    "#weight = torch.tensor([0.5,0.5], device=device)           # For ToTensor9: Augmentation only \n",
    "criterion = FocalLossMulticlass(weight=weight, gamma=2.0, reduction='sum')\n",
    "\n",
    "def findOccurrencesAngle(strr, ch):\n",
    "    kk = list()      # location matched ch\n",
    "    strr = strr[0]\n",
    "    for ff, letter in enumerate(strr):\n",
    "        #print('ff = {}   letter = {}'.format(ff, letter))\n",
    "        if letter == ch:\n",
    "            #print('Match!!! = {}'.format(ch))\n",
    "            kk.append(ff)\n",
    "    #print('kk = {}'.format(kk))\n",
    "    angle = float(strr[kk[-2]+1:kk[-1]])\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set00 = dict()\n",
    "set25 = dict()\n",
    "set50 = dict()\n",
    "set75 = dict()\n",
    "set100 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_based_metrices\n",
    "overlap_based_metrices\n",
    "surface_distance_measurement\n",
    "print('Checked Evaluation metrics: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('### Evaluation loop of Unaligned Dataset ###')\n",
    "torch.cuda.empty_cache()\n",
    "model.eval()\n",
    "\n",
    "time1 = time.time()\n",
    "column_name = ['diff_angle','loss','IoU1','TP1','FP1','FN1','TN1','ASD_gt','ASD_ot','ASSD','RMSD_gt','RMSD_ot','HD_gt','HD_ot','BHD','IoU2','TP2','FP2','FN2','TN2']  # Auxiliary Class\n",
    "#column_name = ['loss','IoU1','TP1','FP1','FN1','TN1','ASD_gt','ASD_ot','ASSD','RMSD_gt','RMSD_ot','HD_gt','HD_ot','BHD']                                  # Without Auxiliary Class\n",
    "result = np.zeros((len(evaLoader),len(column_name)))\n",
    "progressbar = tqdm(enumerate(evaLoader), total=len(evaLoader), desc=\"Process:   \")\n",
    "for  t, eva_sample in progressbar:\n",
    "#for t, val_sample in tqdm(enumerate(valLoader)):\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            target = eva_sample['Target'].to(device=device, dtype=dtype1)\n",
    "            view1 = eva_sample['view1'].to(device=device, dtype=dtype1)\n",
    "            view2 = eva_sample['view2'].to(device=device, dtype=dtype1)\n",
    "            drr1_name = eva_sample['drr1_name']\n",
    "            drr2_name = eva_sample['drr2_name']\n",
    "            ang1 = findOccurrencesAngle(drr1_name,'-')\n",
    "            ang2 = findOccurrencesAngle(drr2_name,'-')\n",
    "            #print('drr1_name = {} {}'.format(drr1_name,findOccurrencesAngle(drr1_name,'-')))\n",
    "            #print('drr2_name = {} {}'.format(drr2_name,findOccurrencesAngle(drr2_name,'-')))\n",
    "            diff_angle = abs(ang1 - ang2)\n",
    "            output = model(view1, view2)\n",
    "            loss = criterion(output,target.long())   # for ToTensor6-7 with FocalMultiClass\n",
    "            #loss_eva.append(loss.item())\n",
    "            \n",
    "            target_aux = (target[0,:]==2).float().detach().cpu()       # for Auxiliary (only) or Auxiliary + FracAugmentation\n",
    "            output_aux = (output[0,2]>=0.5).float().detach().cpu()     # for Auxiliary (only) or Auxiliary + FracAugmentation\n",
    "            target = (target[0,:]==1).float().detach().cpu()\n",
    "            output = (output[0,1]>=0.5).float().detach().cpu()\n",
    "            \n",
    "            metrices = overlap_based_metrices(output, target, sum_of_matrix=True)                        # for ToTensor7-9\n",
    "            metrices2 = overlap_based_metrices(output_aux, target_aux, sum_of_matrix=True)               # for ToTensor7-9\n",
    "            bound_metrices = surface_distance_measurement(target.numpy(), output.numpy(), res=0.5, return_vert_dist=False, verbose=False)\n",
    "            \n",
    "            result[t,:] = [diff_angle, loss.cpu().numpy(),\n",
    "                           *[ i.item() for i in metrices.values()],   # 5 metrics\n",
    "                           *bound_metrices.values(),                  # 8 metrics\n",
    "                           *[ i.item() for i in metrices2.values()]   # 5 metrics\n",
    "                          ]\n",
    "            progressbar.set_description('Loss:{:,.0f}   IOU:{:.3f}   ASSD:{:,.2f}'.format(loss,metrices['IoU'],bound_metrices['ASSD']))\n",
    "            \n",
    "time2 = time.time()\n",
    "eva_loss_acc = pd.DataFrame(data=result, columns=column_name)\n",
    "print('   Duration Testing time = {} Min. '.format((time2-time1)/60))\n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_loss_acc.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_loss_acc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_loss_acc.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Evaluation sample\n",
    "\n",
    "eval_savename = 'trained\\Recon2X3D6_22100401_unalign12.csv'\n",
    "\n",
    "eval_save_logic = int(input('Save the evaluation [1] Yes [0] No ?'))\n",
    "if eval_save_logic == 1:\n",
    "    verify_eval_savename = int(input('Evaluation savename = {} :\\n   [1] Yes [0] No = '.format(eval_savename)))\n",
    "    if verify_eval_savename == 1:\n",
    "        eva_loss_acc.to_csv(path_or_buf=eval_savename)\n",
    "        print('--- Save is done ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eva_loss_acc\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df00 = df[df['diff_angle']==0.0]\n",
    "df00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df25 = df[df['diff_angle']==2.5]\n",
    "df25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df50 = df[df['diff_angle']==5]\n",
    "df50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df75 = df[df['diff_angle']==7.5]\n",
    "df75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df100 = df[df['diff_angle']==10]\n",
    "df100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test with Real-world X-ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\FEW PhD\\Datasets\\Siriraj Dataset\\Cleaning Data\\3D Intensity Volume (RAW)\\Xray'   \n",
    "\n",
    "xray1 = np.load(path+r'\\Xray1-52827012-Scale-0.7.npy')   # Intact\n",
    "xray2 = np.load(path+r'\\Xray2-52827012-Scale-0.7.npy')   # Intact\n",
    "#xray1 = np.load(path+'\\Xray1-53568941-Scale-0.6.npy')   # Nondisplaced\n",
    "#xray2 = np.load(path+'\\Xray2-53568941-Scale-0.6.npy')   # Nondisplaced\n",
    "#xray1 = np.load(path+'\\Xray1--Scale-0.6.npy')   # Displaced\n",
    "#xray2 = np.load(path+'\\Xray2--Scale-0.6.npy')   # Displaced\n",
    "print('xray1 = {}  {}  {}\\nvalue = {} - {} '.format(xray1.shape, type(xray1), xray1.dtype,xray1.min(),xray1.max()))\n",
    "print('xray2 = {}  {}  {}\\nvalue = {} - {} '.format(xray2.shape, type(xray2), xray2.dtype,xray2.min(),xray2.max()))\n",
    "maxx = xray1.max()\n",
    "xray1 = xray1/maxx\n",
    "xray2 = xray2/maxx\n",
    "print('xray1 = {}  {}  {}\\nvalue = {} - {} '.format(xray1.shape, type(xray1), xray1.dtype,xray1.min(),xray1.max()))\n",
    "print('xray2 = {}  {}  {}\\nvalue = {} - {} '.format(xray2.shape, type(xray2), xray2.dtype,xray2.min(),xray2.max()))\n",
    "fig, ax = plt.subplots(1,2,figsize=(24,24))\n",
    "ax[0].imshow(1-xray1, cmap='gray')\n",
    "ax[0].set_title('Judet Xray1')\n",
    "ax[1].imshow(1-xray2, cmap='gray')\n",
    "ax[1].set_title('Judet Xray2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(xray1,dtype=torch.float).unsqueeze(dim=0).unsqueeze(dim=1).cuda()\n",
    "x2 = torch.tensor(xray2,dtype=torch.float).unsqueeze(dim=0).unsqueeze(dim=1).cuda()\n",
    "print('x1 = {} {} {} cuda{}'.format(x1.size(),type(x1),x1.dtype,x1.get_device()))\n",
    "print('x2 = {} {} {} cuda{}'.format(x2.size(),type(x2),x2.dtype,x2.get_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model(1-x1,1-x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_view1 = [0,0,-1.5, 0,0,0 ,0,-1,0]   # view1\n",
    "cam_view2 = [1.5,0,0, 0,0,0 ,1,-1,0]    # view2\n",
    "plot = k3d.plot(name='Plot output')\n",
    "obj = k3d.volume(predict[0,2].detach().cpu().numpy(), name='predict', \n",
    "                 color_map=k3d.colormaps.matplotlib_color_maps.Bone,\n",
    "                 gradient_step=0.005,\n",
    "                 shadow='dynamic',\n",
    "                 shadow_delay=10,\n",
    "                )\n",
    "plot += obj + k3d.text2d(text='Predict', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "plot.display()\n",
    "plot.camera = cam_view2\n",
    "#plot.camera = cam_view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xray_filename = r'D:\\FEW PhD\\Program\\3D_Reconstruction\\femur3dnet\\trained\\K3DResult\\Siriraj_Dataset\\Volume\\sample.html'\n",
    "with open(xray_filename,'w') as fp:\n",
    "    fp.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset4_Unaligned\\DRR1-20140225CT0101-1-1-41-4.npy')\n",
    "print('x = {} {} {}'.format(x.shape,type(x),x.dtype))\n",
    "print('x values = {} - {}'.format(x.min(),x.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# ETC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T11:59:15.169206Z",
     "start_time": "2020-10-21T11:59:15.158207Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T12:00:10.467239Z",
     "start_time": "2020-10-21T12:00:09.516191Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "femurBatched = next(iter(femurLoader))\n",
    "print('femurBatched.size = {}'.format(femurBatched['AP'].size()))\n",
    "img = femurBatched['AP']\n",
    "#img = img.unsqueeze(dim=0)\n",
    "img = img.to(device=device)                # img.to(device=torch.device('cpu'))\n",
    "print('img dtype = {}'.format(img.type()))\n",
    "print('img.size = {}'.format(img.size()))\n",
    "imgs = utils.make_grid(femurBatched['AP'])\n",
    "imgs_show = imgs.numpy().transpose(1,2,0)\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(imgs_show, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T12:09:29.938194Z",
     "start_time": "2020-10-21T12:09:27.923820Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "writer.add_image('AP',imgs)\n",
    "writer.add_graph(model,img.to(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(logRecordDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T12:10:24.012406Z",
     "start_time": "2020-10-21T12:10:23.996322Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir='runs/Recon3DUNet_201023'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Register Hook\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    def __init__(self, module):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = torch.tensor(output,requires_grad=True).cuda()\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "        \n",
    "class FilterVisualizer():\n",
    "    def __init__(self, size=56, upscaling_steps=12, upscaling_factor=1.2):\n",
    "        self.size, self.upscaling_steps, self.upscaling_factor = size, upscaling_steps, upscaling_factor\n",
    "        self.model = model.cuda().eval()\n",
    "        #print(model.parameters)\n",
    "        #self.model = vgg16(pretrained=True).cuda().eval()\n",
    "        #set_trainable(self.model, False)\n",
    "\n",
    "    def visualize(self, layer, filter, lr=0.1, opt_steps=20, blur=None):\n",
    "        sz = self.size\n",
    "        img = np.uint8(np.random.uniform(150, 180, (sz, sz, 3)))/255  # generate random image\n",
    "        activations = SaveFeatures(list(self.model.children())[layer])  # register hook\n",
    "\n",
    "        for _ in range(self.upscaling_steps):  # scale the image up upscaling_steps times\n",
    "            train_tfms, val_tfms = tfms_from_model(vgg16, sz)\n",
    "            img_var = V(val_tfms(img)[None], requires_grad=True)  # convert image to Variable that requires grad\n",
    "            optimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n",
    "            for n in range(opt_steps):  # optimize pixel values for opt_steps times\n",
    "                optimizer.zero_grad()\n",
    "                self.model(img_var)\n",
    "                loss = -activations.features[0, filter].mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            img = val_tfms.denorm(img_var.data.cpu().numpy()[0].transpose(1,2,0))\n",
    "            self.output = img\n",
    "            sz = int(self.upscaling_factor * sz)  # calculate new image size\n",
    "            img = cv2.resize(img, (sz, sz), interpolation = cv2.INTER_CUBIC)  # scale image up\n",
    "            if blur is not None: img = cv2.blur(img,(blur,blur))  # blur image to reduce high frequency patterns\n",
    "        self.save(layer, filter)\n",
    "        activations.close()\n",
    "        \n",
    "    def save(self, layer, filter):\n",
    "        plt.imsave(\"layer_\"+str(layer)+\"_filter_\"+str(filter)+\".jpg\", np.clip(self.output, 0, 1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#vgg16 = torchvision.models.vgg16(pretrained=True)  # load and save in C:\\Users\\BDML\\.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
    "layer = 40\n",
    "filter = 265\n",
    "FV = FilterVisualizer(size=56, upscaling_steps=12, upscaling_factor=1.2)\n",
    "print(type(FV))\n",
    "FV.visualize(layer, filter, blur=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    def __init__(self, module, backward=False):    # module can be any torch.nn.Module such as nn.Linear, nn.Conv2d\n",
    "        if backward==False:\n",
    "            self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = module.register_backward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#hookF = [Hook(layer[1]) for layer in list(model._modules.items())]          # forward pass hooked\n",
    "#hookB = [Hook(layer[1],backward=True) for layer in list(model._modules.items())]     # backward pass hooked\n",
    "\n",
    "hookF = None\n",
    "hookB = None\n",
    "for layer in list(model._modules.items()):\n",
    "    if isinstance(layer,nn.MaxPool2d) or isinstance(layer,nn.Upsample):\n",
    "        break\n",
    "    hookF = Hook(layer[1])\n",
    "    hookB = Hook(layer[1],backward=True)\n",
    "    \n",
    "# run a data batch\n",
    "out=model(ap_I).to(device=device)\n",
    "\n",
    "\n",
    "print('***'*3 + '  Forward Hooks Inputs & Outputs  '+'***'*3)\n",
    "for hook in hookF:\n",
    "    print('layer input = {}'.format(hook.input))\n",
    "    print('layer output = {}'.format(hook.output))\n",
    "    print('---'*17)\n",
    "print('\\n')\n",
    "\n",
    "print('***'*3 + '  Backward Hooks Inputs & Outputs  '+'***'*3)\n",
    "for hook in hookB:             \n",
    "    print('layer input = {}'.format(hook.input))\n",
    "    print('layer output = {}'.format(hook.output))       \n",
    "    print('---'*17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Feature show\n",
    "\n",
    "https://towardsdatascience.com/visualizing-convolution-neural-networks-using-pytorch-3dfa8443e74e\n",
    "https://towardsdatascience.com/visualizing-convolution-neural-networks-using-pytorch-3dfa8443e74e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for ploting CNN paraemters\n",
    "\n",
    "def plot_filters_single_channel_big(t):\n",
    "    \n",
    "    #setting the rows and columns\n",
    "    nrows = t.shape[0]*t.shape[2]\n",
    "    ncols = t.shape[1]*t.shape[3]\n",
    "    \n",
    "    npimg = np.array(t.numpy(), np.float32)\n",
    "    npimg = npimg.transpose((0, 2, 1, 3))\n",
    "    npimg = npimg.ravel().reshape(nrows, ncols)\n",
    "    \n",
    "    npimg = npimg.T\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(ncols/10, nrows/200))    \n",
    "    imgplot = sns.heatmap(npimg, xticklabels=False, yticklabels=False, cmap='gray', ax=ax, cbar=False)\n",
    "\n",
    "    \n",
    "def plot_filters_single_channel(t):    # t = 4-Dimension tensor\n",
    "    \n",
    "    #kernels depth * number of kernels\n",
    "    nplots = t.shape[0]*t.shape[1]\n",
    "    ncols = 12\n",
    "    \n",
    "    nrows = 1 + nplots//ncols\n",
    "    #convert tensor to numpy image\n",
    "    #npimg = np.array(t.numpy(), np.float32)\n",
    "    \n",
    "    count = 0\n",
    "    fig = plt.figure(figsize=(ncols, nrows))    # figsize=(ncols, nrows)\n",
    "    #looping through all the kernels in each channel\n",
    "    for i in range(t.shape[0]):\n",
    "        for j in range(t.shape[1]):\n",
    "            count += 1\n",
    "            ax1 = fig.add_subplot(nrows, ncols, count)\n",
    "            npimg = np.array(t[i, j].cpu().numpy(), np.float32)\n",
    "            #standardize the numpy image\n",
    "            #npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "            #npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "            ax1.imshow(npimg,cmap='gray')\n",
    "            ax1.set_title(str(i) + ',' + str(j))\n",
    "            ax1.axis('off')\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.set_yticklabels([])\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_normlayer_single_channel(t):    # t = 3-Dimension tensor\n",
    "    \n",
    "    #kernels depth * number of kernels\n",
    "    nplots = t.shape[0]\n",
    "    ncols = 12\n",
    "    \n",
    "    nrows = 1 + nplots//ncols\n",
    "    #convert tensor to numpy image\n",
    "    #npimg = np.array(t.numpy(), np.float32)\n",
    "    \n",
    "    count = 0\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    #looping through all the kernels in each channel\n",
    "    for i in range(t.size(dim=0)):\n",
    "        count += 1\n",
    "        ax1 = fig.add_subplot(nrows, ncols, count)\n",
    "        npimg = np.array(t[i,:,:].numpy(), np.float32)\n",
    "        #standardize the numpy image\n",
    "        #npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "        #npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "        ax1.imshow(npimg,cmap='gray')\n",
    "        ax1.set_title(str(i))\n",
    "        ax1.axis('off')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_filters_multi_channel(t):\n",
    "    \n",
    "    #get the number of kernals\n",
    "    num_kernels = t.shape[0]    \n",
    "    \n",
    "    #define number of columns for subplots\n",
    "    num_cols = 12\n",
    "    #rows = num of kernels\n",
    "    num_rows = num_kernels\n",
    "    \n",
    "    #set the figure size\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    \n",
    "    #looping through all the kernels\n",
    "    for i in range(t.shape[0]):\n",
    "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
    "        #for each kernel, we convert the tensor to numpy \n",
    "        npimg = np.array(t[i].numpy(), np.float32)\n",
    "        #standardize the numpy image\n",
    "        #npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "        #npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "        #npimg = npimg.transpose((1, 2, 0))\n",
    "        ax1.imshow(npimg)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title(str(i))\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        \n",
    "    plt.savefig('myimage.png', dpi=100)    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_weights(model, layer_num, single_channel = True, collated = False):\n",
    "    #extracting the model features at the particular layer number\n",
    "    layer = model.features[layer_num]\n",
    "  \n",
    "    #checking whether the layer is convolution layer or not \n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        #getting the weight tensor data\n",
    "        weight_tensor = model.features[layer_num].weight.data\n",
    "    \n",
    "        if single_channel:\n",
    "            if collated:\n",
    "                plot_filters_single_channel_big(weight_tensor)\n",
    "            else:\n",
    "                plot_filters_single_channel(weight_tensor)\n",
    "        else:\n",
    "            if weight_tensor.shape[1] == 3:\n",
    "                plot_filters_multi_channel(weight_tensor)\n",
    "            else:\n",
    "                print(\"Can only plot weights with three channels with single channel = False\") \n",
    "    else:\n",
    "        print(\"Can only visualize layers which are convolutional\")\n",
    "\n",
    "def plotFilterWeight(model):\n",
    "    nameList = list()\n",
    "    paramList = list()\n",
    "    for n,p in model.named_parameters():\n",
    "        nameList.append(n)\n",
    "        paramList.append(p)\n",
    "    \n",
    "    print('Number of Parameter = {} \\n'.format(len(nameList)))\n",
    "    for i in range(len(nameList)):\n",
    "        if i > 20:\n",
    "            print('!!! Break !!!')\n",
    "            break\n",
    "        \n",
    "        name = nameList.pop(0)\n",
    "        param = paramList.pop(0).detach()\n",
    "        print('Name: {}   |   Parameter.size = {}'.format(name,param.size()))\n",
    "        if param.ndim == 4:\n",
    "            plot_filters_single_channel(param)\n",
    "        elif param.ndim == 1:\n",
    "            print('Bias = {}'.format(param))\n",
    "        elif param.ndim == 3:\n",
    "            #plot_normlayer_single_channel(param)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#plot_weights(model, 1, single_channel = False)\n",
    "plotFilterWeight(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1 , count2 , count3 , count4 = 0 , 0 , 0 , 0\n",
    "for n , w in model.named_parameters():\n",
    "    if isinstance(n , nn.Conv2d):\n",
    "        count1 += 1\n",
    "        print('Conv2d = {} :'.format(n) , end=' :  ')\n",
    "    elif isinstance(n , nn.LayerNorm):\n",
    "        count2 += 1\n",
    "        print('LayerNorm = {} '.format(n) , end=' :  ')\n",
    "    elif isinstance(n , nn.MaxPool2d):\n",
    "        count3 += 1\n",
    "        print('MaxPool2d = {}'.format(n) , end=' :  ')\n",
    "    elif isinstance(n , nn.Upsample):\n",
    "        count4 += 1\n",
    "        print('Upsample = {}'.format(n) , end=' :  ')\n",
    "    print('{}'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1 , count2 , count3 , count4 = 0 , 0 , 0 , 0\n",
    "for n in model.children():\n",
    "    if isinstance(n , nn.Conv2d):\n",
    "        count1 += 1\n",
    "        print('Conv2d-({})   '.format(str(count1).zfill(2)) , end=' :  ')\n",
    "    elif isinstance(n , nn.LayerNorm):\n",
    "        count2 += 1\n",
    "        print('LayerNorm-({})'.format(str(count2).zfill(2)) , end=' :  ')\n",
    "    elif isinstance(n , nn.MaxPool2d):\n",
    "        count3 += 1\n",
    "        print('MaxPool2d-({})'.format(str(count3).zfill(2)) , end=' :  ')\n",
    "    elif isinstance(n , nn.Upsample):\n",
    "        count4 += 1\n",
    "        print('Upsample-({}) '.format(str(count4).zfill(2)) , end=' :  ')\n",
    "    print('{}'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nameList = list()\n",
    "paramList = list()\n",
    "for name, param in model.named_parameters():\n",
    "    #print('Layer = {}    Size = {}  '.format(name, param.size()))\n",
    "    nameList.append(name)\n",
    "    paramList.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = nameList.pop(0)\n",
    "param = paramList.pop(0)\n",
    "print('Name = {} :   Weight size = {}'.format(name,param.size()))\n",
    "#plot_filters_single_channel(param.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for ploting CNN paraemters\n",
    "\n",
    "def plot_filters_single_channel_big(t):\n",
    "    \n",
    "    #setting the rows and columns\n",
    "    nrows = t.shape[0]*t.shape[2]\n",
    "    ncols = t.shape[1]*t.shape[3]\n",
    "    \n",
    "    npimg = np.array(t.numpy(), np.float32)\n",
    "    npimg = npimg.transpose((0, 2, 1, 3))\n",
    "    npimg = npimg.ravel().reshape(nrows, ncols)\n",
    "    \n",
    "    npimg = npimg.T\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(ncols/10, nrows/200))    \n",
    "    imgplot = sns.heatmap(npimg, xticklabels=False, yticklabels=False, cmap='gray', ax=ax, cbar=False)\n",
    "\n",
    "    \n",
    "def plot_filters_single_channel(t):\n",
    "    \n",
    "    #kernels depth * number of kernels\n",
    "    nplots = t.shape[0]*t.shape[1]\n",
    "    ncols = 12\n",
    "    \n",
    "    nrows = 1 + nplots//ncols\n",
    "    #convert tensor to numpy image\n",
    "    npimg = np.array(t.numpy(), np.float32)\n",
    "    \n",
    "    count = 0\n",
    "    fig = plt.figure(figsize=(ncols, nrows))\n",
    "    \n",
    "    #looping through all the kernels in each channel\n",
    "    for i in range(t.shape[0]):\n",
    "        for j in range(t.shape[1]):\n",
    "            count += 1\n",
    "            ax1 = fig.add_subplot(nrows, ncols, count)\n",
    "            npimg = np.array(t[i, j].numpy(), np.float32)\n",
    "            npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "            ax1.imshow(npimg)\n",
    "            ax1.set_title(str(i) + ',' + str(j))\n",
    "            ax1.axis('off')\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.set_yticklabels([])\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_filters_multi_channel(t):\n",
    "    \n",
    "    #get the number of kernals\n",
    "    num_kernels = t.shape[0]    \n",
    "    \n",
    "    #define number of columns for subplots\n",
    "    num_cols = 12\n",
    "    #rows = num of kernels\n",
    "    num_rows = num_kernels\n",
    "    \n",
    "    #set the figure size\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    \n",
    "    #looping through all the kernels\n",
    "    for i in range(t.shape[0]):\n",
    "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
    "        \n",
    "        #for each kernel, we convert the tensor to numpy \n",
    "        npimg = np.array(t[i].numpy(), np.float32)\n",
    "        #standardize the numpy image\n",
    "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "        npimg = npimg.transpose((1, 2, 0))\n",
    "        ax1.imshow(npimg)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title(str(i))\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        \n",
    "    plt.savefig('myimage.png', dpi=100)    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_weights(model, layer_num, single_channel = True, collated = False):\n",
    "    #extracting the model features at the particular layer number\n",
    "    layer = model.features[layer_num]\n",
    "  \n",
    "    #checking whether the layer is convolution layer or not \n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        #getting the weight tensor data\n",
    "        weight_tensor = model.features[layer_num].weight.data\n",
    "    \n",
    "        if single_channel:\n",
    "            if collated:\n",
    "                plot_filters_single_channel_big(weight_tensor)\n",
    "            else:\n",
    "                plot_filters_single_channel(weight_tensor)\n",
    "        else:\n",
    "            if weight_tensor.shape[1] == 3:\n",
    "                plot_filters_multi_channel(weight_tensor)\n",
    "            else:\n",
    "                print(\"Can only plot weights with three channels with single channel = False\") \n",
    "    else:\n",
    "        print(\"Can only visualize layers which are convolutional\")\n",
    "        \n",
    "        \n",
    "plot_weights(model, 1, single_channel = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize Feature Map\n",
    "\n",
    "https://github.com/utkuozbulak/pytorch-cnn-visualizations#convolutional-neural-network-filter-visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchCNNVisualization.src import cnn_layer_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layer = 17\n",
    "filter_pos = 5\n",
    "\n",
    "# Fully connected layer is not needed\n",
    "pretrained_model = torchvision.models.vgg16(pretrained=True).features\n",
    "layer_vis = cnn_layer_visualization.CNNLayerVisualization(pretrained_model, cnn_layer, filter_pos)\n",
    "\n",
    "# Layer visualization with pytorch hooks\n",
    "layer_vis.visualise_layer_with_hooks()\n",
    "\n",
    "# Layer visualization without pytorch hooks\n",
    "# layer_vis.visualise_layer_without_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "756.989px",
    "left": "1385.45px",
    "top": "110px",
    "width": "330px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": "10",
    "lenType": "10",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 797,
   "position": {
    "height": "588px",
    "left": "443.938px",
    "right": "20px",
    "top": "108.875px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
