{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff2626f-cddd-47a6-8409-8d3a4ad7ad1c",
   "metadata": {},
   "source": [
    "# Demo FracReconNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20acce7f-216b-4bd9-877c-c1461a8c0285",
   "metadata": {},
   "source": [
    "We recommend using Jupyter Lab to be consistent with K3D visualization toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0785080-7757-43d7-9bad-e9d1541eb892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Toolbox \n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import skimage\n",
    "#from skimage import io, transform, measure\n",
    "import scipy\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "#import cv2\n",
    "#from PIL import Image\n",
    "#import sys\n",
    "import ipyvolume as ipv\n",
    "import k3d\n",
    "import pythreejs   # for controlling the Camera using with ipyvolume\n",
    "import ipywidgets\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import joblib\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Advance Toolbox \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F                                      # useful stateless functions\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torch.autograd import Function, Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "from torchvision import utils, transforms\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "from model.utils import show_sample, FemurDataset, NormalizeSample, ToTensor, ToTensor2\n",
    "from model.matricesOperator import iou, TP, FP, TN, FN, union, hausdorff_voxel, overlap_based_metrices, mesh_surface_nearest_distance, surface_distance_measurement\n",
    "from model.losses import FocalLossMulticlass\n",
    "import model.model as Model\n",
    "from model.training import train_mixed\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32        # we will be using float throughout this tutorial\n",
    "print('Available device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34bd1b-8d4f-4631-b084-340593b19219",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FracReconNet's Configuration ###\n",
    "\n",
    "# load save state dict\n",
    "saved_name = '.\\weight\\FracReconNet.pt'\n",
    "saved_dict = torch.load(saved_name, map_location=device)\n",
    "print('Model: in_c  = {}'.format(saved_dict['in_c']))\n",
    "print('Model: en_sz = {}'.format(saved_dict['en_sz']))\n",
    "print('Model: de_sz = {}'.format(saved_dict['de_sz']))\n",
    "print('Model: de3d_sz = {}'.format(saved_dict['de3d_sz']))\n",
    "print('Model: final_sz = {}'.format(saved_dict['final_sz']))\n",
    "print('Note: {}'.format(saved_dict['note']))\n",
    "print('Timestamp = {}\\n'.format(saved_dict['timestamp']))\n",
    "\n",
    "# Building the model\n",
    "fracReconNet = Model.fracReconNet(saved_dict['in_c'],saved_dict['en_sz'],saved_dict['de_sz'],saved_dict['de3d_sz'],saved_dict['final_sz'])\n",
    "fracReconNet.load_state_dict(saved_dict['model_state_dict'])          # Load the best model to test and inference\n",
    "fracReconNet.to(device=device)\n",
    "\n",
    "# Optimizer configuration\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(fracReconNet.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, threshold=1e-3, \n",
    "                              threshold_mode='rel', cooldown=0, min_lr=0, verbose=True)\n",
    "optimizer.load_state_dict(saved_dict['optimizer_state_dict'])         # default: strict=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e16e66-f4cc-4556-8882-80a0f18f7598",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bd43a-81fe-480c-9d97-8cdb0651da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "root_dir = '.\\samples'\n",
    "test_set = '.\\samples\\Testing_set.xlsx'\n",
    "\n",
    "test_transformedFemur = FemurDataset(csv_file=test_set, root_dir=root_dir, transform=transforms.Compose([NormalizeSample(),ToTensor()]))  # For Testing\n",
    "testLoader = DataLoader(test_transformedFemur, batch_size=1, shuffle=0, num_workers=4)\n",
    "first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da076ae-e1aa-42f4-84de-a4541f8bd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "torch.cuda.empty_cache()\n",
    "if first==True:\n",
    "    print('First testing sample')\n",
    "    femurIter = iter(testLoader)\n",
    "    first = False\n",
    "else:\n",
    "    print('Next testing sample')\n",
    "sample = next(femurIter)\n",
    "target , ap , lat = sample['Target'] , sample['view1'] , sample['view2']\n",
    "print('Raw: target={}  ap={}  lat={}'.format(target.size(),ap.size(),lat.size()))\n",
    "target, ap, lat = target[0],ap[0].unsqueeze(0).to(device=device),lat[0].unsqueeze(0).to(device=device)  # for ToTensor6-8\n",
    "print('Final: target={}  ap={}  lat={}'.format(target.size(), ap.size(), lat.size()))\n",
    "    \n",
    "### Inference ###\n",
    "with torch.cuda.amp.autocast(enabled=True):\n",
    "    t1 = time.time()\n",
    "    fracReconNet.eval()\n",
    "    output = fracReconNet(ap,lat).detach()\n",
    "    t2 = time.time()\n",
    "    print('Inference time = {:,.4f} sec.'.format(t2-t1))\n",
    "    \n",
    "##### OUTPUT: Bone Visualization #####\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,20))\n",
    "ax[0].imshow(ap.detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "ax[0].set_title('Input view 1')\n",
    "ax[1].imshow(lat.detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "ax[1].set_title('Input view 2')\n",
    "plt.show()\n",
    "\n",
    "# Accuracy check\n",
    "gt = (target==1).detach().cpu().numpy()\n",
    "ot = (output[0,1,:]>0.5).detach().cpu().numpy()\n",
    "#print('IoU = {:,.3f}'.format(iou((target==1).float().cpu(), (output[0,1,:]>0.5).float().cpu())), end=', ')\n",
    "#dist_metrics = surface_distance_measurement(output[0,1].cpu().numpy(), (target==1).cpu().numpy(), res=0.5, verbose=False)\n",
    "#print('ASSD = {:.3f} mm'.format(dist_metrics['ASSD']))\n",
    "\n",
    "# Camera view\n",
    "cam_view1 = [0,0,-1.5, 0,0,0 ,0,-1,0]   # view1\n",
    "cam_view2 = [1.5,0,0, 0,0,0 ,1,-1,0]    # view2\n",
    "\n",
    "### Output : Bone Class ###\n",
    "plot = k3d.plot(name='Plot 2 : [view2]')\n",
    "obj = k3d.volume(ot, name='Output',\n",
    "                 color_map=k3d.colormaps.matplotlib_color_maps.Bone,\n",
    "                 gradient_step=0.005,\n",
    "                 shadow='dynamic',\n",
    "                 shadow_delay=10,\n",
    "                )\n",
    "plot += obj + k3d.text2d(text='Output', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "plot.display()\n",
    "plot.camera = cam_view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01cc370-5fbf-4bfc-a3a9-25ed713122e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show Volume rotation360 ###\n",
    "N = 3   # number of round\n",
    "r_orbit = 1.5\n",
    "camera_rotate = list([-r_orbit*np.sin(t), -0.2,r_orbit*np.cos(t), 0,0,0, 0,-1,0] for t in np.linspace(0, 2*np.pi*N, num=360*N) )  # 360*N\n",
    "k3d.plot()\n",
    "#plot4.grid_visible = False\n",
    "for i, view in enumerate(camera_rotate):\n",
    "    plot.camera = view\n",
    "    time.sleep(6/360)\n",
    "print('--- Rotation END ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175049c3-21f1-4021-a089-49822d891154",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "surf_dist_result = surface_distance_measurement(gt, ot, res=0.5, return_vert_dist=True, verbose=False)\n",
    "t2 = time.time()\n",
    "print('\\nTotal time of calculation = {} sec.'.format(t2-t1))\n",
    "print('key result = {}'.format(surf_dist_result.keys()))\n",
    "\n",
    "background_color = 65536*255 + 256*255 + 255\n",
    "pltmesh = k3d.plot(background_color=background_color)\n",
    "title_text = k3d.text2d(text='Min. Surface Distance (GroundTruth-based', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "meshsurf = k3d.mesh(surf_dist_result['surface_dist_gt']['target_vert'], surf_dist_result['surface_dist_gt']['target_face'],\n",
    "                    name='surface distance (mm)',\n",
    "                    color_map=k3d.colormaps.basic_color_maps.Jet,\n",
    "                    #color_map=k3d.colormaps.paraview_color_maps.Bone_Matlab\n",
    "                    color_range=[0,3],   # [0,3] for align dataset   |   [0,6] for unalign dataset\n",
    "                    attribute=surf_dist_result['surface_dist_gt']['distances'].astype(np.float32())\n",
    "                   )   #     -target_vert[:,2]\n",
    "pltmesh += title_text + meshsurf\n",
    "pltmesh.display()\n",
    "pltmesh.camera = [128,-100,512, 128,-128,128,  0,1,0]\n",
    "\n",
    "pltmesh2 = k3d.plot(background_color=background_color)\n",
    "title_text2 = k3d.text2d(text='Min. Surface Distance (Output-based)', color=0, size=1 ,position=(0.01,0.025), label_box=False)\n",
    "meshsurf2 = k3d.mesh(surf_dist_result['surface_dist_ot']['target_vert'], surf_dist_result['surface_dist_ot']['target_face'],\n",
    "                    name='surface distance (mm)',\n",
    "                    color_map=k3d.colormaps.basic_color_maps.Jet, \n",
    "                    color_range=[0,3],  # [0,3] for align dataset   |   [0,6] for unalign dataset\n",
    "                    attribute=surf_dist_result['surface_dist_ot']['distances'].astype(np.float32())\n",
    "                   )   #     -target_vert[:,2]\n",
    "pltmesh2 += title_text2 + meshsurf2\n",
    "pltmesh2.display()\n",
    "pltmesh2.camera = [128,-100,512, 128,-128,128,  0,1,0]\n",
    "print('--- END ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2ec46-e251-4992-9f76-5eb193e42a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show Mesh rotation360 ###\n",
    "N = 3   # number of round\n",
    "r_orbit = 384   #384\n",
    "camera_rotate = list([128+r_orbit*np.sin(t),-90,128-r_orbit*np.cos(t), 128,-128,128, 0,1,0] \n",
    "                     for t in np.linspace(0+np.pi/2, 2*np.pi*N+np.pi/2, num=360*N) )  # 360*N\n",
    "#k3d.plot()\n",
    "#pltmesh.grid_visible = False\n",
    "#pltmesh2.grid_visible = False\n",
    "\n",
    "for i, view in enumerate(camera_rotate):\n",
    "    #print('Order:{} | {}'.format(i,view))\n",
    "    pltmesh.camera = view\n",
    "    pltmesh2.camera = view\n",
    "    time.sleep(6/360)\n",
    "\n",
    "print('--- Rotation END ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e482245-5754-4b72-b7d4-1ef284896b84",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab3c3c-8b82-444c-9618-0a3a20540820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "saved_name = r'.\\weight\\test_of_training.pt'\n",
    "batch_sz = 3\n",
    "epoch_number = 1\n",
    "num_workers = 4\n",
    "learning_rate = 1e-4\n",
    "weight = torch.tensor([0.15,0.25,0.6], device=device)\n",
    "criterion = FocalLossMulticlass(weight=weight, gamma=2.0, reduction='sum')\n",
    "\n",
    "# Dataset preparation\n",
    "root_dir = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2'\n",
    "training_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\Training_set.xlsx'\n",
    "val_file = r'D:\\FEW PhD\\Datasets\\Chula DICOM 2021\\Cleaning Data\\Dataset2\\Validation_set.xlsx'\n",
    "train_transformedFemur = FemurDataset(csv_file=training_file, root_dir=root_dir, transform=transforms.Compose([NormalizeSample(), ToTensor()]))\n",
    "val_transformedFemur = FemurDataset(csv_file=val_file, root_dir=root_dir, transform=transforms.Compose([NormalizeSample(), ToTensor()]))\n",
    "trainLoader = DataLoader(train_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=num_workers)\n",
    "valLoader = DataLoader(val_transformedFemur, batch_size=batch_sz, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# FracReconNet's Configuration\n",
    "in_c = 1\n",
    "de3d_sz = None       # don't use now\n",
    "en_sz = [[16,4,4],[32,8,4],[64,16,4],[128,16,4],[192,16,4],[256,16,4],[320,16,4]]\n",
    "de_sz = [[64,4],[64,8],[128,16],[128,32],[128,64],[128,128],[256,256]]\n",
    "final_sz = [[2,32,32],[5,32,32],[5,32,32],[5,16,16],[5,16,16],[5,16,16],[5,16,16]]  # fusion layer\n",
    "note = 'Test training loop'\n",
    "\n",
    "fracReconNet = Model.fracReconNet(in_c,en_sz,de_sz,de3d_sz,final_sz)\n",
    "fracReconNet.to(device=device)\n",
    "optimizer = optim.Adam(fracReconNet.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, threshold=1e-3, \n",
    "                              threshold_mode='rel', cooldown=0, min_lr=0, verbose=True)\n",
    "\n",
    "saved_dict = {'timestamp':None ,'note':note , 'in_c':in_c ,'en_sz':en_sz ,'de_sz':de_sz ,'de3d_sz':de3d_sz ,'final_sz':final_sz ,\n",
    "              'train_loss_history':list(), 'train_acc_history':list(), 'val_loss_history':list(), 'val_acc_history':list() ,\n",
    "              'model_state_dict':None,'optimizer_state_dict':None,'scheduler_state_dict':None }\n",
    "\n",
    "print(' ##### Start training loop #####')\n",
    "print('   Total iterations = {:,}'.format(len(saved_dict['train_loss_history'])))\n",
    "print('   Start training at : {}'.format(str(datetime.datetime.now())))\n",
    "timeT1 = time.time()\n",
    "if torch.cuda.is_available():\n",
    "    model = nn.DataParallel(fracReconNet)\n",
    "    print('   Use mixed precision training')\n",
    "    saved_dict = train_mixed(model, trainLoader, valLoader, optimizer, scheduler, criterion, batch_sz, epoch_number, saved_name, saved_dict, device)\n",
    "else:\n",
    "    assert torch.cuda.is_available(), '!!! Training with CPU not available !!!'\n",
    "\n",
    "timeT2 = time.time()\n",
    "print('\\nTotal training time = {} hours \\nTotal epoch = {}\\n'.format((timeT2-timeT1)/3600, len(saved_dict['val_acc_history'])))\n",
    "print('Saved Name: {}'.format(saved_name))\n",
    "print('Finish task at : {}\\n'.format(str(datetime.datetime.now()) ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
